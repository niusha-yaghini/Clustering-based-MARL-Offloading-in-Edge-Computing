{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> V02 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: ./datasets\\ep_000\n"
     ]
    }
   ],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Final Data Generator (version 2)\n",
    "# Structure:\n",
    "# datasets/\n",
    "#    ep_000/\n",
    "#       light/\n",
    "#       moderate/\n",
    "#       heavy/\n",
    "#       dataset_metadata.json\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "# from dataclasses import dataclass, asdict, replace\n",
    "# from typing import List, Dict, Optional\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os, json, math, time, platform, getpass\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # ============================================================================\n",
    "# # GLOBAL SETTINGS\n",
    "# # ============================================================================\n",
    "# GLOBAL_SEED = 42\n",
    "# np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# # ============================================================================\n",
    "# # DATACLASSES\n",
    "# # ============================================================================\n",
    "# @dataclass\n",
    "# class EpisodeConf:\n",
    "#     Delta: float      \n",
    "#     T_slots: int      \n",
    "#     seed: int\n",
    "\n",
    "# @dataclass\n",
    "# class AgentRanges:\n",
    "#     lam_sec_min: float\n",
    "#     lam_sec_max: float\n",
    "#     f_local_min: float\n",
    "#     f_local_max: float\n",
    "#     m_local_min: float\n",
    "#     m_local_max: float\n",
    "\n",
    "# @dataclass\n",
    "# class TaskFeatureDist:\n",
    "#     b_median: float = 3.0\n",
    "#     b_sigma_g: float = 1.3\n",
    "\n",
    "#     rho_median: float = 1.2e9\n",
    "#     rho_sigma_g: float = 1.2\n",
    "\n",
    "#     mem_median: float = 64.0\n",
    "#     mem_sigma_g: float = 1.1\n",
    "\n",
    "#     p_deadline: float = 0.25\n",
    "#     deadline_min: float = 0.3\n",
    "#     deadline_max: float = 1.5\n",
    "\n",
    "#     p_non_atomic: float = 0.35\n",
    "#     split_ratio_min: float = 0.30\n",
    "#     split_ratio_max: float = 0.80\n",
    "\n",
    "#     modality_probs: Optional[List[float]] = None\n",
    "#     modality_labels: Optional[List[str]] = None\n",
    "\n",
    "# @dataclass\n",
    "# class GlobalConfig:\n",
    "#     name: str\n",
    "#     N_agents: int\n",
    "#     Episode: EpisodeConf\n",
    "#     AgentRanges: AgentRanges\n",
    "#     TaskDist: TaskFeatureDist\n",
    "\n",
    "# # ============================================================================\n",
    "# # UTILS\n",
    "# # ============================================================================\n",
    "# def _z_from_p(p):\n",
    "#     Z = {0.90:1.2816, 0.95:1.6448, 0.975:1.9600, 0.99:2.3263}\n",
    "#     return Z[min(Z.keys(), key=lambda x: abs(x - p))]\n",
    "\n",
    "# def lognormal_from_median_sigma(rng, median, sigma_g, qcap=0.99):\n",
    "#     mu = math.log(max(median, 1e-12))\n",
    "#     sigma = math.log(max(sigma_g, 1.0 + 1e-6))\n",
    "#     x = float(rng.lognormal(mu, sigma))\n",
    "#     cap = median * math.exp(_z_from_p(qcap) * sigma)\n",
    "#     return min(x, cap)\n",
    "\n",
    "# @dataclass\n",
    "# class Agent:\n",
    "#     agent_id: int\n",
    "#     f_local: float\n",
    "#     m_local: float\n",
    "#     lam_sec: float\n",
    "\n",
    "# # ============================================================================\n",
    "# # DATA GENERATION\n",
    "# # ============================================================================\n",
    "# def build_agents(cfg: GlobalConfig, rng):\n",
    "#     agents = []\n",
    "#     for i in range(cfg.N_agents):\n",
    "#         agents.append(Agent(\n",
    "#             agent_id=i,\n",
    "#             f_local=rng.uniform(cfg.AgentRanges.f_local_min, cfg.AgentRanges.f_local_max),\n",
    "#             m_local=rng.uniform(cfg.AgentRanges.m_local_min, cfg.AgentRanges.m_local_max),\n",
    "#             lam_sec=rng.uniform(cfg.AgentRanges.lam_sec_min, cfg.AgentRanges.lam_sec_max)\n",
    "#         ))\n",
    "#     return agents\n",
    "\n",
    "\n",
    "# def sample_task(cfg: GlobalConfig, rng):\n",
    "#     d = cfg.TaskDist\n",
    "#     b_mb = lognormal_from_median_sigma(rng, d.b_median, d.b_sigma_g)\n",
    "#     rho = lognormal_from_median_sigma(rng, d.rho_median, d.rho_sigma_g)\n",
    "#     mem = lognormal_from_median_sigma(rng, d.mem_median, d.mem_sigma_g)\n",
    "\n",
    "#     c_cycles = b_mb * rho\n",
    "\n",
    "#     has_deadline = int(rng.random() < d.p_deadline)\n",
    "#     deadline_s = rng.uniform(d.deadline_min, d.deadline_max) if has_deadline else np.nan\n",
    "\n",
    "#     non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "#     split_ratio = rng.uniform(d.split_ratio_min, d.split_ratio_max) if non_atomic else 0.0\n",
    "\n",
    "#     modality = (rng.choice(d.modality_labels) if d.modality_labels\n",
    "#                 else rng.choice([\"image\", \"video\", \"text\", \"sensor\"]))\n",
    "\n",
    "#     return dict(\n",
    "#         b_mb=b_mb,\n",
    "#         rho=rho,\n",
    "#         c_cycles=c_cycles,\n",
    "#         mem_mb=mem,\n",
    "#         modality=modality,\n",
    "#         has_deadline=has_deadline,\n",
    "#         deadline_s=deadline_s,\n",
    "#         non_atomic=non_atomic,\n",
    "#         split_ratio=split_ratio\n",
    "#     )\n",
    "\n",
    "\n",
    "# def generate_episode_for_scenario(cfg: GlobalConfig, episode_id: int, out_dir: str):\n",
    "#     rng = np.random.default_rng(cfg.Episode.seed + episode_id)\n",
    "#     agents = build_agents(cfg, rng)\n",
    "\n",
    "#     T_slots = cfg.Episode.T_slots\n",
    "#     Delta = cfg.Episode.Delta\n",
    "\n",
    "#     rows_ep, rows_agents, rows_arrivals, rows_tasks = [], [], [], []\n",
    "\n",
    "#     # Agents\n",
    "#     for a in agents:\n",
    "#         rows_agents.append(asdict(a))\n",
    "\n",
    "#     task_counter = 0\n",
    "\n",
    "#     for t in range(T_slots):\n",
    "\n",
    "#         t_time = t * Delta\n",
    "#         arrival_phase = (t < 100)\n",
    "\n",
    "#         if arrival_phase:\n",
    "#             for a in agents:\n",
    "#                 lam_slot = a.lam_sec * Delta\n",
    "#                 n_new = rng.poisson(lam_slot)\n",
    "\n",
    "#                 for _ in range(n_new):\n",
    "#                     feat = sample_task(cfg, rng)\n",
    "#                     deadline_time = (t_time + feat[\"deadline_s\"]) if feat[\"has_deadline\"] else np.nan\n",
    "\n",
    "#                     rows_arrivals.append({\n",
    "#                         \"scenario\": cfg.name,\n",
    "#                         \"episode_id\": episode_id,\n",
    "#                         \"t_slot\": t,\n",
    "#                         \"t_time\": t_time,\n",
    "#                         \"agent_id\": a.agent_id,\n",
    "#                         \"task_id\": task_counter\n",
    "#                     })\n",
    "\n",
    "#                     rows_tasks.append({\n",
    "#                         \"scenario\": cfg.name,\n",
    "#                         \"episode_id\": episode_id,\n",
    "#                         \"task_id\": task_counter,\n",
    "#                         \"agent_id\": a.agent_id,\n",
    "#                         \"t_arrival_slot\": t,\n",
    "#                         \"t_arrival_time\": t_time,\n",
    "#                         \"b_mb\": feat[\"b_mb\"],\n",
    "#                         \"rho_cyc_per_mb\": feat[\"rho\"],\n",
    "#                         \"c_cycles\": feat[\"c_cycles\"],\n",
    "#                         \"mem_mb\": feat[\"mem_mb\"],\n",
    "#                         \"modality\": feat[\"modality\"],\n",
    "#                         \"has_deadline\": feat[\"has_deadline\"],\n",
    "#                         \"deadline_s\": feat[\"deadline_s\"],\n",
    "#                         \"deadline_time\": deadline_time,\n",
    "#                         \"non_atomic\": feat[\"non_atomic\"],\n",
    "#                         \"split_ratio\": feat[\"split_ratio\"]\n",
    "#                     })\n",
    "#                     task_counter += 1\n",
    "\n",
    "#     rows_ep.append({\n",
    "#         \"scenario\": cfg.name,\n",
    "#         \"episode_id\": episode_id,\n",
    "#         \"Delta\": Delta,\n",
    "#         \"T_slots\": T_slots,\n",
    "#         \"arrival_slots\": 100,\n",
    "#         \"drain_slots\": T_slots - 100,\n",
    "#         \"hours\": (T_slots * Delta) / 3600.0,\n",
    "#         \"N_agents\": cfg.N_agents\n",
    "#     })\n",
    "\n",
    "#     # SAVE DATA\n",
    "#     dfs = {\n",
    "#         \"episodes\": pd.DataFrame(rows_ep),\n",
    "#         \"agents\": pd.DataFrame(rows_agents),\n",
    "#         \"arrivals\": pd.DataFrame(rows_arrivals),\n",
    "#         \"tasks\": pd.DataFrame(rows_tasks)\n",
    "#     }\n",
    "\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "#     for name, df in dfs.items():\n",
    "#         df.to_csv(os.path.join(out_dir, f\"{name}.csv\"), index=False)\n",
    "\n",
    "#     return dfs\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # SUMMARY + PLOTTING\n",
    "# # ============================================================================\n",
    "# def summarize_and_plot(dfs: Dict[str, pd.DataFrame], out_dir: str):\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     tasks = dfs[\"tasks\"]\n",
    "#     arrivals = dfs[\"arrivals\"]\n",
    "#     episodes = dfs[\"episodes\"]\n",
    "\n",
    "#     def q(x, p):\n",
    "#         x = x.dropna()\n",
    "#         return float(np.nanquantile(x, p)) if len(x) else np.nan\n",
    "\n",
    "#     summary = {\n",
    "#         \"n_tasks\": len(tasks),\n",
    "#         \"n_arrivals\": len(arrivals),\n",
    "#         \"tasks_per_hour\": len(tasks) / episodes.iloc[0][\"hours\"],\n",
    "#         \"b_mb_median\": q(tasks[\"b_mb\"], 0.5),\n",
    "#         \"rho_median\": q(tasks[\"rho_cyc_per_mb\"], 0.5),\n",
    "#         \"c_cycles_p99\": q(tasks[\"c_cycles\"], 0.99),\n",
    "#         \"deadline_share\": float((tasks[\"has_deadline\"] == 1).mean()),\n",
    "#         \"non_atomic_share\": float((tasks[\"non_atomic\"] == 1).mean())\n",
    "#     }\n",
    "\n",
    "#     pd.DataFrame([summary]).to_csv(os.path.join(out_dir, \"summary_stats.csv\"), index=False)\n",
    "\n",
    "#     # Plots\n",
    "#     def hist_plot(series, title, fname):\n",
    "#         plt.figure()\n",
    "#         s = series.dropna()\n",
    "#         plt.hist(s, bins=40) if len(s) else None\n",
    "#         plt.title(title)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(out_dir, fname))\n",
    "#         plt.close()\n",
    "\n",
    "#     hist_plot(tasks[\"b_mb\"], \"Task Size (MB)\", \"hist_b_mb.png\")\n",
    "#     hist_plot(tasks[\"c_cycles\"], \"Total Cycles\", \"hist_c_cycles.png\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # META JSON WRITER\n",
    "# # ============================================================================\n",
    "# def write_metadata_json(ep_dir: str, cfg_list: List[GlobalConfig]):\n",
    "#     meta = {\n",
    "#         \"schema_version\": \"2.0\",\n",
    "#         \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         \"python_version\": platform.python_version(),\n",
    "#         \"user\": getpass.getuser(),\n",
    "#         \"episode_folder\": ep_dir,\n",
    "#         \"scenarios\": [],\n",
    "#         \"notes\": {\n",
    "#             \"slots\": \"100 arrival + 10 drain\",\n",
    "#             \"structure\": \"datasets/ep_XXX/{light,moderate,heavy}/\",\n",
    "#             \"usage\": \"Feed into MDP environment\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     for cfg in cfg_list:\n",
    "#         meta[\"scenarios\"].append({\n",
    "#             \"name\": cfg.name,\n",
    "#             \"N_agents\": cfg.N_agents,\n",
    "#             \"Episode\": asdict(cfg.Episode),\n",
    "#             \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "#             \"TaskDist\": asdict(cfg.TaskDist)\n",
    "#         })\n",
    "\n",
    "#     with open(os.path.join(ep_dir, \"dataset_metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN RUNNER\n",
    "# # ============================================================================\n",
    "# def generate_all_scenarios_in_episode(ep_index: int, out_root=\"./datasets_V02\"):\n",
    "\n",
    "#     # HOODIE-style episode setup\n",
    "#     BASE_EP = EpisodeConf(Delta=1.0, T_slots=110, seed=GLOBAL_SEED + ep_index)\n",
    "\n",
    "#     scenarios = [\n",
    "#         GlobalConfig(\n",
    "#             name=\"light\",\n",
    "#             N_agents=18,\n",
    "#             Episode=replace(BASE_EP, seed=GLOBAL_SEED + 1000 + ep_index),\n",
    "#             AgentRanges=AgentRanges(0.01, 0.05, 0.8e9, 2.4e9, 3e3, 8e3),\n",
    "#             TaskDist=TaskFeatureDist(b_median=2.0, b_sigma_g=1.55)\n",
    "#         ),\n",
    "#         GlobalConfig(\n",
    "#             name=\"moderate\",\n",
    "#             N_agents=18,\n",
    "#             Episode=replace(BASE_EP, seed=GLOBAL_SEED + 2000 + ep_index),\n",
    "#             AgentRanges=AgentRanges(0.05, 0.20, 0.8e9, 2.4e9, 3e3, 8e3),\n",
    "#             TaskDist=TaskFeatureDist(b_median=3.0, b_sigma_g=1.60)\n",
    "#         ),\n",
    "#         GlobalConfig(\n",
    "#             name=\"heavy\",\n",
    "#             N_agents=18,\n",
    "#             Episode=replace(BASE_EP, seed=GLOBAL_SEED + 3000 + ep_index),\n",
    "#             AgentRanges=AgentRanges(0.20, 0.80, 0.8e9, 2.4e9, 3e3, 8e3),\n",
    "#             TaskDist=TaskFeatureDist(b_median=5.0, b_sigma_g=1.70)\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     # Episode folder\n",
    "#     ep_dir = os.path.join(out_root, f\"ep_{ep_index:03d}\")\n",
    "#     os.makedirs(ep_dir, exist_ok=True)\n",
    "\n",
    "#     # Generate for each scenario\n",
    "#     for cfg in scenarios:\n",
    "#         scen_dir = os.path.join(ep_dir, cfg.name)\n",
    "#         dfs = generate_episode_for_scenario(cfg, ep_index, scen_dir)\n",
    "#         summarize_and_plot(dfs, scen_dir)\n",
    "\n",
    "#     # Write metadata JSON\n",
    "#     write_metadata_json(ep_dir, scenarios)\n",
    "\n",
    "#     return ep_dir\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN\n",
    "# # ============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     ep_dir = generate_all_scenarios_in_episode(ep_index=0, out_root=\"./datasets_V02\")\n",
    "#     print(\"Generated:\", ep_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> V01 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مقاله مرجع می‌گوید هر اپیزود شامل T = 110 تایم‌اسلات است (100 اسلات برای تصمیم‌گیری + 10 اسلات برای خالی کردن صف‌ها)، کل آموزش در 5000 اپیزود اجرا شده است. محیط‌های DRL معمولاً event-driven هستند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ep_000\": {\n",
      "    \"light_episodes_csv\": \"./datasets\\\\ep_000\\\\light\\\\episodes.csv\",\n",
      "    \"light_agents_csv\": \"./datasets\\\\ep_000\\\\light\\\\agents.csv\",\n",
      "    \"light_arrivals_csv\": \"./datasets\\\\ep_000\\\\light\\\\arrivals.csv\",\n",
      "    \"light_tasks_csv\": \"./datasets\\\\ep_000\\\\light\\\\tasks.csv\",\n",
      "    \"moderate_episodes_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\episodes.csv\",\n",
      "    \"moderate_agents_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\agents.csv\",\n",
      "    \"moderate_arrivals_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\arrivals.csv\",\n",
      "    \"moderate_tasks_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\tasks.csv\",\n",
      "    \"heavy_episodes_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\episodes.csv\",\n",
      "    \"heavy_agents_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\agents.csv\",\n",
      "    \"heavy_arrivals_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\arrivals.csv\",\n",
      "    \"heavy_tasks_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\tasks.csv\",\n",
      "    \"metadata_json\": \"./datasets\\\\ep_000\\\\dataset_metadata.json\"\n",
      "  }\n",
      "}\n",
      "[sanity] Checking episode directory: ./datasets\\ep_000\n",
      "  [OK] Loaded metadata with 3 scenarios\n",
      "  [scenario=light] n_tasks=70, n_arrivals=70, n_agents=18\n",
      "    [INFO] Poisson check: some agents have realized rate far from expected (too_low=0, too_high=5) in 'light'\n",
      "  [scenario=moderate] n_tasks=261, n_arrivals=261, n_agents=18\n",
      "  [scenario=heavy] n_tasks=841, n_arrivals=841, n_agents=18\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data generator for Edge–MEC–Cloud with Poisson arrivals (per-second),\n",
    "lognormal task features (parameterized by median & sigma_g), and policy-agnostic outputs.\n",
    "\n",
    "HOODIE-style timing:\n",
    "  - Each episode has T = 110 time-slots:\n",
    "      * T_decision = 100 slots for decision-making (with arrivals)\n",
    "      * T_drain    = 10 slots for draining queues (NO new arrivals)\n",
    "  - DRL environment can later consume this as event-driven using the time-stamps.\n",
    "\n",
    "Supports THREE SCENARIOS (light / moderate / heavy) similar to HOODIE experiments.\n",
    "For each episode we:\n",
    "  - synthesize time-stamped arrivals and task features for ALL scenarios\n",
    "  - save CSV + a per-episode dataset_metadata.json\n",
    "  - plot distribution figures (PNG) for key variables\n",
    "  - export a summary_stats.csv with quantiles/means per scenario\n",
    "\n",
    "Directory layout (per episode):\n",
    "  datasets/\n",
    "    ep_000/\n",
    "      light/\n",
    "        episodes.csv\n",
    "        agents.csv\n",
    "        arrivals.csv\n",
    "        tasks.csv\n",
    "        summary_stats.csv\n",
    "        *.png\n",
    "      moderate/\n",
    "        ...\n",
    "      heavy/\n",
    "        ...\n",
    "      dataset_metadata.json   # meta info for ALL scenarios in this episode\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, math, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib, time, platform, getpass\n",
    "\n",
    "# -------------------------\n",
    "# reproducibility\n",
    "# -------------------------\n",
    "GLOBAL_SEED = 42\n",
    "rng_global = np.random.default_rng(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# -------------------------\n",
    "# configuration dataclasses\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class EpisodeConf:\n",
    "    Delta: float        # seconds per slot\n",
    "    T_slots: int        # total number of slots in the episode (e.g. 110)\n",
    "    T_decision: int     # number of slots with arrivals (e.g. 100)\n",
    "    T_drain: int        # number of drain slots without arrivals (e.g. 10)\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class AgentRanges:\n",
    "    # arrival rate per SECOND (will be multiplied by Delta per slot)\n",
    "    lam_sec_min: float\n",
    "    lam_sec_max: float\n",
    "    # optional local capacities (kept as meta for later)\n",
    "    f_local_min: float\n",
    "    f_local_max: float\n",
    "    m_local_min: float\n",
    "    m_local_max: float\n",
    "\n",
    "@dataclass\n",
    "class TaskFeatureDist:\n",
    "    # Lognormal parameterization via median and sigma_g (geometric std).\n",
    "    b_median: float = 3.0          # MB\n",
    "    b_sigma_g: float = 1.5\n",
    "\n",
    "    rho_median: float = 1.2e9      # cycles / MB\n",
    "    rho_sigma_g: float = 1.5\n",
    "\n",
    "    mem_median: float = 64.0       # MB\n",
    "    mem_sigma_g: float = 1.5\n",
    "\n",
    "    p_deadline: float = 0.25\n",
    "    deadline_min: float = 0.3      # seconds (relative)\n",
    "    deadline_max: float = 1.5\n",
    "\n",
    "    p_non_atomic: float = 0.35\n",
    "    split_ratio_min: float = 0.30  # fraction of task size that CAN be split\n",
    "    split_ratio_max: float = 0.80\n",
    "\n",
    "    # Optional modality probabilities (image, video, text, sensor)\n",
    "    modality_probs: Optional[List[float]] = None\n",
    "    modality_labels: List[str] = None\n",
    "\n",
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    name: str\n",
    "    N_agents: int\n",
    "    Episode: EpisodeConf\n",
    "    AgentRanges: AgentRanges\n",
    "    TaskDist: TaskFeatureDist\n",
    "\n",
    "# -------------------------\n",
    "# asserts / validation\n",
    "# -------------------------\n",
    "def _validate_cfg(cfg: GlobalConfig) -> None:\n",
    "    assert cfg.N_agents > 0\n",
    "    assert cfg.Episode.Delta > 0\n",
    "    assert cfg.Episode.T_slots > 0\n",
    "    assert cfg.Episode.T_decision > 0\n",
    "    assert cfg.Episode.T_drain >= 0\n",
    "    assert cfg.Episode.T_slots == cfg.Episode.T_decision + cfg.Episode.T_drain\n",
    "\n",
    "    ar = cfg.AgentRanges\n",
    "    assert ar.lam_sec_min >= 0 and ar.lam_sec_max >= ar.lam_sec_min\n",
    "    td = cfg.TaskDist\n",
    "    assert td.split_ratio_min > 0 and td.split_ratio_max <= 1.0 and td.split_ratio_max >= td.split_ratio_min\n",
    "    assert td.deadline_min <= td.deadline_max\n",
    "    # geometric std must be >= 1.0\n",
    "    assert td.b_sigma_g >= 1.0 and td.rho_sigma_g >= 1.0 and td.mem_sigma_g >= 1.0\n",
    "\n",
    "# -------------------------\n",
    "# helpers\n",
    "# -------------------------\n",
    "_Z_TABLE = {\n",
    "    0.90: 1.2815515655446004,\n",
    "    0.95: 1.6448536269514722,\n",
    "    0.975: 1.959963984540054,\n",
    "    0.99: 2.3263478740408408,\n",
    "    0.995: 2.5758293035489004,\n",
    "    0.999: 3.090232306167813\n",
    "}\n",
    "\n",
    "def _z_from_p(p: float) -> float:\n",
    "    # Use small table + nearest clamp (no SciPy dependency)\n",
    "    if p in _Z_TABLE:\n",
    "        return _Z_TABLE[p]\n",
    "    # clamp to nearest key\n",
    "    return _Z_TABLE[min(_Z_TABLE.keys(), key=lambda k: abs(k - p))]\n",
    "\n",
    "def lognormal_quantile(median: float, sigma_g: float, p: float) -> float:\n",
    "    # X ~ LogNormal(mu, sigma) with median=exp(mu), sigma_g=exp(sigma)\n",
    "    # quantile(p) = median * exp( z_p * ln(sigma_g) )\n",
    "    z = _z_from_p(p)\n",
    "    return median * math.exp(z * math.log(max(sigma_g, 1.0 + 1e-6)))\n",
    "\n",
    "def lognormal_from_median_sigma_g(\n",
    "    rng: np.random.Generator,\n",
    "    median: float,\n",
    "    sigma_g: float,\n",
    "    qcap: float | None = 0.99\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Draw from LogNormal with given median and geometric std:\n",
    "      X ~ LogNormal(mu, sigma) where median = exp(mu), sigma_g = exp(sigma).\n",
    "      => mu = ln(median), sigma = ln(sigma_g)\n",
    "    \"\"\"\n",
    "    mu = math.log(max(median, 1e-12))\n",
    "    sigma = math.log(max(sigma_g, 1.0 + 1e-6))\n",
    "    x = float(rng.lognormal(mean=mu, sigma=sigma))\n",
    "    if qcap is not None:\n",
    "        cap = lognormal_quantile(median, sigma_g, qcap)\n",
    "        x = min(x, cap)\n",
    "    return x\n",
    "\n",
    "# -------------------------\n",
    "# entities\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Agent:\n",
    "    agent_id: int\n",
    "    f_local: float\n",
    "    m_local: float\n",
    "    lam_sec: float   # Poisson rate per second (not per-slot)\n",
    "\n",
    "def build_agents(cfg: GlobalConfig, rng: np.random.Generator) -> List[Agent]:\n",
    "    agents: List[Agent] = []\n",
    "    for i in range(cfg.N_agents):\n",
    "        lam_sec = rng.uniform(cfg.AgentRanges.lam_sec_min, cfg.AgentRanges.lam_sec_max)\n",
    "        f_loc   = rng.uniform(cfg.AgentRanges.f_local_min, cfg.AgentRanges.f_local_max)\n",
    "        m_loc   = rng.uniform(cfg.AgentRanges.m_local_min, cfg.AgentRanges.m_local_max)\n",
    "        agents.append(Agent(agent_id=i, f_local=f_loc, m_local=m_loc, lam_sec=lam_sec))\n",
    "    return agents\n",
    "\n",
    "# -------------------------\n",
    "# task features\n",
    "# -------------------------\n",
    "def _modality_choice(rng: np.random.Generator, d: TaskFeatureDist) -> str:\n",
    "    labels = d.modality_labels or [\"image\", \"video\", \"text\", \"sensor\"]\n",
    "    if d.modality_probs is None:\n",
    "        probs = [0.3, 0.2, 0.3, 0.2]\n",
    "    else:\n",
    "        probs = d.modality_probs\n",
    "        assert abs(sum(probs) - 1.0) < 1e-6 and len(probs) == len(labels)\n",
    "    return rng.choice(labels, p=probs)\n",
    "\n",
    "def sample_task_features(cfg: GlobalConfig, rng: np.random.Generator, qcap: float = 0.99) -> Dict[str, float]:\n",
    "    d = cfg.TaskDist\n",
    "    b_mb   = lognormal_from_median_sigma_g(rng, d.b_median,   d.b_sigma_g,   qcap=qcap)\n",
    "    rho    = lognormal_from_median_sigma_g(rng, d.rho_median, d.rho_sigma_g, qcap=qcap)\n",
    "    c      = b_mb * rho                              # total cycles\n",
    "    mem_mb = lognormal_from_median_sigma_g(rng, d.mem_median, d.mem_sigma_g, qcap=qcap)\n",
    "    modality = _modality_choice(rng, d)\n",
    "    has_deadline = int(rng.random() < d.p_deadline)\n",
    "    deadline_s   = np.nan\n",
    "    if has_deadline:\n",
    "        deadline_s = float(rng.uniform(d.deadline_min, d.deadline_max))\n",
    "    non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "    split_ratio = float(rng.uniform(d.split_ratio_min, d.split_ratio_max)) if non_atomic else 0.0\n",
    "    \n",
    "    return dict(\n",
    "        b_mb=b_mb, rho=rho, c_cycles=c, mem_mb=mem_mb, modality=modality,\n",
    "        has_deadline=has_deadline, deadline_s=deadline_s, non_atomic=non_atomic, split_ratio=split_ratio\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# episode generator (arrivals only)\n",
    "# -------------------------\n",
    "def run_episode(\n",
    "    cfg: GlobalConfig,\n",
    "    agents: List[Agent],\n",
    "    episode_id: int = 0,\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate time-stamped arrivals and task features for ONE episode of ONE scenario.\n",
    "\n",
    "    HOODIE-style:\n",
    "      - t = 0 .. T_decision-1 → arrivals via Poisson\n",
    "      - t = T_decision .. T_slots-1 → NO new arrivals (drain phase)\n",
    "    \"\"\"\n",
    "    _validate_cfg(cfg)\n",
    "    rng_local = np.random.default_rng(cfg.Episode.seed + episode_id)\n",
    "\n",
    "    rows_episodes: List[Dict] = []\n",
    "    rows_agents:   List[Dict] = []\n",
    "    rows_arrivals: List[Dict] = []\n",
    "    rows_tasks:    List[Dict] = []\n",
    "\n",
    "    Delta      = cfg.Episode.Delta\n",
    "    T_slots    = cfg.Episode.T_slots\n",
    "    T_decision = cfg.Episode.T_decision\n",
    "\n",
    "    # agents (include scenario for easier joins)\n",
    "    for a in agents:\n",
    "        rows_agents.append({\n",
    "            \"scenario\": cfg.name,\n",
    "            \"agent_id\": a.agent_id,\n",
    "            \"f_local\": a.f_local,\n",
    "            \"m_local\": a.m_local,\n",
    "            \"lam_sec\": a.lam_sec\n",
    "        })\n",
    "\n",
    "    task_id_counter = 0\n",
    "\n",
    "    for t in range(T_slots):\n",
    "        t_time = t * Delta\n",
    "\n",
    "        # Only first T_decision slots have new arrivals (HOODIE style)\n",
    "        if t >= T_decision:\n",
    "            continue\n",
    "\n",
    "        for a in agents:\n",
    "            # per-slot rate from per-second rate:\n",
    "            lam_slot = a.lam_sec * Delta\n",
    "            n_new = rng_local.poisson(lam=lam_slot)\n",
    "            if n_new <= 0:\n",
    "                continue\n",
    "            for _ in range(n_new):\n",
    "                feat = sample_task_features(cfg, rng_local, qcap=qcap)\n",
    "\n",
    "                # absolute deadline time (NaN if none)\n",
    "                if np.isnan(feat[\"deadline_s\"]):\n",
    "                    deadline_time = np.nan\n",
    "                else:\n",
    "                    deadline_time = t_time + feat[\"deadline_s\"]\n",
    "\n",
    "                action_space_hint = \"continuous\" if feat[\"non_atomic\"] == 1 else \"discrete\"\n",
    "\n",
    "                rows_arrivals.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"t_slot\": t,\n",
    "                    \"t_time\": t_time,\n",
    "                    \"agent_id\": a.agent_id,\n",
    "                    \"task_id\": task_id_counter\n",
    "                })\n",
    "\n",
    "                rows_tasks.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"task_id\": task_id_counter,\n",
    "                    \"agent_id\": a.agent_id,\n",
    "                    \"t_arrival_slot\": t,\n",
    "                    \"t_arrival_time\": t_time,\n",
    "                    \"b_mb\": feat[\"b_mb\"],\n",
    "                    \"rho_cyc_per_mb\": feat[\"rho\"],\n",
    "                    \"c_cycles\": feat[\"c_cycles\"],\n",
    "                    \"mem_mb\": feat[\"mem_mb\"],\n",
    "                    \"modality\": feat[\"modality\"],\n",
    "                    \"has_deadline\": feat[\"has_deadline\"],\n",
    "                    \"deadline_s\": feat[\"deadline_s\"],\n",
    "                    \"deadline_time\": deadline_time,\n",
    "                    \"non_atomic\": feat[\"non_atomic\"],\n",
    "                    \"split_ratio\": feat[\"split_ratio\"],\n",
    "                    \"action_space_hint\": action_space_hint\n",
    "                })\n",
    "\n",
    "                task_id_counter += 1\n",
    "\n",
    "    rows_episodes.append({\n",
    "        \"scenario\": cfg.name,\n",
    "        \"episode_id\": episode_id,\n",
    "        \"Delta\": Delta,\n",
    "        \"T_slots\": T_slots,\n",
    "        \"T_decision\": T_decision,\n",
    "        \"T_drain\": cfg.Episode.T_drain,\n",
    "        \"hours\": T_slots * Delta / 3600.0,\n",
    "        \"N_agents\": len(agents),\n",
    "        \"seed\": cfg.Episode.seed + episode_id\n",
    "    })\n",
    "\n",
    "    episodes_df = pd.DataFrame(rows_episodes)\n",
    "    agents_df   = pd.DataFrame(rows_agents)\n",
    "    arrivals_df = pd.DataFrame(rows_arrivals)\n",
    "    tasks_df    = pd.DataFrame(rows_tasks)\n",
    "\n",
    "    # Optimize dtypes\n",
    "    if len(tasks_df):\n",
    "        tasks_df[\"modality\"] = tasks_df[\"modality\"].astype(\"category\")\n",
    "        tasks_df[\"action_space_hint\"] = tasks_df[\"action_space_hint\"].astype(\"category\")\n",
    "        # ints\n",
    "        for col in [\"episode_id\", \"task_id\", \"agent_id\", \"t_arrival_slot\", \"has_deadline\", \"non_atomic\"]:\n",
    "            if col in tasks_df:\n",
    "                tasks_df[col] = tasks_df[col].astype(\"int32\")\n",
    "        # floats\n",
    "        for col in [\"t_arrival_time\", \"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\",\n",
    "                    \"deadline_s\", \"deadline_time\", \"split_ratio\"]:\n",
    "            if col in tasks_df:\n",
    "                tasks_df[col] = tasks_df[col].astype(\"float32\")\n",
    "\n",
    "    if len(arrivals_df):\n",
    "        for col in [\"episode_id\", \"t_slot\", \"agent_id\", \"task_id\"]:\n",
    "            if col in arrivals_df:\n",
    "                arrivals_df[col] = arrivals_df[col].astype(\"int32\")\n",
    "        for col in [\"t_time\"]:\n",
    "            if col in arrivals_df:\n",
    "                arrivals_df[col] = arrivals_df[col].astype(\"float32\")\n",
    "\n",
    "    if len(agents_df):\n",
    "        for col in [\"agent_id\"]:\n",
    "            agents_df[col] = agents_df[col].astype(\"int32\")\n",
    "        for col in [\"f_local\", \"m_local\", \"lam_sec\"]:\n",
    "            agents_df[col] = agents_df[col].astype(\"float64\")\n",
    "\n",
    "    if len(episodes_df):\n",
    "        for col in [\"episode_id\", \"N_agents\", \"T_slots\", \"T_decision\", \"T_drain\"]:\n",
    "            if col in episodes_df:\n",
    "                episodes_df[col] = episodes_df[col].astype(\"int32\")\n",
    "        for col in [\"Delta\", \"hours\"]:\n",
    "            if col in episodes_df:\n",
    "                episodes_df[col] = episodes_df[col].astype(\"float32\")\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes_df,\n",
    "        \"agents\":   agents_df,\n",
    "        \"arrivals\": arrivals_df,\n",
    "        \"tasks\":    tasks_df\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# save & plotting utilities\n",
    "# -------------------------\n",
    "def save_dataset(dfs: Dict[str, pd.DataFrame], out_dir: str = \".\") -> Dict[str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    paths: Dict[str, str] = {}\n",
    "    for name, df in dfs.items():\n",
    "        csv_path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        paths[name + \"_csv\"] = csv_path\n",
    "    return paths\n",
    "\n",
    "def _config_fingerprint(cfg: GlobalConfig) -> str:\n",
    "    s = json.dumps({\n",
    "        \"scenario\": cfg.name,\n",
    "        \"Episode\": asdict(cfg.Episode),\n",
    "        \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "        \"TaskDist\": asdict(cfg.TaskDist)\n",
    "    }, sort_keys=True).encode(\"utf-8\")\n",
    "    return hashlib.sha256(s).hexdigest()[:16]\n",
    "\n",
    "def save_episode_meta(\n",
    "    cfgs: List[GlobalConfig],\n",
    "    ep_dir: str,\n",
    "    qcap: float = 0.99\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save a single dataset_metadata.json inside ep_xxx containing\n",
    "    config info for ALL scenarios used in this episode.\n",
    "    \"\"\"\n",
    "    meta = {\n",
    "        \"schema_version\": \"1.0.0\",\n",
    "        \"generated_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "        \"env\": {\n",
    "            \"python\": platform.python_version(),\n",
    "            \"user\": getpass.getuser()\n",
    "        },\n",
    "        \"episodes_root\": os.path.abspath(ep_dir),\n",
    "        \"notes\": {\n",
    "            \"policy_agnostic\": True,\n",
    "            \"queueing\": \"not simulated here\",\n",
    "            \"timing\": {\n",
    "                \"description\": \"HOODIE-style: T_decision slots with arrivals + T_drain slots without arrivals.\",\n",
    "            },\n",
    "            \"clipping\": {\n",
    "                \"enabled\": True,\n",
    "                \"method\": \"lognormal analytic quantile cap\",\n",
    "                \"qcap\": qcap,\n",
    "                \"z_table_keys\": sorted(list(_Z_TABLE.keys()))\n",
    "            },\n",
    "            \"action_space_hint\": \"derived from non_atomic; final decision belongs to environment\"\n",
    "        },\n",
    "        \"scenarios\": []\n",
    "    }\n",
    "\n",
    "    for cfg in cfgs:\n",
    "        meta[\"scenarios\"].append({\n",
    "            \"name\": cfg.name,\n",
    "            \"fingerprint\": _config_fingerprint(cfg),\n",
    "            \"Episode\": asdict(cfg.Episode),\n",
    "            \"N_agents\": cfg.N_agents,\n",
    "            \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "            \"TaskDist\": asdict(cfg.TaskDist),\n",
    "            \"units\": {\n",
    "                \"Delta\": \"seconds\",\n",
    "                \"lam_sec\": \"tasks per second (per agent)\",\n",
    "                \"per_slot_rate\": \"lam_sec * Delta\",\n",
    "                \"b_mb\": \"MB\",\n",
    "                \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
    "                \"c_cycles\": \"CPU cycles\",\n",
    "                \"mem_mb\": \"MB\",\n",
    "                \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\",\n",
    "                \"f_local\": \"Hz\",\n",
    "                \"m_local\": \"MB\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    path = os.path.join(ep_dir, \"dataset_metadata.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    return path\n",
    "\n",
    "def summarize_and_plot(dfs: Dict[str, pd.DataFrame], out_dir: str) -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tasks = dfs[\"tasks\"].copy()\n",
    "    arrivals = dfs[\"arrivals\"].copy()\n",
    "    episodes = dfs[\"episodes\"].copy()\n",
    "\n",
    "    # ---- summary stats\n",
    "    def q(series: pd.Series, p: float):\n",
    "        s = series.dropna()\n",
    "        return float(np.nanquantile(s, p)) if len(s) else float(\"nan\")\n",
    "\n",
    "    tasks_per_hour = float(len(tasks)) / float(episodes.iloc[0][\"hours\"]) if len(episodes) and episodes.iloc[0][\"hours\"] > 0 else float(\"nan\")\n",
    "\n",
    "    # modality distribution\n",
    "    if len(tasks):\n",
    "        mod_counts = tasks[\"modality\"].value_counts(dropna=False)\n",
    "        mod_dist = {str(k): int(v) for k, v in mod_counts.to_dict().items()}\n",
    "    else:\n",
    "        mod_dist = {}\n",
    "\n",
    "    summary = {\n",
    "        \"n_tasks\": [len(tasks)],\n",
    "        \"n_arrivals\": [len(arrivals)],\n",
    "        \"tasks_per_hour\": [tasks_per_hour],\n",
    "        \"b_mb_median\": [q(tasks[\"b_mb\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"b_mb_p90\": [q(tasks[\"b_mb\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"b_mb_p99\": [q(tasks[\"b_mb\"], 0.99)] if len(tasks) else [float(\"nan\")],\n",
    "        \"rho_median\": [q(tasks[\"rho_cyc_per_mb\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"rho_p90\": [q(tasks[\"rho_cyc_per_mb\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_median\": [q(tasks[\"c_cycles\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_p90\": [q(tasks[\"c_cycles\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_p99\": [q(tasks[\"c_cycles\"], 0.99)] if len(tasks) else [float(\"nan\")],\n",
    "        \"deadline_share\": [float((tasks[\"has_deadline\"] == 1).mean()) if len(tasks) else float(\"nan\")],\n",
    "        \"non_atomic_share\": [float((tasks[\"non_atomic\"] == 1).mean()) if len(tasks) else float(\"nan\")],\n",
    "        \"modality_counts_json\": [json.dumps(mod_dist)]\n",
    "    }\n",
    "    pd.DataFrame(summary).to_csv(os.path.join(out_dir, \"summary_stats.csv\"), index=False)\n",
    "\n",
    "    # ---- plots (each in its own figure)\n",
    "    def hist_plot(series: pd.Series, title: str, fname: str, logx: bool = False):\n",
    "        plt.figure()\n",
    "        s = series.dropna()\n",
    "        if len(s) == 0:\n",
    "            plt.title(title + \" (no data)\")\n",
    "        else:\n",
    "            plt.hist(s, bins=50)\n",
    "            if logx:\n",
    "                plt.xscale(\"log\")\n",
    "            plt.title(title)\n",
    "            plt.xlabel(title)\n",
    "            plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, fname), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    if len(tasks):\n",
    "        hist_plot(tasks[\"b_mb\"],            title=\"Task size (MB)\",                fname=\"hist_b_mb.png\", logx=True)\n",
    "        hist_plot(tasks[\"rho_cyc_per_mb\"],  title=\"Compute density (cycles/MB)\",   fname=\"hist_rho.png\",  logx=True)\n",
    "        hist_plot(tasks[\"c_cycles\"],        title=\"Total cycles\",                  fname=\"hist_c_cycles.png\", logx=True)\n",
    "        hist_plot(tasks[\"deadline_s\"],      title=\"Deadline (s)\",                  fname=\"hist_deadline_s.png\", logx=False)\n",
    "        hist_plot(tasks.loc[tasks[\"non_atomic\"] == 1, \"split_ratio\"],\n",
    "                  title=\"Split ratio (only non-atomic)\", fname=\"hist_split_ratio.png\", logx=False)\n",
    "\n",
    "    # arrivals per agent\n",
    "    if len(arrivals):\n",
    "        per_agent = arrivals.groupby(\"agent_id\").size()\n",
    "        plt.figure()\n",
    "        plt.bar(per_agent.index.astype(str), per_agent.values)\n",
    "        plt.title(\"Arrivals per agent\")\n",
    "        plt.xlabel(\"agent_id\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"bar_arrivals_per_agent.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# scenario presets (light / moderate / heavy) – HOODIE-style timing\n",
    "# -------------------------\n",
    "DEFAULT_DELTA = 1.0      # 1 second per slot (unit choice)\n",
    "T_DECISION = 100         # 100 decision slots\n",
    "T_DRAIN    = 10          # 10 drain slots\n",
    "DEFAULT_T_SLOTS = T_DECISION + T_DRAIN  # 110 total\n",
    "\n",
    "BASE_EPISODE = EpisodeConf(\n",
    "    Delta=DEFAULT_DELTA,\n",
    "    T_slots=DEFAULT_T_SLOTS,\n",
    "    T_decision=T_DECISION,\n",
    "    T_drain=T_DRAIN,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "BASE_AGENT_RANGES = AgentRanges(\n",
    "    lam_sec_min=0.02, lam_sec_max=0.80,\n",
    "    f_local_min=0.8e9, f_local_max=2.4e9,\n",
    "    m_local_min=3e3,  m_local_max=8e3  # MB\n",
    ")\n",
    "BASE_TASK_DIST = TaskFeatureDist()\n",
    "\n",
    "SCENARIOS: List[GlobalConfig] = [\n",
    "    GlobalConfig(\n",
    "        name=\"light\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 101),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.01, lam_sec_max=0.05),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=2.0,  b_sigma_g=1.55,\n",
    "            rho_median=1.0e9, rho_sigma_g=1.45,\n",
    "            mem_median=64.0, mem_sigma_g=1.40,\n",
    "            p_deadline=0.15, deadline_min=0.8, deadline_max=2.0,\n",
    "            p_non_atomic=0.25, split_ratio_min=0.25, split_ratio_max=0.75\n",
    "        )\n",
    "    ),\n",
    "    GlobalConfig(\n",
    "        name=\"moderate\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 202),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.05, lam_sec_max=0.20),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=3.0,  b_sigma_g=1.60,\n",
    "            rho_median=1.2e9, rho_sigma_g=1.50,\n",
    "            mem_median=64.0, mem_sigma_g=1.45,\n",
    "            p_deadline=0.25, deadline_min=0.5, deadline_max=1.5,\n",
    "            p_non_atomic=0.35, split_ratio_min=0.30, split_ratio_max=0.80\n",
    "        )\n",
    "    ),\n",
    "    GlobalConfig(\n",
    "        name=\"heavy\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 303),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.20, lam_sec_max=0.80),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=5.0,  b_sigma_g=1.70,\n",
    "            rho_median=1.5e9, rho_sigma_g=1.55,\n",
    "            mem_median=64.0, mem_sigma_g=1.50,\n",
    "            p_deadline=0.35, deadline_min=0.3, deadline_max=1.0,\n",
    "            p_non_atomic=0.45, split_ratio_min=0.40, split_ratio_max=0.85\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# drivers: per-scenario & all-scenarios\n",
    "# -------------------------\n",
    "def main_generate_for_scenario(\n",
    "    cfg: GlobalConfig,\n",
    "    agents: List[Agent],\n",
    "    episode_id: int,\n",
    "    ep_dir: str,\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate ONE episode for ONE scenario under:\n",
    "        ep_dir/<scenario>/\n",
    "    For example: datasets/ep_000/heavy/\n",
    "    \"\"\"\n",
    "    _validate_cfg(cfg)\n",
    "\n",
    "    scenario_dir = os.path.join(ep_dir, cfg.name)\n",
    "    os.makedirs(scenario_dir, exist_ok=True)\n",
    "\n",
    "    dfs = run_episode(cfg, agents, episode_id=episode_id, qcap=qcap)\n",
    "    paths = save_dataset(dfs, out_dir=scenario_dir)\n",
    "    summarize_and_plot(dfs, out_dir=scenario_dir)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def generate_all_scenarios(\n",
    "    episodes_each: int = 1,\n",
    "    out_root: str = \"./datasets\",\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Layout:\n",
    "        out_root/ep_000/<scenario>/\n",
    "        out_root/ep_001/<scenario>/\n",
    "        ...\n",
    "    And inside each ep_xxx we store dataset_metadata.json\n",
    "    with info for ALL scenarios.\n",
    "\n",
    "    NOTE:\n",
    "      - To fully mimic HOODIE training (5000 episodes), call:\n",
    "            generate_all_scenarios(episodes_each=5000, ...)\n",
    "      - Default episodes_each=1 is for debugging.\n",
    "    \"\"\"\n",
    "    results: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "    # For stability, build agents per scenario once (same pool across episodes for that scenario)\n",
    "    agents_per_scenario: Dict[str, List[Agent]] = {}\n",
    "    for cfg in SCENARIOS:\n",
    "        rng_agents = np.random.default_rng(cfg.Episode.seed + 10_000)\n",
    "        agents_per_scenario[cfg.name] = build_agents(cfg, rng_agents)\n",
    "\n",
    "    for ep in range(episodes_each):\n",
    "        ep_name = f\"ep_{ep:03d}\"\n",
    "        ep_dir = os.path.join(out_root, ep_name)\n",
    "        os.makedirs(ep_dir, exist_ok=True)\n",
    "\n",
    "        episode_paths: Dict[str, str] = {}\n",
    "        for cfg in SCENARIOS:\n",
    "            agents = agents_per_scenario[cfg.name]\n",
    "            paths = main_generate_for_scenario(\n",
    "                cfg=cfg,\n",
    "                agents=agents,\n",
    "                episode_id=ep,\n",
    "                ep_dir=ep_dir,\n",
    "                qcap=qcap\n",
    "            )\n",
    "            # keys like: heavy_episodes_csv, heavy_tasks_csv, ...\n",
    "            for k, v in paths.items():\n",
    "                episode_paths[f\"{cfg.name}_{k}\"] = v\n",
    "\n",
    "        # per-episode metadata (ALL scenarios)\n",
    "        meta_path = save_episode_meta(SCENARIOS, ep_dir=ep_dir, qcap=qcap)\n",
    "        episode_paths[\"metadata_json\"] = meta_path\n",
    "\n",
    "        results[ep_name] = episode_paths\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# sanity checks\n",
    "# -------------------------\n",
    "def sanity_check_episode(ep_dir: str, scenario_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Lightweight sanity checks for one ep_xxx:\n",
    "      - basic file presence\n",
    "      - tasks vs arrivals count & task_id consistency\n",
    "      - agent_id consistency\n",
    "      - basic value ranges (positivity, deadlines, split_ratio)\n",
    "      - rough consistency between lambda and realized tasks/hour (on decision slots)\n",
    "    \"\"\"\n",
    "    print(f\"[sanity] Checking episode directory: {ep_dir}\")\n",
    "    meta_path = os.path.join(ep_dir, \"dataset_metadata.json\")\n",
    "    if not os.path.isfile(meta_path):\n",
    "        print(f\"  [WARN] No dataset_metadata.json found in {ep_dir}\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            print(f\"  [OK] Loaded metadata with {len(meta.get('scenarios', []))} scenarios\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to read metadata: {e}\")\n",
    "\n",
    "    for scen in scenario_names:\n",
    "        scen_dir = os.path.join(ep_dir, scen)\n",
    "        if not os.path.isdir(scen_dir):\n",
    "            print(f\"  [WARN] Scenario dir missing: {scen_dir}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            episodes_df = pd.read_csv(os.path.join(scen_dir, \"episodes.csv\"))\n",
    "            agents_df   = pd.read_csv(os.path.join(scen_dir, \"agents.csv\"))\n",
    "            arrivals_df = pd.read_csv(os.path.join(scen_dir, \"arrivals.csv\"))\n",
    "            tasks_df    = pd.read_csv(os.path.join(scen_dir, \"tasks.csv\"))\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to load CSVs for scenario '{scen}': {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  [scenario={scen}] n_tasks={len(tasks_df)}, n_arrivals={len(arrivals_df)}, n_agents={len(agents_df)}\")\n",
    "\n",
    "        # 1) non-empty checks\n",
    "        if len(tasks_df) == 0 or len(arrivals_df) == 0:\n",
    "            print(f\"    [WARN] Empty tasks or arrivals for scenario '{scen}'\")\n",
    "            continue\n",
    "\n",
    "        # 2) tasks vs arrivals counts & unique task_ids\n",
    "        if len(tasks_df) != len(arrivals_df):\n",
    "            print(f\"    [WARN] tasks ({len(tasks_df)}) != arrivals ({len(arrivals_df)})\")\n",
    "\n",
    "        if tasks_df[\"task_id\"].nunique() != len(tasks_df):\n",
    "            print(f\"    [WARN] Duplicate task_id in tasks.csv for scenario '{scen}'\")\n",
    "\n",
    "        # 3) agent_id consistency\n",
    "        agents_set = set(agents_df[\"agent_id\"].tolist())\n",
    "        arr_agents = set(arrivals_df[\"agent_id\"].tolist())\n",
    "        task_agents = set(tasks_df[\"agent_id\"].tolist())\n",
    "        if not arr_agents.issubset(agents_set):\n",
    "            print(f\"    [WARN] Some arrival agent_ids not in agents table for '{scen}'\")\n",
    "        if not task_agents.issubset(agents_set):\n",
    "            print(f\"    [WARN] Some task agent_ids not in agents table for '{scen}'\")\n",
    "\n",
    "        # 4) basic value ranges\n",
    "        if (tasks_df[\"b_mb\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive b_mb values in tasks for '{scen}'\")\n",
    "        if (tasks_df[\"mem_mb\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive mem_mb values in tasks for '{scen}'\")\n",
    "        if (tasks_df[\"c_cycles\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive c_cycles values in tasks for '{scen}'\")\n",
    "\n",
    "        # deadlines: deadline_time >= t_arrival_time when has_deadline\n",
    "        with_deadline = tasks_df[\"has_deadline\"] == 1\n",
    "        if with_deadline.any():\n",
    "            bad_deadlines = (tasks_df.loc[with_deadline, \"deadline_time\"] <\n",
    "                             tasks_df.loc[with_deadline, \"t_arrival_time\"]).sum()\n",
    "            if bad_deadlines > 0:\n",
    "                print(f\"    [WARN] {bad_deadlines} rows with deadline_time < t_arrival_time in '{scen}'\")\n",
    "\n",
    "        # split ratio range\n",
    "        if ((tasks_df[\"split_ratio\"] < 0) | (tasks_df[\"split_ratio\"] > 1)).any():\n",
    "            print(f\"    [WARN] split_ratio out of [0,1] range in '{scen}'\")\n",
    "        # non_atomic=0 => split_ratio==0\n",
    "        non_atomic_zero = tasks_df[\"non_atomic\"] == 0\n",
    "        if (tasks_df.loc[non_atomic_zero, \"split_ratio\"] != 0).any():\n",
    "            print(f\"    [WARN] non_atomic==0 but split_ratio != 0 in '{scen}'\")\n",
    "\n",
    "        # 5) rough lambda consistency check (using decision horizon length)\n",
    "        if len(episodes_df):\n",
    "            hours = float(episodes_df.iloc[0][\"T_decision\"] * episodes_df.iloc[0][\"Delta\"]) / 3600.0\n",
    "            if hours > 0:\n",
    "                # arrivals only exist in decision slots by construction\n",
    "                tasks_per_agent = arrivals_df.groupby(\"agent_id\").size() / hours\n",
    "                merged = pd.merge(\n",
    "                    agents_df[[\"agent_id\", \"lam_sec\"]],\n",
    "                    tasks_per_agent.rename(\"tasks_per_hour\").reset_index(),\n",
    "                    on=\"agent_id\",\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                merged[\"lam_per_hour\"] = merged[\"lam_sec\"] * 3600.0\n",
    "                merged[\"ratio\"] = merged[\"tasks_per_hour\"] / merged[\"lam_per_hour\"]\n",
    "                too_low = (merged[\"ratio\"] < 0.4).sum()\n",
    "                too_high = (merged[\"ratio\"] > 1.6).sum()\n",
    "                if too_low + too_high > 0:\n",
    "                    print(f\"    [INFO] Poisson check: some agents have realized rate far from expected \"\n",
    "                          f\"(too_low={too_low}, too_high={too_high}) in '{scen}'\")\n",
    "\n",
    "def sanity_check_root(out_root: str, scenario_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Run sanity checks over all ep_* folders under out_root.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(out_root):\n",
    "        print(f\"[sanity] Root directory '{out_root}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    episodes = sorted([d for d in os.listdir(out_root) if d.startswith(\"ep_\")])\n",
    "    if not episodes:\n",
    "        print(f\"[sanity] No ep_* folders found under '{out_root}'.\")\n",
    "        return\n",
    "\n",
    "    for ep_name in episodes:\n",
    "        ep_dir = os.path.join(out_root, ep_name)\n",
    "        if os.path.isdir(ep_dir):\n",
    "            sanity_check_episode(ep_dir, scenario_names)\n",
    "\n",
    "# -------------------------\n",
    "# main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    out_root = \"./datasets\"\n",
    "    # For testing: 1 episode; to get closer to the original HOODIE:\n",
    "    #   generate_all_scenarios(episodes_each=5000, ...)\n",
    "    results = generate_all_scenarios(episodes_each=1, out_root=out_root, qcap=0.99)\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "    # basic sanity checks\n",
    "    scenario_names = [cfg.name for cfg in SCENARIOS]\n",
    "    sanity_check_root(out_root, scenario_names=scenario_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"light\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\light\\\\ep_000\\\\episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\light\\\\ep_000\\\\agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\light\\\\ep_000\\\\arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\light\\\\ep_000\\\\tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\light\\\\dataset_meta.json\"\n",
      "  },\n",
      "  \"moderate\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\moderate\\\\ep_000\\\\episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\moderate\\\\ep_000\\\\agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\moderate\\\\ep_000\\\\arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\moderate\\\\ep_000\\\\tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\moderate\\\\dataset_meta.json\"\n",
      "  },\n",
      "  \"heavy\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\heavy\\\\ep_000\\\\episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\heavy\\\\ep_000\\\\agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\heavy\\\\ep_000\\\\arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\heavy\\\\ep_000\\\\tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\heavy\\\\dataset_meta.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Data generator for Edge–MEC–Cloud with Poisson arrivals (per-second),\n",
    "# lognormal task features (parameterized by median & sigma_g), and policy-agnostic outputs.\n",
    "\n",
    "# Now supports THREE SCENARIOS (light / moderate / heavy) similar to HOODIE experiments.\n",
    "# For each scenario we:\n",
    "#   - synthesize time-stamped arrivals and task features for one or more episodes\n",
    "#   - save CSV + a dataset_meta.json\n",
    "#   - plot distribution figures (PNG) for key variables\n",
    "#   - export a summary_stats.csv with quantiles/means\n",
    "\n",
    "# Fixes/Improvements:\n",
    "# - Per-episode subfolders (avoid overwrite) → ./datasets/<scenario>/ep_XXX/\n",
    "# - Flexible lognormal quantile with small z-table + clamp\n",
    "# - Parameter asserts & unit consistency\n",
    "# - Richer summary (p90/p99, modality dist, tasks/hour)\n",
    "# - Optional modality probabilities in TaskFeatureDist\n",
    "# - Dtype optimization for large datasets\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "# from dataclasses import dataclass, asdict, replace\n",
    "# from typing import List, Dict, Tuple, Optional\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random, math, os, json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import hashlib, time, platform, getpass\n",
    "\n",
    "# # -------------------------\n",
    "# # reproducibility\n",
    "# # -------------------------\n",
    "# GLOBAL_SEED = 42\n",
    "# rng_global = np.random.default_rng(GLOBAL_SEED)\n",
    "# random.seed(GLOBAL_SEED)\n",
    "# np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# # -------------------------\n",
    "# # configuration dataclasses\n",
    "# # -------------------------\n",
    "# @dataclass\n",
    "# class EpisodeConf:\n",
    "#     Delta: float      # seconds per slot\n",
    "#     T_slots: int      # number of slots in the episode\n",
    "#     seed: int\n",
    "\n",
    "# @dataclass\n",
    "# class AgentRanges:\n",
    "#     # arrival rate per SECOND (will be multiplied by Delta per slot)\n",
    "#     lam_sec_min: float\n",
    "#     lam_sec_max: float\n",
    "#     # optional local capacities (kept as meta for later)\n",
    "#     f_local_min: float\n",
    "#     f_local_max: float\n",
    "#     m_local_min: float\n",
    "#     m_local_max: float\n",
    "\n",
    "# @dataclass\n",
    "# class TaskFeatureDist:\n",
    "#     # Lognormal parameterization via median and sigma_g (geometric std).\n",
    "#     b_median: float = 3.0          # MB\n",
    "#     b_sigma_g: float = 0.6\n",
    "\n",
    "#     rho_median: float = 1.2e9      # cycles / MB\n",
    "#     rho_sigma_g: float = 0.5\n",
    "\n",
    "#     mem_median: float = 64.0       # MB\n",
    "#     mem_sigma_g: float = 0.5\n",
    "\n",
    "#     p_deadline: float = 0.25\n",
    "#     deadline_min: float = 0.3      # seconds (relative)\n",
    "#     deadline_max: float = 1.5\n",
    "\n",
    "#     p_non_atomic: float = 0.35\n",
    "#     split_ratio_min: float = 0.30  # fraction of task size that CAN be split\n",
    "#     split_ratio_max: float = 0.80\n",
    "\n",
    "#     # Optional modality probabilities (image, video, text, sensor)\n",
    "#     modality_probs: Optional[List[float]] = None\n",
    "#     modality_labels: List[str] = None\n",
    "\n",
    "# @dataclass\n",
    "# class GlobalConfig:\n",
    "#     name: str\n",
    "#     N_agents: int\n",
    "#     Episode: EpisodeConf\n",
    "#     AgentRanges: AgentRanges\n",
    "#     TaskDist: TaskFeatureDist\n",
    "\n",
    "# # -------------------------\n",
    "# # asserts / validation\n",
    "# # -------------------------\n",
    "# def _validate_cfg(cfg: GlobalConfig) -> None:\n",
    "#     assert cfg.N_agents > 0\n",
    "#     assert cfg.Episode.Delta > 0 and cfg.Episode.T_slots > 0\n",
    "#     ar = cfg.AgentRanges\n",
    "#     assert ar.lam_sec_min >= 0 and ar.lam_sec_max >= ar.lam_sec_min\n",
    "#     td = cfg.TaskDist\n",
    "#     assert td.split_ratio_min > 0 and td.split_ratio_max <= 1.0 and td.split_ratio_max >= td.split_ratio_min\n",
    "#     assert td.deadline_min <= td.deadline_max\n",
    "#     # geometric std must be >= 1.0 (clamped later, but assert for awareness)\n",
    "#     assert td.b_sigma_g >= 1.0 and td.rho_sigma_g >= 1.0 and td.mem_sigma_g >= 1.0\n",
    "\n",
    "# # -------------------------\n",
    "# # helpers\n",
    "# # -------------------------\n",
    "# _Z_TABLE = {\n",
    "#     0.90: 1.2815515655446004,\n",
    "#     0.95: 1.6448536269514722,\n",
    "#     0.975: 1.959963984540054,\n",
    "#     0.99: 2.3263478740408408,\n",
    "#     0.995: 2.5758293035489004,\n",
    "#     0.999: 3.090232306167813\n",
    "# }\n",
    "\n",
    "# def _z_from_p(p: float) -> float:\n",
    "#     # Use small table + nearest clamp (no SciPy dependency)\n",
    "#     if p in _Z_TABLE: return _Z_TABLE[p]\n",
    "#     # clamp to nearest key\n",
    "#     return _Z_TABLE[min(_Z_TABLE.keys(), key=lambda k: abs(k - p))]\n",
    "\n",
    "# def lognormal_quantile(median: float, sigma_g: float, p: float) -> float:\n",
    "#     # X ~ LogNormal(mu, sigma) with median=exp(mu), sigma_g=exp(sigma)\n",
    "#     # quantile(p) = median * exp( z_p * ln(sigma_g) )\n",
    "#     z = _z_from_p(p)\n",
    "#     return median * math.exp(z * math.log(max(sigma_g, 1.0 + 1e-6)))\n",
    "\n",
    "# def lognormal_from_median_sigma_g(rng, median: float, sigma_g: float, qcap: float | None = 0.99) -> float:\n",
    "#     \"\"\"\n",
    "#     Draw from LogNormal with given median and geometric std:\n",
    "#       X ~ LogNormal(mu, sigma) where median = exp(mu), sigma_g = exp(sigma).\n",
    "#       => mu = ln(median), sigma = ln(sigma_g)\n",
    "#     \"\"\"\n",
    "#     mu = math.log(max(median, 1e-12))\n",
    "#     sigma = math.log(max(sigma_g, 1.0 + 1e-6))\n",
    "#     x = float(rng.lognormal(mean=mu, sigma=sigma))\n",
    "#     if qcap is not None:\n",
    "#         cap = lognormal_quantile(median, sigma_g, qcap)\n",
    "#         x = min(x, cap)\n",
    "#     return x\n",
    "\n",
    "# # -------------------------\n",
    "# # entities\n",
    "# # -------------------------\n",
    "# @dataclass\n",
    "# class Agent:\n",
    "#     agent_id: int\n",
    "#     f_local: float\n",
    "#     m_local: float\n",
    "#     lam_sec: float   # Poisson rate per second (not per-slot)\n",
    "\n",
    "# def build_agents(cfg: GlobalConfig, rng: np.random.Generator) -> List[Agent]:\n",
    "#     agents: List[Agent] = []\n",
    "#     for i in range(cfg.N_agents):\n",
    "#         lam_sec = rng.uniform(cfg.AgentRanges.lam_sec_min, cfg.AgentRanges.lam_sec_max)\n",
    "#         f_loc   = rng.uniform(cfg.AgentRanges.f_local_min, cfg.AgentRanges.f_local_max)\n",
    "#         m_loc   = rng.uniform(cfg.AgentRanges.m_local_min, cfg.AgentRanges.m_local_max)\n",
    "#         agents.append(Agent(agent_id=i, f_local=f_loc, m_local=m_loc, lam_sec=lam_sec))\n",
    "#     return agents\n",
    "\n",
    "# # -------------------------\n",
    "# # task features\n",
    "# # -------------------------\n",
    "# def _modality_choice(rng: np.random.Generator, d: TaskFeatureDist) -> str:\n",
    "#     labels = d.modality_labels or [\"image\",\"video\",\"text\",\"sensor\"]\n",
    "#     if d.modality_probs is None:\n",
    "#         probs = [0.3, 0.2, 0.3, 0.2]\n",
    "#     else:\n",
    "#         probs = d.modality_probs\n",
    "#         assert abs(sum(probs) - 1.0) < 1e-6 and len(probs) == len(labels)\n",
    "#     return rng.choice(labels, p=probs)\n",
    "\n",
    "# def sample_task_features(cfg: GlobalConfig, rng: np.random.Generator, qcap: float = 0.99) -> Dict[str, float]:\n",
    "#     d = cfg.TaskDist\n",
    "#     b_mb   = lognormal_from_median_sigma_g(rng, d.b_median,   d.b_sigma_g,   qcap=qcap)\n",
    "#     rho    = lognormal_from_median_sigma_g(rng, d.rho_median, d.rho_sigma_g, qcap=qcap)\n",
    "#     c      = b_mb * rho                              # total cycles\n",
    "#     mem_mb = lognormal_from_median_sigma_g(rng, d.mem_median, d.mem_sigma_g, qcap=qcap)\n",
    "#     modality = _modality_choice(rng, d)\n",
    "#     has_deadline = int(rng.random() < d.p_deadline)\n",
    "#     deadline_s   = np.nan\n",
    "#     if has_deadline:\n",
    "#         deadline_s = float(rng.uniform(d.deadline_min, d.deadline_max))\n",
    "#     non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "#     split_ratio = float(rng.uniform(d.split_ratio_min, d.split_ratio_max)) if non_atomic else 0.0\n",
    "    \n",
    "#     return dict(\n",
    "#         b_mb=b_mb, rho=rho, c_cycles=c, mem_mb=mem_mb, modality=modality,\n",
    "#         has_deadline=has_deadline, deadline_s=deadline_s, non_atomic=non_atomic, split_ratio=split_ratio\n",
    "#     )\n",
    "\n",
    "# # -------------------------\n",
    "# # episode generator (arrivals only)\n",
    "# # -------------------------\n",
    "# def run_episode(cfg: GlobalConfig, agents: List[Agent], episode_id: int = 0, qcap: float = 0.99) -> Dict[str, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Generate time-stamped arrivals and task features for one episode.\n",
    "#     \"\"\"\n",
    "#     _validate_cfg(cfg)\n",
    "#     rng_local = np.random.default_rng(cfg.Episode.seed + episode_id)\n",
    "\n",
    "#     rows_episodes: List[Dict] = []\n",
    "#     rows_agents:   List[Dict] = [asdict(a) for a in agents]\n",
    "#     rows_arrivals: List[Dict] = []\n",
    "#     rows_tasks:    List[Dict] = []\n",
    "\n",
    "#     Delta   = cfg.Episode.Delta\n",
    "#     T_slots = cfg.Episode.T_slots\n",
    "\n",
    "#     task_id_counter = 0\n",
    "\n",
    "#     for t in range(T_slots):\n",
    "#         t_time = t * Delta\n",
    "#         for a in agents:\n",
    "#             # per-slot rate from per-second rate:\n",
    "#             lam_slot = a.lam_sec * Delta\n",
    "#             n_new = rng_local.poisson(lam=lam_slot)\n",
    "#             if n_new <= 0:\n",
    "#                 continue\n",
    "#             for _ in range(n_new):\n",
    "#                 feat = sample_task_features(cfg, rng_local, qcap=qcap)\n",
    "\n",
    "#                 # absolute deadline time (NaN if none)\n",
    "#                 if np.isnan(feat[\"deadline_s\"]):\n",
    "#                     deadline_time = np.nan\n",
    "#                 else:\n",
    "#                     deadline_time = t_time + feat[\"deadline_s\"]\n",
    "\n",
    "#                 action_space_hint = \"continuous\" if feat[\"non_atomic\"] == 1 else \"discrete\"\n",
    "\n",
    "#                 rows_arrivals.append({\n",
    "#                     \"scenario\": cfg.name,\n",
    "#                     \"episode_id\": episode_id,\n",
    "#                     \"t_slot\": t,\n",
    "#                     \"t_time\": t_time,\n",
    "#                     \"agent_id\": a.agent_id,\n",
    "#                     \"task_id\": task_id_counter\n",
    "#                 })\n",
    "\n",
    "#                 rows_tasks.append({\n",
    "#                     \"scenario\": cfg.name,\n",
    "#                     \"episode_id\": episode_id,\n",
    "#                     \"task_id\": task_id_counter,\n",
    "#                     \"agent_id\": a.agent_id,\n",
    "#                     \"t_arrival_slot\": t,\n",
    "#                     \"t_arrival_time\": t_time,\n",
    "#                     \"b_mb\": feat[\"b_mb\"],\n",
    "#                     \"rho_cyc_per_mb\": feat[\"rho\"],\n",
    "#                     \"c_cycles\": feat[\"c_cycles\"],\n",
    "#                     \"mem_mb\": feat[\"mem_mb\"],\n",
    "#                     \"modality\": feat[\"modality\"],\n",
    "#                     \"has_deadline\": feat[\"has_deadline\"],\n",
    "#                     \"deadline_s\": feat[\"deadline_s\"],\n",
    "#                     \"deadline_time\": deadline_time,\n",
    "#                     \"non_atomic\": feat[\"non_atomic\"],\n",
    "#                     \"split_ratio\": feat[\"split_ratio\"],\n",
    "#                     \"action_space_hint\": action_space_hint\n",
    "#                 })\n",
    "\n",
    "#                 task_id_counter += 1\n",
    "\n",
    "#     rows_episodes.append({\n",
    "#         \"scenario\": cfg.name,\n",
    "#         \"episode_id\": episode_id,\n",
    "#         \"Delta\": Delta,\n",
    "#         \"T_slots\": T_slots,\n",
    "#         \"hours\": T_slots * Delta / 3600.0,\n",
    "#         \"N_agents\": len(agents),\n",
    "#         \"seed\": cfg.Episode.seed + episode_id\n",
    "#     })\n",
    "\n",
    "#     episodes_df = pd.DataFrame(rows_episodes)\n",
    "#     agents_df   = pd.DataFrame(rows_agents)\n",
    "#     arrivals_df = pd.DataFrame(rows_arrivals)\n",
    "#     tasks_df    = pd.DataFrame(rows_tasks)\n",
    "\n",
    "#     # Optimize dtypes (optional but useful)\n",
    "#     if len(tasks_df):\n",
    "#         tasks_df[\"modality\"] = tasks_df[\"modality\"].astype(\"category\")\n",
    "#         tasks_df[\"action_space_hint\"] = tasks_df[\"action_space_hint\"].astype(\"category\")\n",
    "#         # ints\n",
    "#         for col in [\"episode_id\",\"task_id\",\"agent_id\",\"t_arrival_slot\",\"has_deadline\",\"non_atomic\"]:\n",
    "#             if col in tasks_df:\n",
    "#                 tasks_df[col] = tasks_df[col].astype(\"int32\")\n",
    "#         # floats\n",
    "#         for col in [\"t_arrival_time\",\"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\",\"deadline_s\",\"deadline_time\",\"split_ratio\"]:\n",
    "#             if col in tasks_df:\n",
    "#                 tasks_df[col] = tasks_df[col].astype(\"float32\")\n",
    "\n",
    "#     if len(arrivals_df):\n",
    "#         for col in [\"episode_id\",\"t_slot\",\"agent_id\",\"task_id\"]:\n",
    "#             if col in arrivals_df:\n",
    "#                 arrivals_df[col] = arrivals_df[col].astype(\"int32\")\n",
    "#         for col in [\"t_time\"]:\n",
    "#             if col in arrivals_df:\n",
    "#                 arrivals_df[col] = arrivals_df[col].astype(\"float32\")\n",
    "\n",
    "#     if len(agents_df):\n",
    "#         for col in [\"agent_id\"]:\n",
    "#             agents_df[col] = agents_df[col].astype(\"int32\")\n",
    "#         for col in [\"f_local\",\"m_local\",\"lam_sec\"]:\n",
    "#             agents_df[col] = agents_df[col].astype(\"float64\")\n",
    "\n",
    "#     return {\n",
    "#         \"episodes\": episodes_df,\n",
    "#         \"agents\":   agents_df,\n",
    "#         \"arrivals\": arrivals_df,\n",
    "#         \"tasks\":    tasks_df\n",
    "#     }\n",
    "\n",
    "# # -------------------------\n",
    "# # save & plotting utilities\n",
    "# # -------------------------\n",
    "# def save_dataset(dfs: Dict[str, pd.DataFrame], out_dir: str = \".\") -> Dict[str, str]:\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "#     paths: Dict[str, str] = {}\n",
    "#     for name, df in dfs.items():\n",
    "#         csv_path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "#         df.to_csv(csv_path, index=False)\n",
    "#         paths[name + \"_csv\"] = csv_path\n",
    "#     return paths\n",
    "\n",
    "# def _config_fingerprint(cfg: GlobalConfig) -> str:\n",
    "#     s = json.dumps({\n",
    "#         \"scenario\": cfg.name,\n",
    "#         \"Episode\": asdict(cfg.Episode),\n",
    "#         \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "#         \"TaskDist\": asdict(cfg.TaskDist)\n",
    "#     }, sort_keys=True).encode(\"utf-8\")\n",
    "#     return hashlib.sha256(s).hexdigest()[:16]\n",
    "\n",
    "# def save_meta(cfg: GlobalConfig, out_dir: str = \".\", qcap: float = 0.99) -> str:\n",
    "#     meta = {\n",
    "#         \"schema_version\": \"1.0.0\",\n",
    "#         \"scenario\": cfg.name,\n",
    "#         \"seed\": cfg.Episode.seed,\n",
    "#         \"fingerprint\": _config_fingerprint(cfg),\n",
    "#         \"generated_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "#         \"env\": {\"python\": platform.python_version(), \"user\": getpass.getuser()},\n",
    "#         \"Episode\": asdict(cfg.Episode),\n",
    "#         \"N_agents\": cfg.N_agents,\n",
    "#         \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "#         \"TaskDist\": asdict(cfg.TaskDist),\n",
    "#         \"units\": {\n",
    "#             \"Delta\": \"seconds\",\n",
    "#             \"lam_sec\": \"tasks per second (per agent)\",\n",
    "#             \"per_slot_rate\": \"lam_sec * Delta\",\n",
    "#             \"b_mb\": \"MB\",\n",
    "#             \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
    "#             \"c_cycles\": \"CPU cycles\",\n",
    "#             \"mem_mb\": \"MB\",\n",
    "#             \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\",\n",
    "#             \"f_local\": \"Hz\",\n",
    "#             \"m_local\": \"MB\"\n",
    "#         },\n",
    "#         \"notes\": {\n",
    "#             \"policy_agnostic\": True,\n",
    "#             \"queueing\": \"not simulated here\",\n",
    "#             \"clipping\": {\n",
    "#                 \"enabled\": True,\n",
    "#                 \"method\": \"lognormal analytic quantile cap\",\n",
    "#                 \"qcap\": qcap,\n",
    "#                 \"z_table_keys\": sorted(list(_Z_TABLE.keys()))\n",
    "#             },\n",
    "#             \"action_space_hint\": \"derived from non_atomic; final decision belongs to environment\"\n",
    "#         }\n",
    "#     }\n",
    "#     path = os.path.join(out_dir, f\"dataset_meta.json\")\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "#     return path\n",
    "\n",
    "# def summarize_and_plot(dfs: Dict[str, pd.DataFrame], out_dir: str) -> None:\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "#     tasks = dfs[\"tasks\"].copy()\n",
    "#     arrivals = dfs[\"arrivals\"].copy()\n",
    "#     episodes = dfs[\"episodes\"].copy()\n",
    "\n",
    "#     # ---- summary stats\n",
    "#     def q(series: pd.Series, p: float):\n",
    "#         return float(np.nanquantile(series.dropna(), p)) if len(series.dropna()) else float(\"nan\")\n",
    "\n",
    "#     tasks_per_hour = float(len(tasks)) / float(episodes.iloc[0][\"hours\"]) if len(episodes) and episodes.iloc[0][\"hours\"] > 0 else float(\"nan\")\n",
    "\n",
    "#     # modality distribution\n",
    "#     mod_counts = tasks[\"modality\"].value_counts(dropna=False) if len(tasks) else pd.Series(dtype=int)\n",
    "#     mod_dist = {str(k): int(v) for k, v in mod_counts.to_dict().items()}\n",
    "\n",
    "#     summary = {\n",
    "#         \"n_tasks\": [len(tasks)],\n",
    "#         \"n_arrivals\": [len(arrivals)],\n",
    "#         \"tasks_per_hour\": [tasks_per_hour],\n",
    "#         \"b_mb_median\": [q(tasks[\"b_mb\"], 0.5)],\n",
    "#         \"b_mb_p90\": [q(tasks[\"b_mb\"], 0.9)],\n",
    "#         \"b_mb_p99\": [q(tasks[\"b_mb\"], 0.99)],\n",
    "#         \"rho_median\": [q(tasks[\"rho_cyc_per_mb\"], 0.5)],\n",
    "#         \"rho_p90\": [q(tasks[\"rho_cyc_per_mb\"], 0.9)],\n",
    "#         \"c_cycles_median\": [q(tasks[\"c_cycles\"], 0.5)],\n",
    "#         \"c_cycles_p90\": [q(tasks[\"c_cycles\"], 0.9)],\n",
    "#         \"c_cycles_p99\": [q(tasks[\"c_cycles\"], 0.99)],\n",
    "#         \"deadline_share\": [float((tasks[\"has_deadline\"]==1).mean()) if len(tasks) else float(\"nan\")],\n",
    "#         \"non_atomic_share\": [float((tasks[\"non_atomic\"]==1).mean()) if len(tasks) else float(\"nan\")],\n",
    "#         \"modality_counts_json\": [json.dumps(mod_dist)]\n",
    "#     }\n",
    "#     pd.DataFrame(summary).to_csv(os.path.join(out_dir, f\"summary_stats.csv\"), index=False)\n",
    "\n",
    "#     # ---- plots (each in its own figure)\n",
    "#     def hist_plot(series: pd.Series, title: str, fname: str, logx: bool=False):\n",
    "#         plt.figure()\n",
    "#         s = series.dropna()\n",
    "#         if len(s) == 0:\n",
    "#             plt.title(title + \" (no data)\")\n",
    "#         else:\n",
    "#             plt.hist(s, bins=50)\n",
    "#             if logx:\n",
    "#                 plt.xscale('log')\n",
    "#             plt.title(title)\n",
    "#             plt.xlabel(title)\n",
    "#             plt.ylabel(\"count\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(out_dir, fname), dpi=160)\n",
    "#         plt.close()\n",
    "\n",
    "#     hist_plot(tasks[\"b_mb\"],            title=\"Task size (MB)\",                fname=f\"hist_b_mb.png\", logx=True)\n",
    "#     hist_plot(tasks[\"rho_cyc_per_mb\"],  title=\"Compute density (cycles/MB)\",   fname=f\"hist_rho.png\",  logx=True)\n",
    "#     hist_plot(tasks[\"c_cycles\"],        title=\"Total cycles\",                  fname=f\"hist_c_cycles.png\", logx=True)\n",
    "#     hist_plot(tasks[\"deadline_s\"],      title=\"Deadline (s)\",                  fname=f\"hist_deadline_s.png\", logx=False)\n",
    "#     hist_plot(tasks.loc[tasks[\"non_atomic\"]==1, \"split_ratio\"], title=\"Split ratio (only non-atomic)\", fname=f\"hist_split_ratio.png\", logx=False)\n",
    "\n",
    "#     # arrivals per agent\n",
    "#     if len(arrivals):\n",
    "#         per_agent = arrivals.groupby(\"agent_id\").size()\n",
    "#         plt.figure()\n",
    "#         plt.bar(per_agent.index.astype(str), per_agent.values)\n",
    "#         plt.title(\"Arrivals per agent\")\n",
    "#         plt.xlabel(\"agent_id\")\n",
    "#         plt.ylabel(\"count\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(out_dir, f\"bar_arrivals_per_agent.png\"), dpi=160)\n",
    "#         plt.close()\n",
    "\n",
    "# # -------------------------\n",
    "# # scenario presets (light / moderate / heavy)\n",
    "# # -------------------------\n",
    "# HOURS = 1\n",
    "# DEFAULT_DELTA = 1.0\n",
    "# DEFAULT_T_SLOTS = int(HOURS * 3600 / DEFAULT_DELTA)\n",
    "\n",
    "# BASE_EPISODE = EpisodeConf(Delta=DEFAULT_DELTA, T_slots=DEFAULT_T_SLOTS, seed=GLOBAL_SEED)\n",
    "# BASE_AGENT_RANGES = AgentRanges(\n",
    "#     lam_sec_min=0.02, lam_sec_max=0.80,\n",
    "#     f_local_min=0.8e9, f_local_max=2.4e9,\n",
    "#     m_local_min=3e3,  m_local_max=8e3 # MB\n",
    "# )\n",
    "# BASE_TASK_DIST = TaskFeatureDist()\n",
    "\n",
    "# SCENARIOS: List[GlobalConfig] = [\n",
    "#     GlobalConfig(\n",
    "#         name=\"light\",\n",
    "#         N_agents=18,\n",
    "#         Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 101),\n",
    "#         AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.01, lam_sec_max=0.05),\n",
    "#         TaskDist=replace(BASE_TASK_DIST,\n",
    "#             b_median=2.0,  b_sigma_g=1.55,\n",
    "#             rho_median=1.0e9, rho_sigma_g=1.45,\n",
    "#             mem_median=64.0, mem_sigma_g=1.40,\n",
    "#             p_deadline=0.15, deadline_min=0.8, deadline_max=2.0,\n",
    "#             p_non_atomic=0.25, split_ratio_min=0.25, split_ratio_max=0.75)\n",
    "#     ),\n",
    "#     GlobalConfig(\n",
    "#         name=\"moderate\",\n",
    "#         N_agents=18,\n",
    "#         Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 202),\n",
    "#         AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.05, lam_sec_max=0.20),\n",
    "#         TaskDist=replace(BASE_TASK_DIST,\n",
    "#             b_median=3.0,  b_sigma_g=1.60,\n",
    "#             rho_median=1.2e9, rho_sigma_g=1.50,\n",
    "#             mem_median=64.0, mem_sigma_g=1.45,\n",
    "#             p_deadline=0.25, deadline_min=0.5, deadline_max=1.5,\n",
    "#             p_non_atomic=0.35, split_ratio_min=0.30, split_ratio_max=0.80)\n",
    "#     ),\n",
    "#     GlobalConfig(\n",
    "#         name=\"heavy\",\n",
    "#         N_agents=18,\n",
    "#         Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 303),\n",
    "#         AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.20, lam_sec_max=0.80),\n",
    "#         TaskDist=replace(BASE_TASK_DIST,\n",
    "#             b_median=5.0,  b_sigma_g=1.70,\n",
    "#             rho_median=1.5e9, rho_sigma_g=1.55,\n",
    "#             mem_median=64.0, mem_sigma_g=1.50,\n",
    "#             p_deadline=0.35, deadline_min=0.3, deadline_max=1.0,\n",
    "#             p_non_atomic=0.45, split_ratio_min=0.40, split_ratio_max=0.85)\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # main driver\n",
    "# # -------------------------\n",
    "# def main_generate(cfg: GlobalConfig, episodes: int = 1, out_root: str = \"./datasets\", qcap: float = 0.99) -> Dict[str, str]:\n",
    "#     \"\"\"Generate 'episodes' episodes for one scenario (fixed agent pool per scenario).\"\"\"\n",
    "#     _validate_cfg(cfg)\n",
    "#     out_dir = os.path.join(out_root, cfg.name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # build agents once per scenario to keep them consistent across its episodes\n",
    "#     rng_agents = np.random.default_rng(cfg.Episode.seed + 10_000)\n",
    "#     agents = build_agents(cfg, rng_agents)\n",
    "\n",
    "#     all_paths: Dict[str, str] = {}\n",
    "#     for ep in range(episodes):\n",
    "#         ep_dir = os.path.join(out_dir, f\"ep_{ep:03d}\")\n",
    "#         os.makedirs(ep_dir, exist_ok=True)\n",
    "\n",
    "#         dfs = run_episode(cfg, agents, episode_id=ep, qcap=qcap)\n",
    "#         paths = save_dataset(dfs, out_dir=ep_dir)\n",
    "#         summarize_and_plot(dfs, out_dir=ep_dir)\n",
    "#         all_paths.update({f\"ep{ep}_{k}\": v for k, v in paths.items()})\n",
    "\n",
    "#     meta_path = save_meta(cfg, out_dir=out_dir, qcap=qcap)\n",
    "#     all_paths[\"meta\"] = meta_path\n",
    "#     return all_paths\n",
    "\n",
    "# def generate_all_scenarios(episodes_each: int = 1, out_root: str = \"./datasets\", qcap: float = 0.99) -> Dict[str, Dict[str, str]]:\n",
    "#     results: Dict[str, Dict[str, str]] = {}\n",
    "#     for cfg in SCENARIOS:\n",
    "#         results[cfg.name] = main_generate(cfg, episodes=episodes_each, out_root=out_root, qcap=qcap)\n",
    "#     return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     out = generate_all_scenarios(episodes_each=1, out_root=\"./datasets\")\n",
    "#     print(json.dumps(out, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
