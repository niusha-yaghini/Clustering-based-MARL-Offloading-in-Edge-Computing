{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference paper says that each episode consists of T = 110 timeslots (100 slots for decision making + 10 slots for clearing queues), the entire training was performed in 5000 episodes. DRL environments are usually event-driven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import platform\n",
    "import getpass\n",
    "import hashlib\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ep_000\": {\n",
      "    \"light_episodes_csv\": \"./datasets\\\\ep_000\\\\light\\\\episodes.csv\",\n",
      "    \"light_agents_csv\": \"./datasets\\\\ep_000\\\\light\\\\agents.csv\",\n",
      "    \"light_arrivals_csv\": \"./datasets\\\\ep_000\\\\light\\\\arrivals.csv\",\n",
      "    \"light_tasks_csv\": \"./datasets\\\\ep_000\\\\light\\\\tasks.csv\",\n",
      "    \"moderate_episodes_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\episodes.csv\",\n",
      "    \"moderate_agents_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\agents.csv\",\n",
      "    \"moderate_arrivals_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\arrivals.csv\",\n",
      "    \"moderate_tasks_csv\": \"./datasets\\\\ep_000\\\\moderate\\\\tasks.csv\",\n",
      "    \"heavy_episodes_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\episodes.csv\",\n",
      "    \"heavy_agents_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\agents.csv\",\n",
      "    \"heavy_arrivals_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\arrivals.csv\",\n",
      "    \"heavy_tasks_csv\": \"./datasets\\\\ep_000\\\\heavy\\\\tasks.csv\",\n",
      "    \"metadata_json\": \"./datasets\\\\ep_000\\\\dataset_metadata.json\"\n",
      "  }\n",
      "}\n",
      "[sanity] Checking episode directory: ./datasets\\ep_000\n",
      "  [OK] Loaded metadata with 3 scenarios\n",
      "  [scenario=light] n_tasks=63, n_arrivals=63, n_mecs=18\n",
      "    [INFO] Poisson check: some MECs have realized rate far from expected (too_low=2, too_high=3) in 'light'\n",
      "  [scenario=moderate] n_tasks=232, n_arrivals=232, n_mecs=18\n",
      "    [INFO] Poisson check: some MECs have realized rate far from expected (too_low=0, too_high=2) in 'moderate'\n",
      "  [scenario=heavy] n_tasks=838, n_arrivals=838, n_mecs=18\n",
      "    [INFO] Poisson check: some MECs have realized rate far from expected (too_low=0, too_high=1) in 'heavy'\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data generator for Edge–MEC–Cloud with Poisson arrivals (per-second),\n",
    "discrete task sizes (uniform integers in [task_size_min_mb, task_size_max_mb]),\n",
    "lognormal compute & memory features, and policy-agnostic outputs.\n",
    "\n",
    "HOODIE-style timing:\n",
    "  - Each episode has T slots:\n",
    "      * T_decision slots for decision-making (with arrivals)\n",
    "      * T_drain   slots for draining queues (NO new arrivals)\n",
    "  - DRL environment can later consume this as event-driven using the time-stamps.\n",
    "\n",
    "Supports THREE SCENARIOS (light / moderate / heavy).\n",
    "For each episode we:\n",
    "  - synthesize time-stamped arrivals and task features for ALL scenarios\n",
    "  - save CSV + a per-episode dataset_metadata.json\n",
    "  - plot distribution figures (PNG) for key variables\n",
    "  - export a summary_stats.csv with quantiles/means per scenario\n",
    "\n",
    "Directory layout (per episode):\n",
    "  datasets/\n",
    "    ep_000/\n",
    "      light/\n",
    "        episodes.csv\n",
    "        agents.csv        # represents MEC nodes; contains mec_id + lam_sec\n",
    "        arrivals.csv\n",
    "        tasks.csv\n",
    "        summary_stats.csv\n",
    "        *.png\n",
    "      moderate/\n",
    "        ...\n",
    "      heavy/\n",
    "        ...\n",
    "      dataset_metadata.json   # meta info for ALL scenarios in this episode\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Load configuration from JSON\n",
    "# ============================================================\n",
    "CONFIG_PATH = \"taskgen_config.json\"  # adjust path if needed\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "# Global seed and RNGs\n",
    "GLOBAL_SEED = CFG[\"global_seed\"]\n",
    "rng_global = np.random.default_rng(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Dataset-level config (used later in main)\n",
    "DATASET_CFG = CFG.get(\"dataset\", {})\n",
    "DATASET_EPISODES_EACH = DATASET_CFG.get(\"episodes_each\", 1)\n",
    "DATASET_OUT_ROOT = DATASET_CFG.get(\"out_root\", \"./datasets\")\n",
    "DATASET_QCAP = DATASET_CFG.get(\"qcap\", 0.99)\n",
    "\n",
    "# Task-level config (task size range)\n",
    "TASK_CFG = CFG[\"task\"]\n",
    "TASK_SIZE_MIN_MB = TASK_CFG[\"task_size_min_mb\"]\n",
    "TASK_SIZE_MAX_MB = TASK_CFG[\"task_size_max_mb\"]\n",
    "\n",
    "# ============================================================\n",
    "# configuration dataclasses\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class EpisodeConf:\n",
    "    Delta: float        # seconds per slot\n",
    "    T_slots: int        # total number of slots in the episode\n",
    "    T_decision: int     # number of slots with arrivals\n",
    "    T_drain: int        # number of drain slots without arrivals\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class AgentRanges:\n",
    "    # arrival rate per SECOND (will be multiplied by Delta per slot)\n",
    "    lam_sec_min: float\n",
    "    lam_sec_max: float\n",
    "\n",
    "@dataclass\n",
    "class TaskFeatureDist:\n",
    "    # Lognormal parameterization via median and sigma_g (geometric std).\n",
    "    b_median: float = 3.0          # MB (kept for metadata; actual size is discrete now)\n",
    "    b_sigma_g: float = 1.5\n",
    "\n",
    "    rho_median: float = 1.2e9      # cycles / MB\n",
    "    rho_sigma_g: float = 1.5\n",
    "\n",
    "    mem_median: float = 64.0       # MB\n",
    "    mem_sigma_g: float = 1.5\n",
    "\n",
    "    p_deadline: float = 0.25\n",
    "    deadline_min: float = 0.3      # seconds (relative)\n",
    "    deadline_max: float = 1.5\n",
    "\n",
    "    p_non_atomic: float = 0.35\n",
    "    split_ratio_min: float = 0.30  # fraction of task size that CAN be split\n",
    "    split_ratio_max: float = 0.80\n",
    "\n",
    "    # Optional modality probabilities (image, video, text, sensor)\n",
    "    modality_probs: Optional[List[float]] = None\n",
    "    modality_labels: List[str] = None\n",
    "\n",
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    name: str\n",
    "    N_agents: int              # number of MEC nodes (kept name for compatibility)\n",
    "    Episode: EpisodeConf\n",
    "    AgentRanges: AgentRanges\n",
    "    TaskDist: TaskFeatureDist\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# asserts / validation\n",
    "# ============================================================\n",
    "def _validate_cfg(cfg: GlobalConfig) -> None:\n",
    "    assert cfg.N_agents > 0\n",
    "    assert cfg.Episode.Delta > 0\n",
    "    assert cfg.Episode.T_slots > 0\n",
    "    assert cfg.Episode.T_decision > 0\n",
    "    assert cfg.Episode.T_drain >= 0\n",
    "    assert cfg.Episode.T_slots == cfg.Episode.T_decision + cfg.Episode.T_drain\n",
    "\n",
    "    ar = cfg.AgentRanges\n",
    "    assert ar.lam_sec_min >= 0 and ar.lam_sec_max >= ar.lam_sec_min\n",
    "    td = cfg.TaskDist\n",
    "    assert td.split_ratio_min > 0 and td.split_ratio_max <= 1.0 and td.split_ratio_max >= td.split_ratio_min\n",
    "    assert td.deadline_min <= td.deadline_max\n",
    "    # geometric std must be >= 1.0\n",
    "    assert td.b_sigma_g >= 1.0 and td.rho_sigma_g >= 1.0 and td.mem_sigma_g >= 1.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# helpers (lognormal quantiles)\n",
    "# ============================================================\n",
    "_Z_TABLE = {\n",
    "    0.90: 1.2815515655446004,\n",
    "    0.95: 1.6448536269514722,\n",
    "    0.975: 1.959963984540054,\n",
    "    0.99: 2.3263478740408408,\n",
    "    0.995: 2.5758293035489004,\n",
    "    0.999: 3.090232306167813,\n",
    "}\n",
    "\n",
    "def _z_from_p(p: float) -> float:\n",
    "    # Use small table + nearest clamp (no SciPy dependency)\n",
    "    if p in _Z_TABLE:\n",
    "        return _Z_TABLE[p]\n",
    "    # clamp to nearest key\n",
    "    return _Z_TABLE[min(_Z_TABLE.keys(), key=lambda k: abs(k - p))]\n",
    "\n",
    "def lognormal_quantile(median: float, sigma_g: float, p: float) -> float:\n",
    "    # X ~ LogNormal(mu, sigma) with median=exp(mu), sigma_g=exp(sigma)\n",
    "    # quantile(p) = median * exp( z_p * ln(sigma_g) )\n",
    "    z = _z_from_p(p)\n",
    "    return median * math.exp(z * math.log(max(sigma_g, 1.0 + 1e-6)))\n",
    "\n",
    "def lognormal_from_median_sigma_g(\n",
    "    rng: np.random.Generator,\n",
    "    median: float,\n",
    "    sigma_g: float,\n",
    "    qcap: float | None = 0.99\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Draw from LogNormal with given median and geometric std:\n",
    "      X ~ LogNormal(mu, sigma) where median = exp(mu), sigma_g = exp(sigma).\n",
    "      => mu = ln(median), sigma = ln(sigma_g)\n",
    "    \"\"\"\n",
    "    mu = math.log(max(median, 1e-12))\n",
    "    sigma = math.log(max(sigma_g, 1.0 + 1e-6))\n",
    "    x = float(rng.lognormal(mean=mu, sigma=sigma))\n",
    "    if qcap is not None:\n",
    "        cap = lognormal_quantile(median, sigma_g, qcap)\n",
    "        x = min(x, cap)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# entities (MEC nodes)\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Represents a MEC node from the dataset generator's point of view.\n",
    "    \"\"\"\n",
    "    agent_id: int      # will be stored as mec_id in CSVs\n",
    "    lam_sec: float     # Poisson rate per second (not per-slot)\n",
    "\n",
    "def build_agents(cfg: GlobalConfig, rng: np.random.Generator) -> List[Agent]:\n",
    "    \"\"\"\n",
    "    Build a list of MEC nodes with Poisson arrival rates lam_sec.\n",
    "    No local resource capacities are modeled here; they are assumed to\n",
    "    live in the environment code.\n",
    "    \"\"\"\n",
    "    agents: List[Agent] = []\n",
    "    for i in range(cfg.N_agents):\n",
    "        lam_sec = rng.uniform(cfg.AgentRanges.lam_sec_min, cfg.AgentRanges.lam_sec_max)\n",
    "        agents.append(Agent(agent_id=i, lam_sec=lam_sec))\n",
    "    return agents\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# task features\n",
    "# ============================================================\n",
    "def _modality_choice(rng: np.random.Generator, d: TaskFeatureDist) -> str:\n",
    "    # modality labels and probabilities\n",
    "    if d.modality_labels is None:\n",
    "        labels = [\"image\", \"video\", \"text\", \"sensor\"]\n",
    "    else:\n",
    "        labels = d.modality_labels\n",
    "\n",
    "    if d.modality_probs is None:\n",
    "        probs = [0.3, 0.2, 0.3, 0.2]\n",
    "    else:\n",
    "        probs = d.modality_probs\n",
    "        assert abs(sum(probs) - 1.0) < 1e-6 and len(probs) == len(labels)\n",
    "    return rng.choice(labels, p=probs)\n",
    "\n",
    "def sample_task_features(cfg: GlobalConfig, rng: np.random.Generator, qcap: float = 0.99) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Sample a single task's features.\n",
    "\n",
    "    - Task size (b_mb) is now discrete uniform integer in [TASK_SIZE_MIN_MB, TASK_SIZE_MAX_MB].\n",
    "    - Compute density (rho) and memory (mem_mb) remain lognormal.\n",
    "    \"\"\"\n",
    "    d = cfg.TaskDist\n",
    "\n",
    "    # Discrete uniform task size (MB) from JSON config\n",
    "    task_size_min = TASK_SIZE_MIN_MB\n",
    "    task_size_max = TASK_SIZE_MAX_MB\n",
    "    b_mb = int(rng.integers(task_size_min, task_size_max + 1))\n",
    "\n",
    "    rho    = lognormal_from_median_sigma_g(rng, d.rho_median, d.rho_sigma_g, qcap=qcap)\n",
    "    c      = b_mb * rho                              # total cycles\n",
    "    mem_mb = lognormal_from_median_sigma_g(rng, d.mem_median, d.mem_sigma_g, qcap=qcap)\n",
    "    modality = _modality_choice(rng, d)\n",
    "    has_deadline = int(rng.random() < d.p_deadline)\n",
    "    deadline_s   = np.nan\n",
    "    if has_deadline:\n",
    "        deadline_s = float(rng.uniform(d.deadline_min, d.deadline_max))\n",
    "    non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "    split_ratio = float(rng.uniform(d.split_ratio_min, d.split_ratio_max)) if non_atomic else 0.0\n",
    "    \n",
    "    return dict(\n",
    "        b_mb=b_mb, rho=rho, c_cycles=c, mem_mb=mem_mb, modality=modality,\n",
    "        has_deadline=has_deadline, deadline_s=deadline_s, non_atomic=non_atomic, split_ratio=split_ratio\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# episode generator (arrivals only)\n",
    "# ============================================================\n",
    "def run_episode(\n",
    "    cfg: GlobalConfig,\n",
    "    agents: List[Agent],\n",
    "    episode_id: int = 0,\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate time-stamped arrivals and task features for ONE episode of ONE scenario.\n",
    "\n",
    "    HOODIE-style:\n",
    "      - t = 0 .. T_decision-1 → arrivals via Poisson\n",
    "      - t = T_decision .. T_slots-1 → NO new arrivals (drain phase)\n",
    "    \"\"\"\n",
    "    _validate_cfg(cfg)\n",
    "    rng_local = np.random.default_rng(cfg.Episode.seed + episode_id)\n",
    "\n",
    "    rows_episodes: List[Dict] = []\n",
    "    rows_agents:   List[Dict] = []\n",
    "    rows_arrivals: List[Dict] = []\n",
    "    rows_tasks:    List[Dict] = []\n",
    "\n",
    "    Delta      = cfg.Episode.Delta\n",
    "    T_slots    = cfg.Episode.T_slots\n",
    "    T_decision = cfg.Episode.T_decision\n",
    "\n",
    "    # MEC nodes (stored in agents.csv; include scenario for easier joins)\n",
    "    for a in agents:\n",
    "        rows_agents.append({\n",
    "            \"scenario\": cfg.name,\n",
    "            \"mec_id\": a.agent_id,\n",
    "            \"lam_sec\": a.lam_sec\n",
    "        })\n",
    "\n",
    "    task_id_counter = 0\n",
    "\n",
    "    for t in range(T_slots):\n",
    "        t_time = t * Delta\n",
    "\n",
    "        # Only first T_decision slots have new arrivals (HOODIE style)\n",
    "        if t >= T_decision:\n",
    "            continue\n",
    "\n",
    "        for a in agents:\n",
    "            # per-slot rate from per-second rate:\n",
    "            lam_slot = a.lam_sec * Delta\n",
    "            n_new = rng_local.poisson(lam=lam_slot)\n",
    "            if n_new <= 0:\n",
    "                continue\n",
    "            for _ in range(n_new):\n",
    "                feat = sample_task_features(cfg, rng_local, qcap=qcap)\n",
    "\n",
    "                # absolute deadline time (NaN if none)\n",
    "                if np.isnan(feat[\"deadline_s\"]):\n",
    "                    deadline_time = np.nan\n",
    "                else:\n",
    "                    deadline_time = t_time + feat[\"deadline_s\"]\n",
    "\n",
    "                action_space_hint = \"continuous\" if feat[\"non_atomic\"] == 1 else \"discrete\"\n",
    "\n",
    "                rows_arrivals.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"t_slot\": t,\n",
    "                    \"t_time\": t_time,\n",
    "                    \"mec_id\": a.agent_id,\n",
    "                    \"task_id\": task_id_counter\n",
    "                })\n",
    "\n",
    "                rows_tasks.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"task_id\": task_id_counter,\n",
    "                    \"mec_id\": a.agent_id,\n",
    "                    \"t_arrival_slot\": t,\n",
    "                    \"t_arrival_time\": t_time,\n",
    "                    \"b_mb\": feat[\"b_mb\"],\n",
    "                    \"rho_cyc_per_mb\": feat[\"rho\"],\n",
    "                    \"c_cycles\": feat[\"c_cycles\"],\n",
    "                    \"mem_mb\": feat[\"mem_mb\"],\n",
    "                    \"modality\": feat[\"modality\"],\n",
    "                    \"has_deadline\": feat[\"has_deadline\"],\n",
    "                    \"deadline_s\": feat[\"deadline_s\"],\n",
    "                    \"deadline_time\": deadline_time,\n",
    "                    \"non_atomic\": feat[\"non_atomic\"],\n",
    "                    \"split_ratio\": feat[\"split_ratio\"],\n",
    "                    \"action_space_hint\": action_space_hint\n",
    "                })\n",
    "\n",
    "                task_id_counter += 1\n",
    "\n",
    "    rows_episodes.append({\n",
    "        \"scenario\": cfg.name,\n",
    "        \"episode_id\": episode_id,\n",
    "        \"Delta\": Delta,\n",
    "        \"T_slots\": T_slots,\n",
    "        \"T_decision\": T_decision,\n",
    "        \"T_drain\": cfg.Episode.T_drain,\n",
    "        \"hours\": T_slots * Delta / 3600.0,\n",
    "        \"N_agents\": len(agents),       # number of MEC nodes; name kept for compatibility\n",
    "        \"seed\": cfg.Episode.seed + episode_id\n",
    "    })\n",
    "\n",
    "    episodes_df = pd.DataFrame(rows_episodes)\n",
    "    agents_df   = pd.DataFrame(rows_agents)\n",
    "    arrivals_df = pd.DataFrame(rows_arrivals)\n",
    "    tasks_df    = pd.DataFrame(rows_tasks)\n",
    "\n",
    "    # Optimize dtypes\n",
    "    if len(tasks_df):\n",
    "        tasks_df[\"modality\"] = tasks_df[\"modality\"].astype(\"category\")\n",
    "        tasks_df[\"action_space_hint\"] = tasks_df[\"action_space_hint\"].astype(\"category\")\n",
    "        # ints\n",
    "        for col in [\"episode_id\", \"task_id\", \"mec_id\", \"t_arrival_slot\", \"has_deadline\", \"non_atomic\"]:\n",
    "            if col in tasks_df:\n",
    "                tasks_df[col] = tasks_df[col].astype(\"int32\")\n",
    "        # floats\n",
    "        for col in [\"t_arrival_time\", \"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\",\n",
    "                    \"deadline_s\", \"deadline_time\", \"split_ratio\"]:\n",
    "            if col in tasks_df:\n",
    "                tasks_df[col] = tasks_df[col].astype(\"float32\")\n",
    "\n",
    "    if len(arrivals_df):\n",
    "        for col in [\"episode_id\", \"t_slot\", \"mec_id\", \"task_id\"]:\n",
    "            if col in arrivals_df:\n",
    "                arrivals_df[col] = arrivals_df[col].astype(\"int32\")\n",
    "        for col in [\"t_time\"]:\n",
    "            if col in arrivals_df:\n",
    "                arrivals_df[col] = arrivals_df[col].astype(\"float32\")\n",
    "\n",
    "    if len(agents_df):\n",
    "        for col in [\"mec_id\"]:\n",
    "            agents_df[col] = agents_df[col].astype(\"int32\")\n",
    "        for col in [\"lam_sec\"]:\n",
    "            agents_df[col] = agents_df[col].astype(\"float64\")\n",
    "\n",
    "    if len(episodes_df):\n",
    "        for col in [\"episode_id\", \"N_agents\", \"T_slots\", \"T_decision\", \"T_drain\"]:\n",
    "            if col in episodes_df:\n",
    "                episodes_df[col] = episodes_df[col].astype(\"int32\")\n",
    "        for col in [\"Delta\", \"hours\"]:\n",
    "            if col in episodes_df:\n",
    "                episodes_df[col] = episodes_df[col].astype(\"float32\")\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes_df,\n",
    "        \"agents\":   agents_df,\n",
    "        \"arrivals\": arrivals_df,\n",
    "        \"tasks\":    tasks_df\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# save & plotting utilities\n",
    "# ============================================================\n",
    "def save_dataset(dfs: Dict[str, pd.DataFrame], out_dir: str = \".\") -> Dict[str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    paths: Dict[str, str] = {}\n",
    "    for name, df in dfs.items():\n",
    "        csv_path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        paths[name + \"_csv\"] = csv_path\n",
    "    return paths\n",
    "\n",
    "def _config_fingerprint(cfg: GlobalConfig) -> str:\n",
    "    s = json.dumps({\n",
    "        \"scenario\": cfg.name,\n",
    "        \"Episode\": asdict(cfg.Episode),\n",
    "        \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "        \"TaskDist\": asdict(cfg.TaskDist)\n",
    "    }, sort_keys=True).encode(\"utf-8\")\n",
    "    return hashlib.sha256(s).hexdigest()[:16]\n",
    "\n",
    "def save_episode_meta(\n",
    "    cfgs: List[GlobalConfig],\n",
    "    ep_dir: str,\n",
    "    qcap: float = 0.99\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save a single dataset_metadata.json inside ep_xxx containing\n",
    "    config info for ALL scenarios used in this episode.\n",
    "    \"\"\"\n",
    "    meta = {\n",
    "        \"schema_version\": \"1.0.0\",\n",
    "        \"generated_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "        \"env\": {\n",
    "            \"python\": platform.python_version(),\n",
    "            \"user\": getpass.getuser()\n",
    "        },\n",
    "        \"episodes_root\": os.path.abspath(ep_dir),\n",
    "        \"notes\": {\n",
    "            \"policy_agnostic\": True,\n",
    "            \"queueing\": \"not simulated here\",\n",
    "            \"timing\": {\n",
    "                \"description\": \"HOODIE-style: T_decision slots with arrivals + T_drain slots without arrivals.\",\n",
    "            },\n",
    "            \"clipping\": {\n",
    "                \"enabled\": True,\n",
    "                \"method\": \"lognormal analytic quantile cap\",\n",
    "                \"qcap\": qcap,\n",
    "                \"z_table_keys\": sorted(list(_Z_TABLE.keys()))\n",
    "            },\n",
    "            \"action_space_hint\": \"derived from non_atomic; final decision belongs to environment\"\n",
    "        },\n",
    "        \"scenarios\": []\n",
    "    }\n",
    "\n",
    "    for cfg in cfgs:\n",
    "        meta[\"scenarios\"].append({\n",
    "            \"name\": cfg.name,\n",
    "            \"fingerprint\": _config_fingerprint(cfg),\n",
    "            \"Episode\": asdict(cfg.Episode),\n",
    "            \"N_agents\": cfg.N_agents,              # number of MEC nodes\n",
    "            \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "            \"TaskDist\": asdict(cfg.TaskDist),\n",
    "            \"units\": {\n",
    "                \"Delta\": \"seconds\",\n",
    "                \"lam_sec\": \"tasks per second (per MEC)\",\n",
    "                \"per_slot_rate\": \"lam_sec * Delta\",\n",
    "                \"b_mb\": \"MB\",\n",
    "                \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
    "                \"c_cycles\": \"CPU cycles\",\n",
    "                \"mem_mb\": \"MB\",\n",
    "                \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    path = os.path.join(ep_dir, \"dataset_metadata.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    return path\n",
    "\n",
    "def summarize_and_plot(dfs: Dict[str, pd.DataFrame], out_dir: str) -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tasks = dfs[\"tasks\"].copy()\n",
    "    arrivals = dfs[\"arrivals\"].copy()\n",
    "    episodes = dfs[\"episodes\"].copy()\n",
    "\n",
    "    # ---- summary stats\n",
    "    def q(series: pd.Series, p: float):\n",
    "        s = series.dropna()\n",
    "        return float(np.nanquantile(s, p)) if len(s) else float(\"nan\")\n",
    "\n",
    "    tasks_per_hour = float(len(tasks)) / float(episodes.iloc[0][\"hours\"]) if len(episodes) and episodes.iloc[0][\"hours\"] > 0 else float(\"nan\")\n",
    "\n",
    "    # modality distribution\n",
    "    if len(tasks):\n",
    "        mod_counts = tasks[\"modality\"].value_counts(dropna=False)\n",
    "        mod_dist = {str(k): int(v) for k, v in mod_counts.to_dict().items()}\n",
    "    else:\n",
    "        mod_dist = {}\n",
    "\n",
    "    summary = {\n",
    "        \"n_tasks\": [len(tasks)],\n",
    "        \"n_arrivals\": [len(arrivals)],\n",
    "        \"tasks_per_hour\": [tasks_per_hour],\n",
    "        \"b_mb_median\": [q(tasks[\"b_mb\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"b_mb_p90\": [q(tasks[\"b_mb\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"b_mb_p99\": [q(tasks[\"b_mb\"], 0.99)] if len(tasks) else [float(\"nan\")],\n",
    "        \"rho_median\": [q(tasks[\"rho_cyc_per_mb\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"rho_p90\": [q(tasks[\"rho_cyc_per_mb\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_median\": [q(tasks[\"c_cycles\"], 0.5)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_p90\": [q(tasks[\"c_cycles\"], 0.9)] if len(tasks) else [float(\"nan\")],\n",
    "        \"c_cycles_p99\": [q(tasks[\"c_cycles\"], 0.99)] if len(tasks) else [float(\"nan\")],\n",
    "        \"deadline_share\": [float((tasks[\"has_deadline\"] == 1).mean()) if len(tasks) else float(\"nan\")],\n",
    "        \"non_atomic_share\": [float((tasks[\"non_atomic\"] == 1).mean()) if len(tasks) else float(\"nan\")],\n",
    "        \"modality_counts_json\": [json.dumps(mod_dist)]\n",
    "    }\n",
    "    pd.DataFrame(summary).to_csv(os.path.join(out_dir, \"summary_stats.csv\"), index=False)\n",
    "\n",
    "    # ---- plots (each in its own figure)\n",
    "    def hist_plot(series: pd.Series, title: str, fname: str, logx: bool = False):\n",
    "        plt.figure()\n",
    "        s = series.dropna()\n",
    "        if len(s) == 0:\n",
    "            plt.title(title + \" (no data)\")\n",
    "        else:\n",
    "            plt.hist(s, bins=50)\n",
    "            if logx:\n",
    "                plt.xscale(\"log\")\n",
    "            plt.title(title)\n",
    "            plt.xlabel(title)\n",
    "            plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, fname), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    if len(tasks):\n",
    "        hist_plot(tasks[\"b_mb\"],            title=\"Task size (MB)\",                fname=\"hist_b_mb.png\", logx=False)\n",
    "        hist_plot(tasks[\"rho_cyc_per_mb\"],  title=\"Compute density (cycles/MB)\",   fname=\"hist_rho.png\",  logx=True)\n",
    "        hist_plot(tasks[\"c_cycles\"],        title=\"Total cycles\",                  fname=\"hist_c_cycles.png\", logx=True)\n",
    "        hist_plot(tasks[\"deadline_s\"],      title=\"Deadline (s)\",                  fname=\"hist_deadline_s.png\", logx=False)\n",
    "        hist_plot(tasks.loc[tasks[\"non_atomic\"] == 1, \"split_ratio\"],\n",
    "                  title=\"Split ratio (only non-atomic)\", fname=\"hist_split_ratio.png\", logx=False)\n",
    "\n",
    "    # arrivals per MEC\n",
    "    if len(arrivals):\n",
    "        per_mec = arrivals.groupby(\"mec_id\").size()\n",
    "        plt.figure()\n",
    "        plt.bar(per_mec.index.astype(str), per_mec.values)\n",
    "        plt.title(\"Arrivals per MEC\")\n",
    "        plt.xlabel(\"mec_id\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"bar_arrivals_per_mec.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# scenario presets (constructed from JSON)\n",
    "# ============================================================\n",
    "# Episode base from JSON\n",
    "EPISODE_CFG = CFG[\"episode\"]\n",
    "DELTA      = EPISODE_CFG[\"delta\"]\n",
    "T_DECISION = EPISODE_CFG[\"t_decision\"]\n",
    "T_DRAIN    = EPISODE_CFG[\"t_drain\"]\n",
    "T_SLOTS    = T_DECISION + T_DRAIN\n",
    "\n",
    "BASE_EPISODE = EpisodeConf(\n",
    "    Delta=DELTA,\n",
    "    T_slots=T_SLOTS,\n",
    "    T_decision=T_DECISION,\n",
    "    T_drain=T_DRAIN,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "SC_LIGHT  = CFG[\"scenarios\"][\"light\"]\n",
    "SC_MOD    = CFG[\"scenarios\"][\"moderate\"]\n",
    "SC_HEAVY  = CFG[\"scenarios\"][\"heavy\"]\n",
    "\n",
    "# Base AgentRanges; scenario-specific overrides applied with replace(...)\n",
    "BASE_AGENT_RANGES = AgentRanges(\n",
    "    lam_sec_min=SC_LIGHT[\"lam_sec_min\"],   # base placeholder; overridden for each scenario\n",
    "    lam_sec_max=SC_HEAVY[\"lam_sec_max\"],   # base placeholder\n",
    ")\n",
    "\n",
    "# Base TaskFeatureDist; scenario-specific overrides applied with replace(...)\n",
    "BASE_TASK_DIST = TaskFeatureDist()\n",
    "\n",
    "SCENARIOS: List[GlobalConfig] = [\n",
    "    # ---- LIGHT ----\n",
    "    GlobalConfig(\n",
    "        name=\"light\",\n",
    "        N_agents=SC_LIGHT[\"n_agents\"],\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 101),\n",
    "        AgentRanges=replace(\n",
    "            BASE_AGENT_RANGES,\n",
    "            lam_sec_min=SC_LIGHT[\"lam_sec_min\"],\n",
    "            lam_sec_max=SC_LIGHT[\"lam_sec_max\"]\n",
    "        ),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=SC_LIGHT[\"b_median\"],\n",
    "            b_sigma_g=SC_LIGHT[\"b_sigma_g\"],\n",
    "            rho_median=SC_LIGHT[\"rho_median\"],\n",
    "            rho_sigma_g=SC_LIGHT[\"rho_sigma_g\"],\n",
    "            mem_median=SC_LIGHT[\"mem_median\"],\n",
    "            mem_sigma_g=SC_LIGHT[\"mem_sigma_g\"],\n",
    "            p_deadline=SC_LIGHT[\"p_deadline\"],\n",
    "            deadline_min=SC_LIGHT[\"deadline_min\"],\n",
    "            deadline_max=SC_LIGHT[\"deadline_max\"],\n",
    "            p_non_atomic=SC_LIGHT[\"p_non_atomic\"],\n",
    "            split_ratio_min=SC_LIGHT[\"split_ratio_min\"],\n",
    "            split_ratio_max=SC_LIGHT[\"split_ratio_max\"]\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    # ---- MODERATE ----\n",
    "    GlobalConfig(\n",
    "        name=\"moderate\",\n",
    "        N_agents=SC_MOD[\"n_agents\"],\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 202),\n",
    "        AgentRanges=replace(\n",
    "            BASE_AGENT_RANGES,\n",
    "            lam_sec_min=SC_MOD[\"lam_sec_min\"],\n",
    "            lam_sec_max=SC_MOD[\"lam_sec_max\"]\n",
    "        ),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=SC_MOD[\"b_median\"],\n",
    "            b_sigma_g=SC_MOD[\"b_sigma_g\"],\n",
    "            rho_median=SC_MOD[\"rho_median\"],\n",
    "            rho_sigma_g=SC_MOD[\"rho_sigma_g\"],\n",
    "            mem_median=SC_MOD[\"mem_median\"],\n",
    "            mem_sigma_g=SC_MOD[\"mem_sigma_g\"],\n",
    "            p_deadline=SC_MOD[\"p_deadline\"],\n",
    "            deadline_min=SC_MOD[\"deadline_min\"],\n",
    "            deadline_max=SC_MOD[\"deadline_max\"],\n",
    "            p_non_atomic=SC_MOD[\"p_non_atomic\"],\n",
    "            split_ratio_min=SC_MOD[\"split_ratio_min\"],\n",
    "            split_ratio_max=SC_MOD[\"split_ratio_max\"]\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    # ---- HEAVY ----\n",
    "    GlobalConfig(\n",
    "        name=\"heavy\",\n",
    "        N_agents=SC_HEAVY[\"n_agents\"],\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 303),\n",
    "        AgentRanges=replace(\n",
    "            BASE_AGENT_RANGES,\n",
    "            lam_sec_min=SC_HEAVY[\"lam_sec_min\"],\n",
    "            lam_sec_max=SC_HEAVY[\"lam_sec_max\"]\n",
    "        ),\n",
    "        TaskDist=replace(\n",
    "            BASE_TASK_DIST,\n",
    "            b_median=SC_HEAVY[\"b_median\"],\n",
    "            b_sigma_g=SC_HEAVY[\"b_sigma_g\"],\n",
    "            rho_median=SC_HEAVY[\"rho_median\"],\n",
    "            rho_sigma_g=SC_HEAVY[\"rho_sigma_g\"],\n",
    "            mem_median=SC_HEAVY[\"mem_median\"],\n",
    "            mem_sigma_g=SC_HEAVY[\"mem_sigma_g\"],\n",
    "            p_deadline=SC_HEAVY[\"p_deadline\"],\n",
    "            deadline_min=SC_HEAVY[\"deadline_min\"],\n",
    "            deadline_max=SC_HEAVY[\"deadline_max\"],\n",
    "            p_non_atomic=SC_HEAVY[\"p_non_atomic\"],\n",
    "            split_ratio_min=SC_HEAVY[\"split_ratio_min\"],\n",
    "            split_ratio_max=SC_HEAVY[\"split_ratio_max\"]\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# drivers: per-scenario & all-scenarios\n",
    "# ============================================================\n",
    "def main_generate_for_scenario(\n",
    "    cfg: GlobalConfig,\n",
    "    agents: List[Agent],\n",
    "    episode_id: int,\n",
    "    ep_dir: str,\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate ONE episode for ONE scenario under:\n",
    "        ep_dir/<scenario>/\n",
    "    For example: datasets/ep_000/heavy/\n",
    "    \"\"\"\n",
    "    _validate_cfg(cfg)\n",
    "\n",
    "    scenario_dir = os.path.join(ep_dir, cfg.name)\n",
    "    os.makedirs(scenario_dir, exist_ok=True)\n",
    "\n",
    "    dfs = run_episode(cfg, agents, episode_id=episode_id, qcap=qcap)\n",
    "    paths = save_dataset(dfs, out_dir=scenario_dir)\n",
    "    summarize_and_plot(dfs, out_dir=scenario_dir)\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def generate_all_scenarios(\n",
    "    episodes_each: int = 1,\n",
    "    out_root: str = \"./datasets\",\n",
    "    qcap: float = 0.99\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Layout:\n",
    "        out_root/ep_000/<scenario>/\n",
    "        out_root/ep_001/<scenario>/\n",
    "        ...\n",
    "    And inside each ep_xxx we store dataset_metadata.json\n",
    "    with info for ALL scenarios.\n",
    "\n",
    "    NOTE:\n",
    "      - To fully mimic HOODIE training (e.g., many episodes), call:\n",
    "            generate_all_scenarios(episodes_each=5000, ...)\n",
    "      - Default episodes_each=1 is for debugging.\n",
    "    \"\"\"\n",
    "    results: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "    # For stability, build MEC nodes (\"agents\") per scenario once (same pool across episodes for that scenario)\n",
    "    agents_per_scenario: Dict[str, List[Agent]] = {}\n",
    "    for cfg in SCENARIOS:\n",
    "        rng_agents = np.random.default_rng(cfg.Episode.seed + 10_000)\n",
    "        agents_per_scenario[cfg.name] = build_agents(cfg, rng_agents)\n",
    "\n",
    "    for ep in range(episodes_each):\n",
    "        ep_name = f\"ep_{ep:03d}\"\n",
    "        ep_dir = os.path.join(out_root, ep_name)\n",
    "        os.makedirs(ep_dir, exist_ok=True)\n",
    "\n",
    "        episode_paths: Dict[str, str] = {}\n",
    "        for cfg in SCENARIOS:\n",
    "            agents = agents_per_scenario[cfg.name]\n",
    "            paths = main_generate_for_scenario(\n",
    "                cfg=cfg,\n",
    "                agents=agents,\n",
    "                episode_id=ep,\n",
    "                ep_dir=ep_dir,\n",
    "                qcap=qcap\n",
    "            )\n",
    "            # keys like: heavy_episodes_csv, heavy_tasks_csv, ...\n",
    "            for k, v in paths.items():\n",
    "                episode_paths[f\"{cfg.name}_{k}\"] = v\n",
    "\n",
    "        # per-episode metadata (ALL scenarios)\n",
    "        meta_path = save_episode_meta(SCENARIOS, ep_dir=ep_dir, qcap=qcap)\n",
    "        episode_paths[\"metadata_json\"] = meta_path\n",
    "\n",
    "        results[ep_name] = episode_paths\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# sanity checks\n",
    "# ============================================================\n",
    "def sanity_check_episode(ep_dir: str, scenario_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Lightweight sanity checks for one ep_xxx:\n",
    "      - basic file presence\n",
    "      - tasks vs arrivals count & task_id consistency\n",
    "      - mec_id consistency\n",
    "      - basic value ranges (positivity, deadlines, split_ratio)\n",
    "      - rough consistency between lambda and realized tasks/hour (on decision slots)\n",
    "    \"\"\"\n",
    "    print(f\"[sanity] Checking episode directory: {ep_dir}\")\n",
    "    meta_path = os.path.join(ep_dir, \"dataset_metadata.json\")\n",
    "    if not os.path.isfile(meta_path):\n",
    "        print(f\"  [WARN] No dataset_metadata.json found in {ep_dir}\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            print(f\"  [OK] Loaded metadata with {len(meta.get('scenarios', []))} scenarios\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to read metadata: {e}\")\n",
    "\n",
    "    for scen in scenario_names:\n",
    "        scen_dir = os.path.join(ep_dir, scen)\n",
    "        if not os.path.isdir(scen_dir):\n",
    "            print(f\"  [WARN] Scenario dir missing: {scen_dir}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            episodes_df = pd.read_csv(os.path.join(scen_dir, \"episodes.csv\"))\n",
    "            agents_df   = pd.read_csv(os.path.join(scen_dir, \"agents.csv\"))\n",
    "            arrivals_df = pd.read_csv(os.path.join(scen_dir, \"arrivals.csv\"))\n",
    "            tasks_df    = pd.read_csv(os.path.join(scen_dir, \"tasks.csv\"))\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to load CSVs for scenario '{scen}': {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  [scenario={scen}] n_tasks={len(tasks_df)}, n_arrivals={len(arrivals_df)}, n_mecs={len(agents_df)}\")\n",
    "\n",
    "        # 1) non-empty checks\n",
    "        if len(tasks_df) == 0 or len(arrivals_df) == 0:\n",
    "            print(f\"    [WARN] Empty tasks or arrivals for scenario '{scen}'\")\n",
    "            continue\n",
    "\n",
    "        # 2) tasks vs arrivals counts & unique task_ids\n",
    "        if len(tasks_df) != len(arrivals_df):\n",
    "            print(f\"    [WARN] tasks ({len(tasks_df)}) != arrivals ({len(arrivals_df)})\")\n",
    "\n",
    "        if tasks_df[\"task_id\"].nunique() != len(tasks_df):\n",
    "            print(f\"    [WARN] Duplicate task_id in tasks.csv for scenario '{scen}'\")\n",
    "\n",
    "        # 3) mec_id consistency\n",
    "        mecs_set = set(agents_df[\"mec_id\"].tolist())\n",
    "        arr_mecs = set(arrivals_df[\"mec_id\"].tolist())\n",
    "        task_mecs = set(tasks_df[\"mec_id\"].tolist())\n",
    "        if not arr_mecs.issubset(mecs_set):\n",
    "            print(f\"    [WARN] Some arrival mec_ids not in agents table for '{scen}'\")\n",
    "        if not task_mecs.issubset(mecs_set):\n",
    "            print(f\"    [WARN] Some task mec_ids not in agents table for '{scen}'\")\n",
    "\n",
    "        # 4) basic value ranges\n",
    "        if (tasks_df[\"b_mb\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive b_mb values in tasks for '{scen}'\")\n",
    "        if (tasks_df[\"mem_mb\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive mem_mb values in tasks for '{scen}'\")\n",
    "        if (tasks_df[\"c_cycles\"] <= 0).any():\n",
    "            print(f\"    [WARN] Non-positive c_cycles values in tasks for '{scen}'\")\n",
    "\n",
    "        # deadlines: deadline_time >= t_arrival_time when has_deadline\n",
    "        with_deadline = tasks_df[\"has_deadline\"] == 1\n",
    "        if with_deadline.any():\n",
    "            bad_deadlines = (tasks_df.loc[with_deadline, \"deadline_time\"] <\n",
    "                             tasks_df.loc[with_deadline, \"t_arrival_time\"]).sum()\n",
    "            if bad_deadlines > 0:\n",
    "                print(f\"    [WARN] {bad_deadlines} rows with deadline_time < t_arrival_time in '{scen}'\")\n",
    "\n",
    "        # split ratio range\n",
    "        if ((tasks_df[\"split_ratio\"] < 0) | (tasks_df[\"split_ratio\"] > 1)).any():\n",
    "            print(f\"    [WARN] split_ratio out of [0,1] range in '{scen}'\")\n",
    "        # non_atomic=0 => split_ratio==0\n",
    "        mask = tasks_df[\"non_atomic\"] == 0\n",
    "        if (~np.isclose(tasks_df.loc[mask, \"split_ratio\"], 0.0)).any():\n",
    "            print(f\"    [WARN] non_atomic==0 but split_ratio != 0 (float mismatch) in '{scen}'\")\n",
    "\n",
    "        # 5) rough lambda consistency check (using decision horizon length)\n",
    "        if len(episodes_df):\n",
    "            hours = float(episodes_df.iloc[0][\"T_decision\"] * episodes_df.iloc[0][\"Delta\"]) / 3600.0\n",
    "            if hours > 0:\n",
    "                # arrivals only exist in decision slots by construction\n",
    "                tasks_per_mec = arrivals_df.groupby(\"mec_id\").size() / hours\n",
    "                merged = pd.merge(\n",
    "                    agents_df[[\"mec_id\", \"lam_sec\"]],\n",
    "                    tasks_per_mec.rename(\"tasks_per_hour\").reset_index(),\n",
    "                    on=\"mec_id\",\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                merged[\"lam_per_hour\"] = merged[\"lam_sec\"] * 3600.0\n",
    "                merged[\"ratio\"] = merged[\"tasks_per_hour\"] / merged[\"lam_per_hour\"]\n",
    "                too_low = (merged[\"ratio\"] < 0.4).sum()\n",
    "                too_high = (merged[\"ratio\"] > 1.6).sum()\n",
    "                if too_low + too_high > 0:\n",
    "                    print(f\"    [INFO] Poisson check: some MECs have realized rate far from expected \"\n",
    "                          f\"(too_low={too_low}, too_high={too_high}) in '{scen}'\")\n",
    "\n",
    "def sanity_check_root(out_root: str, scenario_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Run sanity checks over all ep_* folders under out_root.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(out_root):\n",
    "        print(f\"[sanity] Root directory '{out_root}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    episodes = sorted([d for d in os.listdir(out_root) if d.startswith(\"ep_\")])\n",
    "    if not episodes:\n",
    "        print(f\"[sanity] No ep_* folders found under '{out_root}'.\")\n",
    "        return\n",
    "\n",
    "    for ep_name in episodes:\n",
    "        ep_dir = os.path.join(out_root, ep_name)\n",
    "        if os.path.isdir(ep_dir):\n",
    "            sanity_check_episode(ep_dir, scenario_names)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# main\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Use dataset config from JSON\n",
    "    results = generate_all_scenarios(\n",
    "        episodes_each=DATASET_EPISODES_EACH,\n",
    "        out_root=DATASET_OUT_ROOT,\n",
    "        qcap=DATASET_QCAP\n",
    "    )\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "    # basic sanity checks\n",
    "    scenario_names = [cfg.name for cfg in SCENARIOS]\n",
    "    sanity_check_root(DATASET_OUT_ROOT, scenario_names=scenario_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
