{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\niush\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\niush\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"light\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\light\\\\light_ep0_episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\light\\\\light_ep0_agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\light\\\\light_ep0_arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\light\\\\light_ep0_tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\light\\\\light_dataset_meta.json\"\n",
      "  },\n",
      "  \"moderate\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\moderate\\\\moderate_ep0_episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\moderate\\\\moderate_ep0_agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\moderate\\\\moderate_ep0_arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\moderate\\\\moderate_ep0_tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\moderate\\\\moderate_dataset_meta.json\"\n",
      "  },\n",
      "  \"heavy\": {\n",
      "    \"ep0_episodes_csv\": \"./datasets\\\\heavy\\\\heavy_ep0_episodes.csv\",\n",
      "    \"ep0_agents_csv\": \"./datasets\\\\heavy\\\\heavy_ep0_agents.csv\",\n",
      "    \"ep0_arrivals_csv\": \"./datasets\\\\heavy\\\\heavy_ep0_arrivals.csv\",\n",
      "    \"ep0_tasks_csv\": \"./datasets\\\\heavy\\\\heavy_ep0_tasks.csv\",\n",
      "    \"meta\": \"./datasets\\\\heavy\\\\heavy_dataset_meta.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data generator for Edge–MEC–Cloud with Poisson arrivals (per-second),\n",
    "lognormal task features (parameterized by median & sigma_g), and policy-agnostic outputs.\n",
    "\n",
    "Now supports THREE SCENARIOS (light / moderate / heavy) similar in spirit to HOODIE experiments.\n",
    "For each scenario we:\n",
    "  - synthesize time-stamped arrivals and task features for one or more episodes\n",
    "  - save CSV + (optionally) Parquet + a dataset_meta.json\n",
    "  - plot distribution figures (PNG) for key variables\n",
    "  - export a summary_stats.csv with quantiles/means\n",
    "\n",
    "No routing/decisions here; purely input data synthesis for later algorithms.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, math, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# reproducibility\n",
    "# -------------------------\n",
    "GLOBAL_SEED = 42\n",
    "rng_global = np.random.default_rng(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# -------------------------\n",
    "# configuration dataclasses\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class EpisodeConf:\n",
    "    Delta: float      # seconds per slot\n",
    "    T_slots: int      # number of slots in the episode\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class AgentRanges:\n",
    "    # arrival rate per SECOND (will be multiplied by Delta per slot)\n",
    "    lam_sec_min: float\n",
    "    lam_sec_max: float\n",
    "    # optional local capacities (kept as meta for later)\n",
    "    f_local_min: float\n",
    "    f_local_max: float\n",
    "    m_local_min: float\n",
    "    m_local_max: float\n",
    "\n",
    "@dataclass\n",
    "class TaskFeatureDist:\n",
    "    # Lognormal parameterization via median and sigma_g (geometric std).\n",
    "    b_median: float = 3.0          # MB\n",
    "    b_sigma_g: float = 0.6\n",
    "\n",
    "    rho_median: float = 1.2e9      # cycles / MB\n",
    "    rho_sigma_g: float = 0.5\n",
    "\n",
    "    # L_req_min: float = 1.0\n",
    "    # L_req_max: float = 8.0\n",
    "\n",
    "    mem_median: float = 64.0       # MB\n",
    "    mem_sigma_g: float = 0.5\n",
    "\n",
    "    p_deadline: float = 0.25\n",
    "    deadline_min: float = 0.3      # seconds (relative)\n",
    "    deadline_max: float = 1.5\n",
    "\n",
    "    p_non_atomic: float = 0.35\n",
    "    split_ratio_min: float = 0.30  # fraction of task size that CAN be split\n",
    "    split_ratio_max: float = 0.80\n",
    "\n",
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    name: str\n",
    "    N_agents: int\n",
    "    Episode: EpisodeConf\n",
    "    AgentRanges: AgentRanges\n",
    "    TaskDist: TaskFeatureDist\n",
    "\n",
    "# -------------------------\n",
    "# helpers\n",
    "# -------------------------\n",
    "def lognormal_from_median_sigma_g(rng: np.random.Generator, median: float, sigma_g: float) -> float:\n",
    "    \"\"\"\n",
    "    Draw from LogNormal with given median and geometric std:\n",
    "      X ~ LogNormal(mu, sigma) where median = exp(mu), sigma_g = exp(sigma).\n",
    "      => mu = ln(median), sigma = ln(sigma_g)\n",
    "    \"\"\"\n",
    "    mu = math.log(max(median, 1e-12))\n",
    "    sigma = math.log(max(sigma_g, 1.0 + 1e-6))\n",
    "    return float(rng.lognormal(mean=mu, sigma=sigma))\n",
    "\n",
    "# -------------------------\n",
    "# entities\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Agent:\n",
    "    agent_id: int\n",
    "    f_local: float\n",
    "    m_local: float\n",
    "    lam_sec: float   # Poisson rate per second (not per-slot)\n",
    "\n",
    "def build_agents(cfg: GlobalConfig, rng: np.random.Generator) -> List[Agent]:\n",
    "    agents: List[Agent] = []\n",
    "    for i in range(cfg.N_agents):\n",
    "        lam_sec = rng.uniform(cfg.AgentRanges.lam_sec_min, cfg.AgentRanges.lam_sec_max)\n",
    "        f_loc   = rng.uniform(cfg.AgentRanges.f_local_min, cfg.AgentRanges.f_local_max)\n",
    "        m_loc   = rng.uniform(cfg.AgentRanges.m_local_min, cfg.AgentRanges.m_local_max)\n",
    "        agents.append(Agent(agent_id=i, f_local=f_loc, m_local=m_loc, lam_sec=lam_sec))\n",
    "    return agents\n",
    "\n",
    "# -------------------------\n",
    "# task features\n",
    "# -------------------------\n",
    "# def sample_task_features(cfg: GlobalConfig, rng: np.random.Generator) -> Dict[str, float]:\n",
    "#     d = cfg.TaskDist\n",
    "#     # Size (MB) and compute density (cycles/MB)\n",
    "#     b_mb   = lognormal_from_median_sigma_g(rng, d.b_median,   d.b_sigma_g)\n",
    "#     rho    = lognormal_from_median_sigma_g(rng, d.rho_median, d.rho_sigma_g)\n",
    "#     c_cyc  = b_mb * rho                              # total cycles\n",
    "#     L_req  = float(rng.uniform(d.L_req_min, d.L_req_max))\n",
    "#     mem_mb = lognormal_from_median_sigma_g(rng, d.mem_median, d.mem_sigma_g)\n",
    "\n",
    "#     modality = rng.choice([\"image\",\"video\",\"text\",\"sensor\"], p=[0.3,0.2,0.3,0.2])\n",
    "\n",
    "#     has_deadline = int(rng.random() < d.p_deadline)\n",
    "#     deadline_s   = np.nan\n",
    "#     if has_deadline:\n",
    "#         deadline_s = float(rng.uniform(d.deadline_min, d.deadline_max))\n",
    "\n",
    "#     non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "#     split_ratio = float(rng.uniform(d.split_ratio_min, d.split_ratio_max)) if non_atomic else 0.0\n",
    "\n",
    "#     return dict(\n",
    "#         b_mb=b_mb, rho=rho, c_cycles=c_cyc, L_req=L_req, mem_mb=mem_mb, modality=modality,\n",
    "#         has_deadline=has_deadline, deadline_s=deadline_s, non_atomic=non_atomic, split_ratio=split_ratio\n",
    "#     )\n",
    "\n",
    "\n",
    "# حذف L_req از تابع نمونه‌گیری ویژگی‌ها:\n",
    "def sample_task_features(cfg: GlobalConfig, rng: np.random.Generator) -> Dict[str, float]:\n",
    "    d = cfg.TaskDist\n",
    "    b_mb   = lognormal_from_median_sigma_g(rng, d.b_median,   d.b_sigma_g)\n",
    "    rho    = lognormal_from_median_sigma_g(rng, d.rho_median, d.rho_sigma_g)\n",
    "    c      = b_mb * rho                              # total cycles\n",
    "    mem_mb = lognormal_from_median_sigma_g(rng, d.mem_median, d.mem_sigma_g)\n",
    "    modality = rng.choice([\"image\",\"video\",\"text\",\"sensor\"], p=[0.3,0.2,0.3,0.2])\n",
    "    has_deadline = int(rng.random() < d.p_deadline)\n",
    "    deadline_s   = np.nan\n",
    "    if has_deadline:\n",
    "        deadline_s = float(rng.uniform(d.deadline_min, d.deadline_max))\n",
    "    non_atomic = int(rng.random() < d.p_non_atomic)\n",
    "    split_ratio = float(rng.uniform(d.split_ratio_min, d.split_ratio_max)) if non_atomic else 0.0\n",
    "    \n",
    "    # حذف L_req از return\n",
    "    return dict(\n",
    "        b_mb=b_mb, rho=rho, c_cycles=c, mem_mb=mem_mb, modality=modality,\n",
    "        has_deadline=has_deadline, deadline_s=deadline_s, non_atomic=non_atomic, split_ratio=split_ratio\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# episode generator (arrivals only)\n",
    "# -------------------------\n",
    "def run_episode(cfg: GlobalConfig, agents: List[Agent], episode_id: int = 0) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate time-stamped arrivals and task features for one episode.\n",
    "    \"\"\"\n",
    "    rng_local = np.random.default_rng(cfg.Episode.seed + episode_id)\n",
    "\n",
    "    rows_episodes: List[Dict] = []\n",
    "    rows_agents:   List[Dict] = [asdict(a) for a in agents]\n",
    "    rows_arrivals: List[Dict] = []\n",
    "    rows_tasks:    List[Dict] = []\n",
    "\n",
    "    Delta   = cfg.Episode.Delta\n",
    "    T_slots = cfg.Episode.T_slots\n",
    "\n",
    "    task_id_counter = 0\n",
    "\n",
    "    for t in range(T_slots):\n",
    "        t_time = t * Delta\n",
    "        for a in agents:\n",
    "            # per-slot rate from per-second rate:\n",
    "            lam_slot = a.lam_sec * Delta\n",
    "            n_new = rng_local.poisson(lam=lam_slot)\n",
    "            if n_new <= 0:\n",
    "                continue\n",
    "            for _ in range(n_new):\n",
    "                feat = sample_task_features(cfg, rng_local)\n",
    "\n",
    "                # absolute deadline time (NaN if none)\n",
    "                if np.isnan(feat[\"deadline_s\"]):\n",
    "                    deadline_time = np.nan\n",
    "                else:\n",
    "                    deadline_time = t_time + feat[\"deadline_s\"]\n",
    "\n",
    "                action_space_hint = \"continuous\" if feat[\"non_atomic\"] == 1 else \"discrete\"\n",
    "\n",
    "                rows_arrivals.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"t_slot\": t,\n",
    "                    \"t_time\": t_time,\n",
    "                    \"agent_id\": a.agent_id,\n",
    "                    \"task_id\": task_id_counter\n",
    "                })\n",
    "\n",
    "                rows_tasks.append({\n",
    "                    \"scenario\": cfg.name,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"task_id\": task_id_counter,\n",
    "                    \"agent_id\": a.agent_id,\n",
    "                    \"t_arrival_slot\": t,\n",
    "                    \"t_arrival_time\": t_time,\n",
    "                    \"b_mb\": feat[\"b_mb\"],\n",
    "                    \"rho_cyc_per_mb\": feat[\"rho\"],\n",
    "                    \"c_cycles\": feat[\"c_cycles\"],\n",
    "                    # \"L_req\": feat[\"L_req\"],\n",
    "                    \"mem_mb\": feat[\"mem_mb\"],\n",
    "                    \"modality\": feat[\"modality\"],\n",
    "                    \"has_deadline\": feat[\"has_deadline\"],\n",
    "                    \"deadline_s\": feat[\"deadline_s\"],\n",
    "                    \"deadline_time\": deadline_time,\n",
    "                    \"non_atomic\": feat[\"non_atomic\"],\n",
    "                    \"split_ratio\": feat[\"split_ratio\"],\n",
    "                    \"action_space_hint\": action_space_hint\n",
    "                })\n",
    "\n",
    "                task_id_counter += 1\n",
    "\n",
    "    rows_episodes.append({\n",
    "        \"scenario\": cfg.name,\n",
    "        \"episode_id\": episode_id,\n",
    "        \"Delta\": Delta,\n",
    "        \"T_slots\": T_slots,\n",
    "        \"hours\": T_slots * Delta / 3600.0,\n",
    "        \"N_agents\": len(agents),\n",
    "        \"seed\": cfg.Episode.seed + episode_id\n",
    "    })\n",
    "\n",
    "    episodes_df = pd.DataFrame(rows_episodes)\n",
    "    agents_df   = pd.DataFrame(rows_agents)\n",
    "    arrivals_df = pd.DataFrame(rows_arrivals)\n",
    "    tasks_df    = pd.DataFrame(rows_tasks)\n",
    "\n",
    "    # Post-process: assign task classes using quantiles computed on this batch\n",
    "    tasks_df = assign_task_classes(tasks_df)\n",
    "\n",
    "    # Optimize dtypes (optional but useful)\n",
    "    if len(tasks_df):\n",
    "        tasks_df[\"modality\"] = tasks_df[\"modality\"].astype(\"category\")\n",
    "        tasks_df[\"action_space_hint\"] = tasks_df[\"action_space_hint\"].astype(\"category\")\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes_df,\n",
    "        \"agents\":   agents_df,\n",
    "        \"arrivals\": arrivals_df,\n",
    "        \"tasks\":    tasks_df\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_task_classes(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign class_label based on quantiles in the generated batch:\n",
    "      - time_sensitive   : has_deadline == 1 and deadline_s <= q30(deadline_s of non-NaN)\n",
    "      - data_heavy       : b_mb >= q70(b_mb)\n",
    "      - compute_heavy    : c_cycles >= q70(c_cycles)\n",
    "      - normal           : otherwise\n",
    "    Priority: time_sensitive > data_heavy > compute_heavy > normal\n",
    "    \"\"\"\n",
    "    if tasks_df.empty:\n",
    "        tasks_df[\"class_label\"] = pd.Categorical([], categories=[\n",
    "            \"time_sensitive\", \"data_heavy\", \"compute_heavy\", \"normal\"\n",
    "        ])\n",
    "        return tasks_df\n",
    "\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    def safe_quantile(series: pd.Series, q: float, default: float) -> float:\n",
    "        series = series.dropna()\n",
    "        if len(series) == 0:\n",
    "            return default\n",
    "        return float(series.quantile(q))\n",
    "\n",
    "    q_deadline_30 = safe_quantile(df.loc[df[\"has_deadline\"] == 1, \"deadline_s\"], 0.30, default=0.5)\n",
    "    q_b_70        = safe_quantile(df[\"b_mb\"], 0.70, default=float(df[\"b_mb\"].median() if len(df) else 3.0))\n",
    "    q_c_70        = safe_quantile(df[\"c_cycles\"], 0.70, default=float(df[\"c_cycles\"].median() if len(df) else 1.0))\n",
    "\n",
    "    is_time_sens  = (df[\"has_deadline\"] == 1) & (df[\"deadline_s\"] <= q_deadline_30)\n",
    "    is_data_heavy = (df[\"b_mb\"] >= q_b_70)\n",
    "    is_comp_heavy = (df[\"c_cycles\"] >= q_c_70)\n",
    "\n",
    "    class_label = np.full(len(df), \"normal\", dtype=object)\n",
    "    class_label[is_comp_heavy] = \"compute_heavy\"\n",
    "    class_label[is_data_heavy] = \"data_heavy\"\n",
    "    class_label[is_time_sens]  = \"time_sensitive\"\n",
    "\n",
    "    df[\"class_label\"] = pd.Categorical(class_label, categories=[\n",
    "        \"time_sensitive\", \"data_heavy\", \"compute_heavy\", \"normal\"\n",
    "    ], ordered=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# save & plotting utilities\n",
    "# -------------------------\n",
    "def save_dataset(dfs: Dict[str, pd.DataFrame], prefix: str = \"\", out_dir: str = \".\") -> Dict[str, str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    paths: Dict[str, str] = {}\n",
    "    for name, df in dfs.items():\n",
    "        csv_path = os.path.join(out_dir, f\"{prefix}{name}.csv\")\n",
    "        # pq_path  = os.path.join(out_dir, f\"{prefix}{name}.parquet\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        # try:\n",
    "        #     df.to_parquet(pq_path, index=False)\n",
    "        # except Exception:\n",
    "        #     pq_path = \"\"\n",
    "        paths[name + \"_csv\"] = csv_path\n",
    "        # if pq_path:\n",
    "        #     paths[name + \"_parquet\"] = pq_path\n",
    "    return paths\n",
    "\n",
    "\n",
    "def save_meta(cfg: GlobalConfig, prefix: str = \"\", out_dir: str = \".\") -> str:\n",
    "    meta = {\n",
    "        \"scenario\": cfg.name,\n",
    "        \"seed\": cfg.Episode.seed,\n",
    "        \"Delta\": cfg.Episode.Delta,\n",
    "        \"T_slots\": cfg.Episode.T_slots,\n",
    "        \"N_agents\": cfg.N_agents,\n",
    "        \"AgentRanges\": asdict(cfg.AgentRanges),\n",
    "        \"TaskDist\": asdict(cfg.TaskDist),\n",
    "        \"notes\": {\n",
    "            \"rates_unit\": \"lam_sec is per-second; per-slot rate = lam_sec * Delta\",\n",
    "            \"b_unit\": \"MB\",\n",
    "            \"rho_unit\": \"cycles per MB\",\n",
    "            \"c_unit\": \"cycles\",\n",
    "            \"deadline_s\": \"relative seconds; deadline_time = t_arrival_time + deadline_s\",\n",
    "            \"non_atomic\": \"1 means task can be split; split_ratio = fraction of size that can be split\"\n",
    "        }\n",
    "    }\n",
    "    path = os.path.join(out_dir, f\"{prefix}dataset_meta.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    return path\n",
    "\n",
    "\n",
    "def summarize_and_plot(dfs: Dict[str, pd.DataFrame], out_dir: str, prefix: str) -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tasks = dfs[\"tasks\"].copy()\n",
    "    arrivals = dfs[\"arrivals\"].copy()\n",
    "\n",
    "    # ---- summary stats\n",
    "    def q(s, p):\n",
    "        return float(np.nanquantile(s, p)) if len(s.dropna()) else float(\"nan\")\n",
    "\n",
    "    summary = {\n",
    "        \"n_tasks\": [len(tasks)],\n",
    "        \"n_arrivals\": [len(arrivals)],\n",
    "        \"b_mb_median\": [q(tasks[\"b_mb\"], 0.5)],\n",
    "        \"b_mb_p90\": [q(tasks[\"b_mb\"], 0.9)],\n",
    "        \"rho_median\": [q(tasks[\"rho_cyc_per_mb\"], 0.5)],\n",
    "        \"c_cycles_median\": [q(tasks[\"c_cycles\"], 0.5)],\n",
    "        \"deadline_share\": [float((tasks[\"has_deadline\"]==1).mean()) if len(tasks) else float(\"nan\")],\n",
    "        \"non_atomic_share\": [float((tasks[\"non_atomic\"]==1).mean()) if len(tasks) else float(\"nan\")]\n",
    "    }\n",
    "    pd.DataFrame(summary).to_csv(os.path.join(out_dir, f\"{prefix}summary_stats.csv\"), index=False)\n",
    "\n",
    "    # ---- plots (each in its own figure)\n",
    "    def hist_plot(series: pd.Series, title: str, fname: str, logx: bool=False):\n",
    "        plt.figure()\n",
    "        s = series.dropna()\n",
    "        if len(s) == 0:\n",
    "            plt.title(title + \" (no data)\")\n",
    "        else:\n",
    "            # Use automatic bins; avoid specifying colors to keep neutral styling\n",
    "            plt.hist(s, bins=50)\n",
    "            if logx:\n",
    "                plt.xscale('log')\n",
    "            plt.title(title)\n",
    "            plt.xlabel(title)\n",
    "            plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, fname), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    hist_plot(tasks[\"b_mb\"],            title=\"Task size (MB)\",               fname=f\"{prefix}hist_b_mb.png\", logx=True)\n",
    "    hist_plot(tasks[\"rho_cyc_per_mb\"],  title=\"Compute density (cycles/MB)\",  fname=f\"{prefix}hist_rho.png\",  logx=True)\n",
    "    hist_plot(tasks[\"c_cycles\"],        title=\"Total cycles\",                  fname=f\"{prefix}hist_c_cycles.png\", logx=True)\n",
    "    hist_plot(tasks[\"deadline_s\"],      title=\"Deadline (s)\",                  fname=f\"{prefix}hist_deadline_s.png\", logx=False)\n",
    "    hist_plot(tasks.loc[tasks[\"non_atomic\"]==1, \"split_ratio\"], title=\"Split ratio (only non-atomic)\", fname=f\"{prefix}hist_split_ratio.png\", logx=False)\n",
    "\n",
    "    # arrivals per agent\n",
    "    if len(arrivals):\n",
    "        per_agent = arrivals.groupby(\"agent_id\").size()\n",
    "        plt.figure()\n",
    "        plt.bar(per_agent.index.astype(str), per_agent.values)\n",
    "        plt.title(\"Arrivals per agent\")\n",
    "        plt.xlabel(\"agent_id\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"{prefix}bar_arrivals_per_agent.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# scenario presets (light / moderate / heavy)\n",
    "# -------------------------\n",
    "HOURS = 1\n",
    "DEFAULT_DELTA = 1.0\n",
    "DEFAULT_T_SLOTS = int(HOURS * 3600 / DEFAULT_DELTA)\n",
    "\n",
    "BASE_EPISODE = EpisodeConf(Delta=DEFAULT_DELTA, T_slots=DEFAULT_T_SLOTS, seed=GLOBAL_SEED)\n",
    "BASE_AGENT_RANGES = AgentRanges(\n",
    "    lam_sec_min=0.02, lam_sec_max=0.80,\n",
    "    f_local_min=0.8e9, f_local_max=2.4e9,\n",
    "    m_local_min=3e9,  m_local_max=8e9\n",
    ")\n",
    "BASE_TASK_DIST = TaskFeatureDist()\n",
    "\n",
    "SCENARIOS: List[GlobalConfig] = [\n",
    "    GlobalConfig(\n",
    "        name=\"light\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 101),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.01, lam_sec_max=0.05),\n",
    "        TaskDist=replace(BASE_TASK_DIST,\n",
    "            b_median=2.0, b_sigma_g=0.55,\n",
    "            rho_median=1.0e9, rho_sigma_g=0.45,\n",
    "            p_deadline=0.15, deadline_min=0.8, deadline_max=2.0,\n",
    "            p_non_atomic=0.25, split_ratio_min=0.25, split_ratio_max=0.75)\n",
    "    ),\n",
    "    GlobalConfig(\n",
    "        name=\"moderate\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 202),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.05, lam_sec_max=0.20),\n",
    "        TaskDist=replace(BASE_TASK_DIST,\n",
    "            b_median=3.0, b_sigma_g=0.60,\n",
    "            rho_median=1.2e9, rho_sigma_g=0.50,\n",
    "            p_deadline=0.25, deadline_min=0.5, deadline_max=1.5,\n",
    "            p_non_atomic=0.35, split_ratio_min=0.30, split_ratio_max=0.80)\n",
    "    ),\n",
    "    GlobalConfig(\n",
    "        name=\"heavy\",\n",
    "        N_agents=18,\n",
    "        Episode=replace(BASE_EPISODE, seed=GLOBAL_SEED + 303),\n",
    "        AgentRanges=replace(BASE_AGENT_RANGES, lam_sec_min=0.20, lam_sec_max=0.80),\n",
    "        TaskDist=replace(BASE_TASK_DIST,\n",
    "            b_median=5.0, b_sigma_g=0.70,\n",
    "            rho_median=1.5e9, rho_sigma_g=0.55,\n",
    "            p_deadline=0.35, deadline_min=0.3, deadline_max=1.0,\n",
    "            p_non_atomic=0.45, split_ratio_min=0.40, split_ratio_max=0.85)\n",
    "    )\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# main driver\n",
    "# -------------------------\n",
    "def main_generate(cfg: GlobalConfig, episodes: int = 1, out_root: str = \"./datasets\") -> Dict[str, str]:\n",
    "    \"\"\"Generate 'episodes' episodes for one scenario (fixed agent pool per scenario).\"\"\"\n",
    "    out_dir = os.path.join(out_root, cfg.name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # build agents once per scenario to keep them consistent across its episodes\n",
    "    rng_agents = np.random.default_rng(cfg.Episode.seed + 10_000)\n",
    "    agents = build_agents(cfg, rng_agents)\n",
    "\n",
    "    all_paths: Dict[str, str] = {}\n",
    "    for ep in range(episodes):\n",
    "        dfs = run_episode(cfg, agents, episode_id=ep)\n",
    "        prefix = f\"{cfg.name}_ep{ep}_\"\n",
    "        paths = save_dataset(dfs, prefix=prefix, out_dir=out_dir)\n",
    "        summarize_and_plot(dfs, out_dir=out_dir, prefix=prefix)\n",
    "        all_paths.update({f\"ep{ep}_{k}\": v for k, v in paths.items()})\n",
    "\n",
    "    meta_path = save_meta(cfg, prefix=f\"{cfg.name}_\", out_dir=out_dir)\n",
    "    all_paths[\"meta\"] = meta_path\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "def generate_all_scenarios(episodes_each: int = 1, out_root: str = \"./datasets\") -> Dict[str, Dict[str, str]]:\n",
    "    results: Dict[str, Dict[str, str]] = {}\n",
    "    for cfg in SCENARIOS:\n",
    "        results[cfg.name] = main_generate(cfg, episodes=episodes_each, out_root=out_root)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # change episodes_each if you want multiple episodes per scenario\n",
    "    out = generate_all_scenarios(episodes_each=1, out_root=\"./datasets\")\n",
    "    print(json.dumps(out, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
