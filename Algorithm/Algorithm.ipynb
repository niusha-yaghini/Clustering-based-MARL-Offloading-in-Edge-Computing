{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from typing import Dict, Any, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Prepare data and configure the environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1. Data Loading (Data I/O) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "dataset_dir = '../Data_Generator/datasets'\n",
    "topology_dir = '../Topology_Generator/topologies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Global container\n",
    "datasets = {}\n",
    "\n",
    "def load_datasets_from_directory(dataset_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Build 'episode-first' structure:\n",
    "    datasets = {\n",
    "        \"ep_000\": {\n",
    "            \"light\":   { \"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df },\n",
    "            \"moderate\":{ ... },\n",
    "            \"heavy\":   { ... }\n",
    "        },\n",
    "        \"ep_001\": { ... },\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Step 1 — detect scenarios (light/moderate/heavy/...)\n",
    "    scenarios = [\n",
    "        name for name in os.listdir(dataset_dir)\n",
    "        if os.path.isdir(os.path.join(dataset_dir, name))\n",
    "    ]\n",
    "\n",
    "    # Step 2 — load per scenario and per episode (ep_XXX)\n",
    "    scenario_to_episodes = {}\n",
    "    for scenario in scenarios:\n",
    "        scn_path = os.path.join(dataset_dir, scenario)\n",
    "        ep_dirs = sorted([\n",
    "            ep for ep in os.listdir(scn_path)\n",
    "            if os.path.isdir(os.path.join(scn_path, ep)) and ep.startswith(\"ep_\")\n",
    "        ])\n",
    "        if not ep_dirs and verbose:\n",
    "            print(f\"[warn] no ep_* folders found under scenario '{scenario}'\")\n",
    "\n",
    "        scenario_to_episodes[scenario] = {}\n",
    "        for ep_name in ep_dirs:\n",
    "            ep_path = os.path.join(scn_path, ep_name)\n",
    "            try:\n",
    "                scenario_to_episodes[scenario][ep_name] = {\n",
    "                    \"episodes\": pd.read_csv(os.path.join(ep_path, \"episodes.csv\")),\n",
    "                    \"agents\":   pd.read_csv(os.path.join(ep_path, \"agents.csv\")),\n",
    "                    \"arrivals\": pd.read_csv(os.path.join(ep_path, \"arrivals.csv\")),\n",
    "                    \"tasks\":    pd.read_csv(os.path.join(ep_path, \"tasks.csv\")),\n",
    "                }\n",
    "            except FileNotFoundError as e:\n",
    "                if verbose:\n",
    "                    print(f\"[error] missing CSV in {ep_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Step 3 — invert structure: episodes → scenarios\n",
    "    datasets.clear()\n",
    "    for scenario, eps in scenario_to_episodes.items():\n",
    "        for ep_name, dfs in eps.items():\n",
    "            if ep_name not in datasets:\n",
    "                datasets[ep_name] = {}\n",
    "            datasets[ep_name][scenario] = dfs\n",
    "\n",
    "    # Optional summary printing\n",
    "    if verbose:\n",
    "        print(\"=== Dataset Summary (episode-first) ===\")\n",
    "        print(f\"episodes: {len(datasets)}  | scenarios detected: {len(scenarios)} -> {sorted(scenarios)}\")\n",
    "        for ep_name in sorted(datasets.keys()):\n",
    "            scenarios_here = sorted(datasets[ep_name].keys())\n",
    "            print(f\"  - {ep_name}: scenarios = {scenarios_here}\")\n",
    "            for scn in scenarios_here:\n",
    "                dfs = datasets[ep_name][scn]\n",
    "                n_ep   = len(dfs['episodes'])\n",
    "                n_ag   = len(dfs['agents'])\n",
    "                n_arr  = len(dfs['arrivals'])\n",
    "                n_task = len(dfs['tasks'])\n",
    "                print(f\"      {scn:9s} → episodes:{n_ep:3d}  agents:{n_ag:4d}  arrivals:{n_arr:6d}  tasks:{n_task:6d}\")\n",
    "        print(\"=======================================\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Summary (episode-first) ===\n",
      "episodes: 1  | scenarios detected: 3 -> ['heavy', 'light', 'moderate']\n",
      "  - ep_000: scenarios = ['heavy', 'light', 'moderate']\n",
      "      heavy     → episodes:  1  agents:  18  arrivals: 30636  tasks: 30636\n",
      "      light     → episodes:  1  agents:  18  arrivals:  2113  tasks:  2113\n",
      "      moderate  → episodes:  1  agents:  18  arrivals:  8262  tasks:  8262\n",
      "=======================================\n",
      "\n",
      "[info] printing from episode='ep_000', scenario='heavy'\n",
      "\n",
      "agents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lam_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>0.708673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>0.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>0.228174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>0.310369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>0.548990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local      m_local   lam_sec\n",
       "0         0  1.741183e+09  5713.849721  0.708673\n",
       "1         1  1.352326e+09  4566.428755  0.234989\n",
       "2         2  1.726668e+09  5815.120004  0.228174\n",
       "3         3  1.543616e+09  3539.850245  0.310369\n",
       "4         4  1.130883e+09  4161.367769  0.548990"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   agent_id  18 non-null     int64  \n",
      " 1   f_local   18 non-null     float64\n",
      " 2   m_local   18 non-null     float64\n",
      " 3   lam_sec   18 non-null     float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 704.0 bytes\n",
      "\n",
      "arrivals:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>t_slot</th>\n",
       "      <th>t_time</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>task_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  t_slot  t_time  agent_id  task_id\n",
       "0    heavy           0       0     0.0         0        0\n",
       "1    heavy           0       0     0.0         1        1\n",
       "2    heavy           0       0     0.0         4        2\n",
       "3    heavy           0       0     0.0         7        3\n",
       "4    heavy           0       0     0.0        10        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30636 entries, 0 to 30635\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    30636 non-null  object \n",
      " 1   episode_id  30636 non-null  int64  \n",
      " 2   t_slot      30636 non-null  int64  \n",
      " 3   t_time      30636 non-null  float64\n",
      " 4   agent_id    30636 non-null  int64  \n",
      " 5   task_id     30636 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 1.4+ MB\n",
      "\n",
      "episodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>Delta</th>\n",
       "      <th>T_slots</th>\n",
       "      <th>hours</th>\n",
       "      <th>N_agents</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  Delta  T_slots  hours  N_agents  seed\n",
       "0    heavy           0    1.0     3600    1.0        18   345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    1 non-null      object \n",
      " 1   episode_id  1 non-null      int64  \n",
      " 2   Delta       1 non-null      float64\n",
      " 3   T_slots     1 non-null      int64  \n",
      " 4   hours       1 non-null      float64\n",
      " 5   N_agents    1 non-null      int64  \n",
      " 6   seed        1 non-null      int64  \n",
      "dtypes: float64(2), int64(4), object(1)\n",
      "memory usage: 184.0+ bytes\n",
      "\n",
      "tasks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539704</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb modality  has_deadline  \\\n",
       "0   7.202096    9.727147e+08  7.005585e+09  66.611010   sensor             1   \n",
       "1   5.479984    1.314973e+09  7.206031e+09  77.928800    image             1   \n",
       "2   8.421977    2.500222e+09  2.105681e+10  72.966446     text             1   \n",
       "3   6.324986    1.779582e+09  1.125583e+10  56.492900   sensor             1   \n",
       "4  11.473269    1.087572e+09  1.247800e+10  73.389854   sensor             1   \n",
       "\n",
       "   deadline_s  deadline_time  non_atomic  split_ratio action_space_hint  \n",
       "0    0.800726       0.800726           0     0.000000          discrete  \n",
       "1    0.615113       0.615113           0     0.000000          discrete  \n",
       "2    0.323007       0.323007           1     0.539704        continuous  \n",
       "3    0.481587       0.481587           0     0.000000          discrete  \n",
       "4    0.594564       0.594564           0     0.000000          discrete  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30636 entries, 0 to 30635\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   scenario           30636 non-null  object \n",
      " 1   episode_id         30636 non-null  int64  \n",
      " 2   task_id            30636 non-null  int64  \n",
      " 3   agent_id           30636 non-null  int64  \n",
      " 4   t_arrival_slot     30636 non-null  int64  \n",
      " 5   t_arrival_time     30636 non-null  float64\n",
      " 6   b_mb               30636 non-null  float64\n",
      " 7   rho_cyc_per_mb     30636 non-null  float64\n",
      " 8   c_cycles           30636 non-null  float64\n",
      " 9   mem_mb             30636 non-null  float64\n",
      " 10  modality           30636 non-null  object \n",
      " 11  has_deadline       30636 non-null  int64  \n",
      " 12  deadline_s         10673 non-null  float64\n",
      " 13  deadline_time      10673 non-null  float64\n",
      " 14  non_atomic         30636 non-null  int64  \n",
      " 15  split_ratio        30636 non-null  float64\n",
      " 16  action_space_hint  30636 non-null  object \n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# ---- load all datasets (episode-first) ----\n",
    "datasets = load_datasets_from_directory(dataset_dir, verbose=True)\n",
    "\n",
    "# ---- choose an episode and a scenario for printing ----\n",
    "# pick first available episode if you don't want to hardcode\n",
    "ep_name = sorted(datasets.keys())[0] if datasets else None\n",
    "scenario = \"heavy\"  # you can change to \"light\"/\"moderate\" if needed\n",
    "\n",
    "if ep_name is not None and scenario in datasets[ep_name]:\n",
    "    print(f\"\\n[info] printing from episode='{ep_name}', scenario='{scenario}'\")\n",
    "\n",
    "    print(\"\\nagents:\")\n",
    "    display(datasets[ep_name][scenario]['agents'].head())\n",
    "    datasets[ep_name][scenario]['agents'].info()\n",
    "\n",
    "    print(\"\\narrivals:\")\n",
    "    display(datasets[ep_name][scenario]['arrivals'].head())\n",
    "    datasets[ep_name][scenario]['arrivals'].info()\n",
    "\n",
    "    print(\"\\nepisodes:\")\n",
    "    display(datasets[ep_name][scenario]['episodes'].head())\n",
    "    datasets[ep_name][scenario]['episodes'].info()\n",
    "\n",
    "    print(\"\\ntasks:\")\n",
    "    display(datasets[ep_name][scenario]['tasks'].head())\n",
    "    datasets[ep_name][scenario]['tasks'].info()\n",
    "else:\n",
    "    print(\"[error] no datasets found or requested scenario is missing for the chosen episode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topologies = {}\n",
    "\n",
    "def load_topologies_from_directory(topology_dir):\n",
    "    \n",
    "    for topology_name in os.listdir(topology_dir):\n",
    "        topology_path = os.path.join(topology_dir, topology_name)\n",
    "        \n",
    "        # Only process directories\n",
    "        if os.path.isdir(topology_path):\n",
    "            topology_json_path = os.path.join(topology_path, \"topology.json\")\n",
    "            meta_json_path = os.path.join(topology_path, \"topology_meta.json\")\n",
    "            connection_matrix_csv_path = os.path.join(topology_path, \"connection_matrix.csv\")\n",
    "            \n",
    "             # --- Load JSON & CSV files ---\n",
    "            topology_data = None\n",
    "            meta_data = None\n",
    "            with open(topology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                topology_data = json.load(f)\n",
    "            with open(meta_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta_data = json.load(f)\n",
    "            \n",
    "            # The first column is just for displaying row names, not part of the capacity matrix. \n",
    "            # So the best way is to index the first column. (index_col=0)\n",
    "            connection_matrix = pd.read_csv(connection_matrix_csv_path, index_col=0)\n",
    "            \n",
    "            # Store the topology details and the loaded CSV\n",
    "            topologies[topology_name] = {\n",
    "                \"topology_data\": topology_data,\n",
    "                \"meta_data\": meta_data,\n",
    "                \"connection_matrix\": connection_matrix  # Store the loaded CSV data\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topology clustered -> connection_matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_0</th>\n",
       "      <th>mec_1</th>\n",
       "      <th>mec_2</th>\n",
       "      <th>mec_3</th>\n",
       "      <th>mec_4</th>\n",
       "      <th>mec_5</th>\n",
       "      <th>mec_6</th>\n",
       "      <th>mec_7</th>\n",
       "      <th>mec_8</th>\n",
       "      <th>mec_9</th>\n",
       "      <th>mec_10</th>\n",
       "      <th>mec_11</th>\n",
       "      <th>mec_12</th>\n",
       "      <th>mec_13</th>\n",
       "      <th>mec_14</th>\n",
       "      <th>mec_15</th>\n",
       "      <th>mec_16</th>\n",
       "      <th>mec_17</th>\n",
       "      <th>cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mec_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.798801</td>\n",
       "      <td>11.958295</td>\n",
       "      <td>11.470404</td>\n",
       "      <td>9.980195</td>\n",
       "      <td>8.814050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_1</th>\n",
       "      <td>9.798801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.136893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.609202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_2</th>\n",
       "      <td>11.958295</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>9.170700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.055348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_3</th>\n",
       "      <td>11.470404</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>11.920252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.851664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_4</th>\n",
       "      <td>9.980195</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.105383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.076301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mec_0      mec_1      mec_2      mec_3      mec_4      mec_5  \\\n",
       "mec_0   0.000000   9.798801  11.958295  11.470404   9.980195   8.814050   \n",
       "mec_1   9.798801   0.000000   8.375143  11.535600  10.702584   9.136893   \n",
       "mec_2  11.958295   8.375143   0.000000   8.814632   9.637714   9.170700   \n",
       "mec_3  11.470404  11.535600   8.814632   0.000000  10.377054  11.920252   \n",
       "mec_4   9.980195  10.702584   9.637714  10.377054   0.000000   8.105383   \n",
       "\n",
       "       mec_6  mec_7  mec_8  mec_9  mec_10  mec_11  mec_12  mec_13  mec_14  \\\n",
       "mec_0    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_1    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_2    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_3    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_4    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       mec_15  mec_16  mec_17       cloud  \n",
       "mec_0     0.0     0.0     0.0   98.912800  \n",
       "mec_1     0.0     0.0     0.0  104.609202  \n",
       "mec_2     0.0     0.0     0.0   92.055348  \n",
       "mec_3     0.0     0.0     0.0   84.851664  \n",
       "mec_4     0.0     0.0     0.0   99.076301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18 entries, mec_0 to mec_17\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   mec_0   18 non-null     float64\n",
      " 1   mec_1   18 non-null     float64\n",
      " 2   mec_2   18 non-null     float64\n",
      " 3   mec_3   18 non-null     float64\n",
      " 4   mec_4   18 non-null     float64\n",
      " 5   mec_5   18 non-null     float64\n",
      " 6   mec_6   18 non-null     float64\n",
      " 7   mec_7   18 non-null     float64\n",
      " 8   mec_8   18 non-null     float64\n",
      " 9   mec_9   18 non-null     float64\n",
      " 10  mec_10  18 non-null     float64\n",
      " 11  mec_11  18 non-null     float64\n",
      " 12  mec_12  18 non-null     float64\n",
      " 13  mec_13  18 non-null     float64\n",
      " 14  mec_14  18 non-null     float64\n",
      " 15  mec_15  18 non-null     float64\n",
      " 16  mec_16  18 non-null     float64\n",
      " 17  mec_17  18 non-null     float64\n",
      " 18  cloud   18 non-null     float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 2.8+ KB\n",
      "\n",
      "topology clustered -> topology_data\n",
      "{'number_of_servers': 18, 'private_cpu_capacities': [1417026512.123894, 1465835517.1380253, 1400296152.8672054, 1219863115.6758468, 1270376631.237273, 1348019534.1092064, 1786161289.9722981, 1380932007.7716885, 1739680378.9314957, 1606151704.3138125, 1722792016.067419, 1452119690.3718696, 1575323717.1732426, 1675176049.772993, 1465269239.5654364, 1565743740.4168038, 1715259832.5839007, 1225832428.456285], 'public_cpu_capacities': [768946447.3896191, 753418540.8443304, 702715947.6715652, 545197668.8935852, 670994513.1813464, 680525151.7601619, 811159071.5491732, 782943570.2288362, 501997671.50295806, 685198791.9005744, 822519853.1111488, 716138690.9607652, 822355072.296565, 556428866.3646028, 805978940.2755054, 790274917.5537901, 602437544.1146357, 709495765.381235], 'cloud_computational_capacity': 30000000000.0, 'connection_matrix': [[0.0, 9.798800609449902, 11.958295388571093, 11.470404384171585, 9.980195439399376, 8.8140500666217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.91279981562592], [9.798800609449902, 0.0, 8.375143239602235, 11.535600106180025, 10.702584376249934, 9.136893120492793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.60920207554163], [11.958295388571093, 8.375143239602235, 0.0, 8.814631907625701, 9.637714071509851, 9.170700424264968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 92.05534806322484], [11.470404384171585, 11.535600106180025, 8.814631907625701, 0.0, 10.37705391001423, 11.920252079923895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.85166371762908], [9.980195439399376, 10.702584376249934, 9.637714071509851, 10.37705391001423, 0.0, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0763009651688], [8.8140500666217, 9.136893120492793, 9.170700424264968, 11.920252079923895, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.0792791055983], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 9.566567838535661, 11.01158338193066, 10.132206014485726, 11.731761213672993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.87637059619391], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 0.0, 8.17265481777462, 11.317250164676846, 9.944330066939582, 11.698994881774604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.49730491373893], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.566567838535661, 8.17265481777462, 0.0, 11.919271797879857, 9.162931925532463, 11.22074688068535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.13111014716404], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.01158338193066, 11.317250164676846, 11.919271797879857, 0.0, 11.8680001020066, 11.393785145055949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 87.20532264682294], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.132206014485726, 9.944330066939582, 9.162931925532463, 11.8680001020066, 0.0, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.89380625765318], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.731761213672993, 11.698994881774604, 11.22074688068535, 11.393785145055949, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.16242880696842], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 10.155633210306574, 11.202595781660758, 10.69123291440641, 11.439129446848145, 94.68338717140213], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 0.0, 10.795539716597093, 9.382485758460357, 8.156515169162681, 10.108953651086749, 86.2148806997884], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.155633210306574, 10.795539716597093, 0.0, 8.183423253938845, 9.055460579237527, 8.857865582666564, 100.51802149006446], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.202595781660758, 9.382485758460357, 8.183423253938845, 0.0, 10.922031113226154, 8.777507498932518, 85.56329833615376], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.69123291440641, 8.156515169162681, 9.055460579237527, 10.922031113226154, 0.0, 11.48804541896262, 109.44902761831553], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.439129446848145, 10.108953651086749, 8.857865582666564, 8.777507498932518, 11.48804541896262, 0.0, 119.21419244342577]], 'time_step': 1.0, 'topology_type': 'clustered', 'skip_k': 5, 'symmetric': True, 'num_clusters': 3}\n",
      "\n",
      "topology clustered -> meta_data\n",
      "{'generated_at_utc': '2025-11-05T07:48:30Z', 'fingerprint': '9a3a6cc2768b6834', 'env': {'python': '3.10.9', 'user': 'niush'}, 'units': {'compute': 'CPU cycles per slot', 'links': 'MB per slot', 'time_step': 'seconds'}, 'notes': {'inputs_unit': {'compute': 'CPU cycles per second', 'links': 'MB per second'}, 'conversion': 'per_slot = per_second * time_step', 'topology_semantics': {'skip_connections': \"k-nearest ring; each MEC connects to next 'skip_k' neighbors on a circle\"}}, 'hyperparameters': {'number_of_servers': 18, 'time_step': 1.0, 'private_cpu_min': 1200000000.0, 'private_cpu_max': 1800000000.0, 'public_cpu_min': 500000000.0, 'public_cpu_max': 900000000.0, 'cpu_total_min': None, 'cpu_total_max': None, 'public_share': None, 'cloud_capacity': 30000000000.0, 'cloud_capacity_min': None, 'cloud_capacity_max': None, 'horiz_cap_min': 8.0, 'horiz_cap_max': 12.0, 'cloud_cap_min': 80.0, 'cloud_cap_max': 120.0, 'topology_type': 'clustered', 'skip_k': 5, 'symmetric': True, 'num_clusters': 3, 'inter_cluster_frac': 0.0, 'seed': 20251229}}\n"
     ]
    }
   ],
   "source": [
    "load_topologies_from_directory(topology_dir)\n",
    "\n",
    "print('topology clustered -> connection_matrix')\n",
    "display(topologies['clustered']['connection_matrix'].head())\n",
    "topologies['clustered']['connection_matrix'].info()\n",
    "\n",
    "print('\\ntopology clustered -> topology_data')\n",
    "print(topologies['clustered']['topology_data'])\n",
    "\n",
    "print('\\ntopology clustered -> meta_data')\n",
    "print(topologies['clustered']['meta_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2. Data Validation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data, we must validate that required columns exist and that IDs match properly.\n",
    "\n",
    "**The code below performs three layers of checks:** \n",
    "\n",
    "- Validate each dataset (episodes/agents/arrivals/tasks)\n",
    "\n",
    "- Validate each topology (JSON and connection matrix)\n",
    "\n",
    "- Validate dataset–topology pairs for unit alignment and overall consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Validation (episode-first aware)\n",
    "\n",
    "# ---------- Generic helpers ----------\n",
    "def _require(cond: bool, msg: str, errors: list):\n",
    "    # Collect errors instead of stopping at first failure\n",
    "    if not cond:\n",
    "        errors.append(msg)\n",
    "\n",
    "def _has_cols(df: pd.DataFrame, cols: list) -> bool:\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# ---------- Dataset-level validation ----------\n",
    "def validate_one_dataset(dataset_key: str, ds: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single dataset pack (episodes/agents/arrivals/tasks) for one (episode, scenario).\n",
    "    'dataset_key' is just a label for error messages, e.g. 'ep_000/heavy'.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds.get(\"episodes\")\n",
    "    agents   = ds.get(\"agents\")\n",
    "    arrivals = ds.get(\"arrivals\")\n",
    "    tasks    = ds.get(\"tasks\")\n",
    "\n",
    "    # 1) Presence checks\n",
    "    _require(isinstance(episodes, pd.DataFrame), f\"[{dataset_key}] episodes missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(agents,   pd.DataFrame), f\"[{dataset_key}] agents missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(arrivals, pd.DataFrame), f\"[{dataset_key}] arrivals missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(tasks,    pd.DataFrame), f\"[{dataset_key}] tasks missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 2) Required columns (based on your generators)\n",
    "    req_ep_cols  = [\"scenario\", \"episode_id\", \"Delta\", \"T_slots\", \"hours\", \"N_agents\", \"seed\"]\n",
    "    req_ag_cols  = [\"agent_id\", \"f_local\", \"m_local\", \"lam_sec\"]\n",
    "    req_ar_cols  = [\"scenario\", \"episode_id\", \"t_slot\", \"t_time\", \"agent_id\", \"task_id\"]\n",
    "    req_tk_cols  = [\n",
    "        \"scenario\",\"episode_id\",\"task_id\",\"agent_id\",\"t_arrival_slot\",\"t_arrival_time\",\n",
    "        \"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\",\"modality\",\n",
    "        \"has_deadline\",\"deadline_s\",\"deadline_time\",\"non_atomic\",\"split_ratio\",\"action_space_hint\"\n",
    "    ]\n",
    "    _require(_has_cols(episodes, req_ep_cols), f\"[{dataset_key}] episodes missing required columns\", errors)\n",
    "    _require(_has_cols(agents,   req_ag_cols), f\"[{dataset_key}] agents missing required columns\", errors)\n",
    "    _require(_has_cols(arrivals, req_ar_cols), f\"[{dataset_key}] arrivals missing required columns\", errors)\n",
    "    _require(_has_cols(tasks,    req_tk_cols), f\"[{dataset_key}] tasks missing required columns\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 3) Integrity checks\n",
    "    # unique task_id\n",
    "    _require(tasks[\"task_id\"].is_unique, f\"[{dataset_key}] task_id is not unique\", errors)\n",
    "\n",
    "    # agent id range & count vs episodes.N_agents\n",
    "    if len(agents):\n",
    "        min_id = agents[\"agent_id\"].min()\n",
    "        max_id = agents[\"agent_id\"].max()\n",
    "        expected_n = int(episodes[\"N_agents\"].iloc[0])\n",
    "        _require(min_id == 0, f\"[{dataset_key}] agent_id should start at 0 (got {min_id})\", errors)\n",
    "        _require(max_id == expected_n - 1,\n",
    "                 f\"[{dataset_key}] agent_id max should be N_agents-1 ({expected_n-1}), got {max_id}\", errors)\n",
    "\n",
    "    # cross refs\n",
    "    valid_agents = set(agents[\"agent_id\"].tolist())\n",
    "    bad_arr_agents = set(arrivals[\"agent_id\"]) - valid_agents\n",
    "    bad_task_agents = set(tasks[\"agent_id\"]) - valid_agents\n",
    "    _require(len(bad_arr_agents) == 0, f\"[{dataset_key}] arrivals contain unknown agent_id(s): {sorted(bad_arr_agents)}\", errors)\n",
    "    _require(len(bad_task_agents) == 0, f\"[{dataset_key}] tasks contain unknown agent_id(s): {sorted(bad_task_agents)}\", errors)\n",
    "\n",
    "    # non-negative task numerics\n",
    "    for col in [\"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            _require((tasks[col] >= 0).all(), f\"[{dataset_key}] tasks.{col} has negative values\", errors)\n",
    "\n",
    "    # deadline coherence\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        bad_deadline = tasks[(tasks[\"has_deadline\"] == 1) & ((tasks[\"deadline_s\"].isna()) | (tasks[\"deadline_s\"] <= 0))]\n",
    "        _require(len(bad_deadline) == 0, f\"[{dataset_key}] tasks with deadline have invalid deadline_s\", errors)\n",
    "\n",
    "    # single Delta / T_slots inside this (episode, scenario)\n",
    "    _require(episodes[\"Delta\"].nunique() == 1, f\"[{dataset_key}] multiple Delta values in episodes\", errors)\n",
    "    _require(episodes[\"T_slots\"].nunique() == 1, f\"[{dataset_key}] multiple T_slots in episodes\", errors)\n",
    "\n",
    "    # arrivals inside range\n",
    "    T_slots = int(episodes[\"T_slots\"].iloc[0])\n",
    "    _require(int(tasks[\"t_arrival_slot\"].max()) <= T_slots - 1,\n",
    "             f\"[{dataset_key}] t_arrival_slot exceeds T_slots-1\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Topology-level validation ----------\n",
    "def validate_one_topology(topology_name: str, topo_entry: dict) -> list:\n",
    "    errors = []\n",
    "    topo = topo_entry.get(\"topology_data\")\n",
    "    meta = topo_entry.get(\"meta_data\")\n",
    "    Mdf  = topo_entry.get(\"connection_matrix\")\n",
    "\n",
    "    _require(isinstance(topo, dict), f\"[{topology_name}] topology_data missing or not a dict\", errors)\n",
    "    _require(isinstance(meta, dict), f\"[{topology_name}] meta_data missing or not a dict\", errors)\n",
    "    _require(isinstance(Mdf,  pd.DataFrame), f\"[{topology_name}] connection_matrix CSV missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    req_keys = [\n",
    "        \"number_of_servers\",\"private_cpu_capacities\",\"public_cpu_capacities\",\n",
    "        \"cloud_computational_capacity\",\"connection_matrix\",\"time_step\"\n",
    "    ]\n",
    "    for k in req_keys:\n",
    "        _require(k in topo, f\"[{topology_name}] topology.json missing key: {k}\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K = int(topo[\"number_of_servers\"])\n",
    "    _require(len(topo[\"private_cpu_capacities\"]) == K, f\"[{topology_name}] private_cpu_capacities length != K\", errors)\n",
    "    _require(len(topo[\"public_cpu_capacities\"])  == K, f\"[{topology_name}] public_cpu_capacities length != K\", errors)\n",
    "\n",
    "    Mjson = topo[\"connection_matrix\"]\n",
    "    _require(isinstance(Mjson, list) and len(Mjson) == K and (K == 0 or len(Mjson[0]) == K+1),\n",
    "             f\"[{topology_name}] connection_matrix in JSON must be K x (K+1)\", errors)\n",
    "    _require(Mdf.shape == (K, K+1), f\"[{topology_name}] connection_matrix.csv shape must be K x (K+1)\", errors)\n",
    "\n",
    "    vert_csv = Mdf.iloc[:, K]\n",
    "    _require((vert_csv > 0).all(), f\"[{topology_name}] MEC->Cloud capacities must be > 0\", errors)\n",
    "    horiz_csv = Mdf.iloc[:, :K]\n",
    "    _require((horiz_csv.values >= 0).all(), f\"[{topology_name}] MEC<->MEC capacities contain negatives\", errors)\n",
    "\n",
    "    _require(\"time_step\" in topo, f\"[{topology_name}] missing time_step\", errors)\n",
    "    return errors\n",
    "\n",
    "# ---------- Pairwise validation (dataset <-> topology) ----------\n",
    "def validate_dataset_topology_pair(ep_name: str, scenario: str, ds: dict,\n",
    "                                   topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate alignment between one (episode, scenario) dataset and one topology.\n",
    "    Ensures Delta == time_step and basic feasibility checks.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds[\"episodes\"]\n",
    "    topo     = topo_entry[\"topology_data\"]\n",
    "    K        = int(topo[\"number_of_servers\"])\n",
    "\n",
    "    # Delta vs time_step\n",
    "    Delta = float(episodes[\"Delta\"].iloc[0])\n",
    "    time_step = float(topo[\"time_step\"])\n",
    "    _require(abs(Delta - time_step) < 1e-9,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] Delta ({Delta}) != time_step ({time_step})\", errors)\n",
    "\n",
    "    # Non-negative compute capacities\n",
    "    priv = topo[\"private_cpu_capacities\"]\n",
    "    pub  = topo[\"public_cpu_capacities\"]\n",
    "    cloud = topo[\"cloud_computational_capacity\"]\n",
    "    _require(all(x >= 0 for x in priv) and all(x >= 0 for x in pub) and cloud >= 0,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] negative compute capacities detected\", errors)\n",
    "\n",
    "    # Simple agent→MEC mapping (modulo) is within bounds\n",
    "    N_agents = int(episodes[\"N_agents\"].iloc[0])\n",
    "    mapped = [(aid % K) for aid in range(N_agents)] if K > 0 else []\n",
    "    _require(all(0 <= m < K for m in mapped) if mapped else True,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] agent->MEC mapping out of bounds\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Episode-level Delta consistency across scenarios ----------\n",
    "def validate_episode_delta_consistency(ep_name: str, ep_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    Check that all scenarios inside one episode share the same Delta and T_slots.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    deltas = set()\n",
    "    tslots = set()\n",
    "    for scenario, ds in ep_dict.items():\n",
    "        ep_df = ds[\"episodes\"]\n",
    "        if len(ep_df):\n",
    "            deltas.add(float(ep_df[\"Delta\"].iloc[0]))\n",
    "            tslots.add(int(ep_df[\"T_slots\"].iloc[0]))\n",
    "        else:\n",
    "            errors.append(f\"[{ep_name}/{scenario}] episodes.csv is empty\")\n",
    "    if len(deltas) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple Delta values across scenarios: {sorted(deltas)}\")\n",
    "    if len(tslots) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple T_slots values across scenarios: {sorted(tslots)}\")\n",
    "    return errors\n",
    "\n",
    "# ---------- Orchestrator over ALL datasets (episode-first) and ALL topologies ----------\n",
    "def validate_everything_episode_first(datasets: dict, topologies: dict) -> dict:\n",
    "    \"\"\"\n",
    "    'datasets' shape:\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...}\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    report = {\"datasets\": {}, \"episodes_consistency\": {}, \"topologies\": {}, \"pairs\": {}}\n",
    "\n",
    "    # 1) Validate each (episode, scenario)\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        report[\"datasets\"][ep_name] = {}\n",
    "        for scenario, dpack in ep_pack.items():\n",
    "            key = f\"{ep_name}/{scenario}\"\n",
    "            errs = validate_one_dataset(key, dpack)\n",
    "            report[\"datasets\"][ep_name][scenario] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 2) Episode-level Delta/T_slots consistency across scenarios\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        errs = validate_episode_delta_consistency(ep_name, ep_pack)\n",
    "        report[\"episodes_consistency\"][ep_name] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 3) Validate each topology\n",
    "    for tname, tpack in topologies.items():\n",
    "        errs = validate_one_topology(tname, tpack)\n",
    "        report[\"topologies\"][tname] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 4) Pairwise validation for every valid (ep, scenario) × valid topology\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        for scenario, dpack in ep_pack.items():\n",
    "            d_ok = report[\"datasets\"][ep_name][scenario][\"ok\"]\n",
    "            ep_ok = report[\"episodes_consistency\"][ep_name][\"ok\"]\n",
    "            for tname, tres in report[\"topologies\"].items():\n",
    "                key = f\"{ep_name}/{scenario}__{tname}\"\n",
    "                if d_ok and ep_ok and tres[\"ok\"]:\n",
    "                    errs = validate_dataset_topology_pair(ep_name, scenario, dpack, tname, topologies[tname])\n",
    "                    report[\"pairs\"][key] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "                else:\n",
    "                    report[\"pairs\"][key] = {\"ok\": False, \"errors\": [\"Skipped due to upstream invalid dataset/episode/topology.\"]}\n",
    "\n",
    "    return report\n",
    "\n",
    "# ---------- Pretty printer ----------\n",
    "def print_validation_report_episode_first(report: dict):\n",
    "    print(\"=== DATASETS (episode/scenario) ===\")\n",
    "    for ep_name, ep_res in report[\"datasets\"].items():\n",
    "        for scenario, info in ep_res.items():\n",
    "            status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "            print(f\"[{status}] {ep_name}/{scenario}\")\n",
    "            for e in info[\"errors\"]:\n",
    "                print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\")\n",
    "    for ep_name, info in report[\"episodes_consistency\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {ep_name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== TOPOLOGIES ===\")\n",
    "    for name, info in report[\"topologies\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\")\n",
    "    for key, info in report[\"pairs\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {key}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (episode/scenario) ===\n",
      "[OK] ep_000/heavy\n",
      "[OK] ep_000/light\n",
      "[OK] ep_000/moderate\n",
      "\n",
      "=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\n",
      "[OK] ep_000\n",
      "\n",
      "=== TOPOLOGIES ===\n",
      "[OK] clustered\n",
      "[OK] full_mesh\n",
      "[OK] sparse_ring\n",
      "\n",
      "=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\n",
      "[OK] ep_000/heavy__clustered\n",
      "[OK] ep_000/heavy__full_mesh\n",
      "[OK] ep_000/heavy__sparse_ring\n",
      "[OK] ep_000/light__clustered\n",
      "[OK] ep_000/light__full_mesh\n",
      "[OK] ep_000/light__sparse_ring\n",
      "[OK] ep_000/moderate__clustered\n",
      "[OK] ep_000/moderate__full_mesh\n",
      "[OK] ep_000/moderate__sparse_ring\n"
     ]
    }
   ],
   "source": [
    "# ---- run the new validator ----\n",
    "report = validate_everything_episode_first(datasets, topologies)\n",
    "print_validation_report_episode_first(report)\n",
    "\n",
    "all_ok = (\n",
    "    all(info[\"ok\"] for ep in report[\"datasets\"].values() for info in ep.values())\n",
    "    and all(info[\"ok\"] for info in report[\"episodes_consistency\"].values())\n",
    "    and all(info[\"ok\"] for info in report[\"topologies\"].values())\n",
    "    and all(info[\"ok\"] for info in report[\"pairs\"].values())\n",
    ")\n",
    "if not all_ok:\n",
    "    raise RuntimeError(\"Validation failed. See printed report for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3. Units Alignment </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we align units for all dataset scenarios (light / moderate / heavy) and all topologies (full_mesh / clustered / sparse_ring), and run several consistency checks. \n",
    "\n",
    "\n",
    "- In datasets: we take Delta from episodes.csv, create deadline_slots for tasks, and add helper per-slot columns for agents (f_local_slot)\n",
    "\n",
    "- In topologies: capacities are already per-slot (you multiplied by Δ in the generator), so we only verify that time_step == Delta and capacities are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Units Alignment (episode-first aware)\n",
    "\n",
    "# In this section, we align units for all dataset episodes and scenarios\n",
    "# and run consistency checks against all topologies.\n",
    "# - Datasets: use Delta from episodes.csv; add per-slot helpers:\n",
    "#     agents.f_local_slot (cycles/slot), tasks.deadline_slots (integer or NaN)\n",
    "# - Topologies: capacities are already per-slot (generator multiplied by Δ);\n",
    "#     we only verify time_step == Delta and non-negative capacities.\n",
    "\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "# ===== Helpers: safe getters =====\n",
    "def _get_delta(episodes_df: pd.DataFrame) -> float:\n",
    "    # Expect a single Delta value in episodes; take the first row\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain a 'Delta' column.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _ensure_numeric_positive(name: str, arr: np.ndarray):\n",
    "    # Basic sanity: finite and no negatives for capacities/links\n",
    "    if not np.isfinite(arr).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "    if (arr < 0).any():\n",
    "        raise ValueError(f\"{name} contains negative values.\")\n",
    "\n",
    "# ===== Alignment: per-dataset (one (episode, scenario) pack) =====\n",
    "def align_units_for_dataset(dataset: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given one dataset dict {\"episodes\",\"agents\",\"arrivals\",\"tasks\"},\n",
    "    return a copy with aligned/derived columns (per-slot helpers).\n",
    "    \"\"\"\n",
    "    episodes = dataset[\"episodes\"].copy()\n",
    "    agents   = dataset[\"agents\"].copy()\n",
    "    arrivals = dataset[\"arrivals\"].copy()\n",
    "    tasks    = dataset[\"tasks\"].copy()\n",
    "\n",
    "    Delta = _get_delta(episodes)\n",
    "\n",
    "    # Agents: add per-slot compute capacity helper (cycles/slot)\n",
    "    if \"f_local\" not in agents.columns:\n",
    "        raise ValueError(\"agents.csv must contain 'f_local'.\")\n",
    "    agents[\"f_local\"] = agents[\"f_local\"].astype(float)\n",
    "    agents[\"f_local_slot\"] = agents[\"f_local\"] * Delta\n",
    "\n",
    "    # Memory is MB; keep as float\n",
    "    if \"m_local\" in agents.columns:\n",
    "        agents[\"m_local\"] = agents[\"m_local\"].astype(float)\n",
    "\n",
    "    # Tasks: ensure integer arrival slot\n",
    "    if \"t_arrival_slot\" not in tasks.columns:\n",
    "        raise ValueError(\"tasks.csv must contain 't_arrival_slot'.\")\n",
    "    tasks[\"t_arrival_slot\"] = tasks[\"t_arrival_slot\"].astype(int)\n",
    "\n",
    "    # Build deadline_slots = ceil(deadline_s / Delta) when has_deadline == 1, else NaN\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        def _to_deadline_slots(row):\n",
    "            if int(row[\"has_deadline\"]) == 1 and np.isfinite(row[\"deadline_s\"]):\n",
    "                return int(math.ceil(float(row[\"deadline_s\"]) / Delta))\n",
    "            return np.nan\n",
    "        tasks[\"deadline_slots\"] = tasks.apply(_to_deadline_slots, axis=1)\n",
    "        # Keep as nullable integer when possible\n",
    "        try:\n",
    "            tasks[\"deadline_slots\"] = tasks[\"deadline_slots\"].astype(\"Int64\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Ensure key numeric task fields are floats\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            tasks[col] = tasks[col].astype(float)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes,\n",
    "        \"agents\":   agents,\n",
    "        \"arrivals\": arrivals,\n",
    "        \"tasks\":    tasks,\n",
    "    }\n",
    "\n",
    "# ===== Verification: per-topology against a target Delta =====\n",
    "def verify_topology_units(topology: Dict[str, Any], target_Delta: float) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ensure topology capacities are per-slot and consistent with dataset Delta:\n",
    "    - time_step == target_Delta\n",
    "    - shapes are valid (K x (K+1))\n",
    "    - capacities non-negative\n",
    "    Returns (ok, message).\n",
    "    \"\"\"\n",
    "    # time_step check\n",
    "    ts = float(topology.get(\"time_step\", -1.0))\n",
    "    if not np.isclose(ts, target_Delta, atol=1e-9):\n",
    "        return (False, f\"time_step mismatch (topology={ts}, dataset Delta={target_Delta})\")\n",
    "\n",
    "    # K and lists\n",
    "    K = int(topology.get(\"number_of_servers\", -1))\n",
    "    priv = np.array(topology.get(\"private_cpu_capacities\", []), dtype=float)\n",
    "    pub  = np.array(topology.get(\"public_cpu_capacities\", []), dtype=float)\n",
    "    cloud = float(topology.get(\"cloud_computational_capacity\", -1.0))\n",
    "    M = np.array(topology.get(\"connection_matrix\", []), dtype=float)\n",
    "\n",
    "    if K <= 0:\n",
    "        return (False, \"Invalid 'number_of_servers' (K<=0).\")\n",
    "    if priv.shape[0] != K or pub.shape[0] != K:\n",
    "        return (False, \"private/public capacities must have length K.\")\n",
    "    if M.shape != (K, K+1):\n",
    "        return (False, f\"connection_matrix shape must be (K, K+1), got {M.shape}.\")\n",
    "\n",
    "    # Non-negative checks\n",
    "    _ensure_numeric_positive(\"private_cpu_capacities\", priv)\n",
    "    _ensure_numeric_positive(\"public_cpu_capacities\",  pub)\n",
    "    if not np.isfinite(cloud) or cloud < 0:\n",
    "        return (False, \"cloud_computational_capacity must be non-negative and finite.\")\n",
    "    _ensure_numeric_positive(\"connection_matrix\", M)\n",
    "\n",
    "    return (True, \"topology verified (per-slot, consistent).\")\n",
    "\n",
    "# ===== Batch alignment for ALL datasets (episode-first) & ALL topologies =====\n",
    "def align_all_units_episode_first(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    topologies_by_name: Dict[str, Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input 'datasets_ep_first' shape:\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...}\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"datasets_aligned\": { ep_name: { scenario: aligned_pack, ... }, ... },\n",
    "          \"topology_checks\":  { topo_name: { ep_name: { scenario: {ok, message} } } }\n",
    "        }\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"datasets_aligned\": {},\n",
    "        \"topology_checks\":  {}\n",
    "    }\n",
    "\n",
    "    # Align datasets (episode/scenario)\n",
    "    for ep_name, ep_pack in datasets_ep_first.items():\n",
    "        out[\"datasets_aligned\"][ep_name] = {}\n",
    "        for scenario, ds in ep_pack.items():\n",
    "            try:\n",
    "                out[\"datasets_aligned\"][ep_name][scenario] = align_units_for_dataset(ds)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"[{ep_name}/{scenario}] dataset alignment failed: {e}\") from e\n",
    "\n",
    "    # Verify each topology against each (episode, scenario) Delta\n",
    "    for topo_name, topo_bundle in topologies_by_name.items():\n",
    "        topo_obj = topo_bundle.get(\"topology_data\", None)\n",
    "        if not isinstance(topo_obj, dict):\n",
    "            raise RuntimeError(f\"[{topo_name}] 'topology_data' missing or not a dict.\")\n",
    "        out[\"topology_checks\"][topo_name] = {}\n",
    "\n",
    "        for ep_name, ep_pack in out[\"datasets_aligned\"].items():\n",
    "            out[\"topology_checks\"][topo_name][ep_name] = {}\n",
    "            for scenario, aligned in ep_pack.items():\n",
    "                Delta = _get_delta(aligned[\"episodes\"])\n",
    "                ok, msg = verify_topology_units(topo_obj, Delta)\n",
    "                out[\"topology_checks\"][topo_name][ep_name][scenario] = {\"ok\": bool(ok), \"message\": msg}\n",
    "\n",
    "    return out\n",
    "\n",
    "# ===== Pretty printer (episode-first) =====\n",
    "def print_alignment_summary_episode_first(result: Dict[str, Any]):\n",
    "    # Datasets\n",
    "    print(\"=== DATASETS (aligned, episode/scenario) ===\")\n",
    "    for ep_name in sorted(result[\"datasets_aligned\"].keys()):\n",
    "        for scenario in sorted(result[\"datasets_aligned\"][ep_name].keys()):\n",
    "            ds = result[\"datasets_aligned\"][ep_name][scenario]\n",
    "            Delta = _get_delta(ds[\"episodes\"])\n",
    "            n_tasks = len(ds[\"tasks\"])\n",
    "            n_agents = len(ds[\"agents\"])\n",
    "            print(f\"[{ep_name}/{scenario}] Delta={Delta}  tasks={n_tasks}  agents={n_agents}\")\n",
    "\n",
    "    # Topologies\n",
    "    print(\"\\n=== TOPOLOGIES (checks vs each episode/scenario) ===\")\n",
    "    for topo_name, by_ep in result[\"topology_checks\"].items():\n",
    "        print(f\"Topology: {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            for scenario in sorted(by_ep[ep_name].keys()):\n",
    "                r = by_ep[ep_name][scenario]\n",
    "                flag = \"OK\" if r[\"ok\"] else \"FAIL\"\n",
    "                print(f\"  - {ep_name}/{scenario}: {flag}  -> {r['message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (aligned, episode/scenario) ===\n",
      "[ep_000/heavy] Delta=1.0  tasks=30636  agents=18\n",
      "[ep_000/light] Delta=1.0  tasks=2113  agents=18\n",
      "[ep_000/moderate] Delta=1.0  tasks=8262  agents=18\n",
      "\n",
      "=== TOPOLOGIES (checks vs each episode/scenario) ===\n",
      "Topology: clustered\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: full_mesh\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: sparse_ring\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n"
     ]
    }
   ],
   "source": [
    "# ==== Example driver (after your loading step) ====\n",
    "# datasets: episode-first dict\n",
    "# topologies: { \"full_mesh\": {...}, \"clustered\": {...}, \"sparse_ring\": {...} }\n",
    "\n",
    "result_align = align_all_units_episode_first(datasets_ep_first=datasets, topologies_by_name=topologies)\n",
    "print_alignment_summary_episode_first(result_align)\n",
    "\n",
    "# Access aligned data for a specific episode/scenario:\n",
    "# aligned_light_ep0 = result_align[\"datasets_aligned\"][\"ep_000\"][\"light\"]\n",
    "# agents_ep0_light  = aligned_light_ep0[\"agents\"]   # has f_local_slot\n",
    "# tasks_ep0_light   = aligned_light_ep0[\"tasks\"]    # has deadline_slots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.4. Build Scenario–Topology Pairs </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all datasets are paired with all topologies (Cartesian product). Each pair is checked for matching time parameters, then a basic bundle is created for further enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Scenario–Topology Pairs (episode-first aware)\n",
    "\n",
    "# We pair every (episode, scenario) dataset with every topology.\n",
    "# Output shape:\n",
    "# pairs_by_topology = {\n",
    "#   \"<topology_name>\": {\n",
    "#       \"<ep_XXX>\": {\n",
    "#           \"<scenario>\": {\n",
    "#               'scenario': <str>,\n",
    "#               'episode': <str>,\n",
    "#               'topology': <str>,\n",
    "#               'Delta': <float>,\n",
    "#               'K': <int>,\n",
    "#               'dataset': {episodes, agents, arrivals, tasks},\n",
    "#               'topology_data': <dict>,\n",
    "#               'topology_meta_data': <dict or None>,\n",
    "#               'connection_matrix_df': <pd.DataFrame>,  # shape (K, K+1)\n",
    "#               'checks': {'delta_match': bool, 'message': str}\n",
    "#           }, ...\n",
    "#       }, ...\n",
    "#   }, ...\n",
    "# }\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "def _delta_from_episodes(episodes_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Extract a single Delta value from episodes table.\"\"\"\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain 'Delta'.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _topology_time_step(topo_json: Dict[str, Any]) -> float:\n",
    "    \"\"\"Extract the topology time_step.\"\"\"\n",
    "    ts = topo_json.get(\"time_step\", None)\n",
    "    if ts is None:\n",
    "        raise ValueError(\"topology.json must contain 'time_step'.\")\n",
    "    return float(ts)\n",
    "\n",
    "def build_topology_episode_pairs(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    topologies: Dict[str, Dict[str, Any]],\n",
    "    strict_delta_match: bool = True\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Build pairs between every topology and every (episode, scenario) dataset.\n",
    "    If strict_delta_match is True, any mismatch between dataset Delta and topology time_step raises an error.\n",
    "    \"\"\"\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    # Iterate topologies first (topology-centric)\n",
    "    for topo_name, topo_bundle in topologies.items():\n",
    "        topo_data = topo_bundle.get(\"topology_data\", None)\n",
    "        meta_data = topo_bundle.get(\"meta_data\", None)\n",
    "        cm_df     = topo_bundle.get(\"connection_matrix\", None)\n",
    "\n",
    "        if not isinstance(topo_data, dict):\n",
    "            raise ValueError(f\"[{topo_name}] topology_data missing or not a dict.\")\n",
    "        if cm_df is None:\n",
    "            raise ValueError(f\"[{topo_name}] connection_matrix DataFrame is missing.\")\n",
    "\n",
    "        # Validate K and connection matrix shape\n",
    "        K = int(topo_data.get(\"number_of_servers\", -1))\n",
    "        if K <= 0:\n",
    "            raise ValueError(f\"[{topo_name}] invalid 'number_of_servers' in topology.json\")\n",
    "        if not (cm_df.shape[0] == K and cm_df.shape[1] == K + 1):\n",
    "            raise ValueError(\n",
    "                f\"[{topo_name}] connection_matrix shape must be (K, K+1); got {cm_df.shape}\"\n",
    "            )\n",
    "\n",
    "        topo_ts = _topology_time_step(topo_data)\n",
    "\n",
    "        # Prepare container for this topology\n",
    "        if topo_name not in pairs_by_topology:\n",
    "            pairs_by_topology[topo_name] = {}\n",
    "\n",
    "        # Compare with every (episode, scenario)\n",
    "        for ep_name, scenarios in datasets_ep_first.items():\n",
    "            if ep_name not in pairs_by_topology[topo_name]:\n",
    "                pairs_by_topology[topo_name][ep_name] = {}\n",
    "\n",
    "            for scen_name, ds in scenarios.items():\n",
    "                ds_Delta = _delta_from_episodes(ds[\"episodes\"])\n",
    "                delta_ok = bool(np.isclose(ds_Delta, topo_ts, atol=1e-12))\n",
    "                msg = \"OK\" if delta_ok else (\n",
    "                    f\"time_step mismatch (dataset Delta={ds_Delta}, topology time_step={topo_ts})\"\n",
    "                )\n",
    "                if (not delta_ok) and strict_delta_match:\n",
    "                    raise ValueError(f\"[{topo_name} × {ep_name}/{scen_name}] {msg}\")\n",
    "\n",
    "                # Store bundle\n",
    "                pairs_by_topology[topo_name][ep_name][scen_name] = {\n",
    "                    \"scenario\": scen_name,\n",
    "                    \"episode\": ep_name,\n",
    "                    \"topology\": topo_name,\n",
    "                    \"Delta\": ds_Delta,\n",
    "                    \"K\": K,\n",
    "                    \"dataset\": ds,\n",
    "                    \"topology_data\": topo_data,\n",
    "                    \"topology_meta_data\": meta_data,\n",
    "                    \"connection_matrix_df\": cm_df,\n",
    "                    \"checks\": {\"delta_match\": delta_ok, \"message\": msg}\n",
    "                }\n",
    "\n",
    "    return pairs_by_topology\n",
    "\n",
    "def print_pairs_summary_topology_first_ep(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> None:\n",
    "    \"\"\"Pretty-print summary as topology → episode → scenario.\"\"\"\n",
    "    print(\"=== TOPOLOGY × EPISODE × SCENARIO ===\")\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        print(f\"[TOPOLOGY] {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            print(f\"  ├─ Episode: {ep_name}\")\n",
    "            scen_map = by_ep[ep_name]\n",
    "            for scen_name in sorted(scen_map.keys()):\n",
    "                bundle = scen_map[scen_name]\n",
    "                flag  = \"OK\" if bundle[\"checks\"][\"delta_match\"] else \"FAIL\"\n",
    "                K     = bundle[\"K\"]\n",
    "                Delta = bundle[\"Delta\"]\n",
    "                msg   = bundle[\"checks\"][\"message\"]\n",
    "                print(f\"  │    - [{flag}] {scen_name:9s} | K={K:2d}  Δ={Delta:g}  -> {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPOLOGY × EPISODE × SCENARIO ===\n",
      "[TOPOLOGY] clustered\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n",
      "[TOPOLOGY] full_mesh\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n",
      "[TOPOLOGY] sparse_ring\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n"
     ]
    }
   ],
   "source": [
    "# --- Example driver (with your current variables) ---\n",
    "pairs_by_topology = build_topology_episode_pairs(\n",
    "    datasets_ep_first=datasets,   # episode-first dict you already built\n",
    "    topologies=topologies,\n",
    "    strict_delta_match=True\n",
    ")\n",
    "\n",
    "print_pairs_summary_topology_first_ep(pairs_by_topology)\n",
    "\n",
    "# Access examples:\n",
    "# pairs_by_topology[\"full_mesh\"][\"ep_000\"][\"light\"][\"dataset\"][\"tasks\"]\n",
    "# pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"connection_matrix_df\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.5. Agent→MEC mapping (for all pairs) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent → MEC Mapping assigns each agent to a specific MEC server.\n",
    "This creates a fixed mec_id for every agent (e.g., agent_id % K), which determines where its tasks are initially queued and processed in the MDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Agent→MEC mapping (for all pairs) — 3-level: topology → episode → scenario\n",
    "\n",
    "def assign_agents_to_mecs(pairs_by_topology):\n",
    "    \"\"\"\n",
    "    Adds agent→MEC mapping to each (topology / ep / scenario) bundle.\n",
    "    - Rule: mec_id = agent_id % K\n",
    "    - Writes:\n",
    "        bundle[\"agent_to_mec\"]                  (pd.Series, index=agent_id)\n",
    "        bundle[\"dataset\"][\"agents\"][\"mec_id\"]   (added column)\n",
    "    \"\"\"\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            for scen_name, bundle in by_scen.items():\n",
    "\n",
    "                ds = bundle[\"dataset\"]\n",
    "                agents = ds[\"agents\"].copy()\n",
    "                K = int(bundle[\"K\"])\n",
    "\n",
    "                if \"agent_id\" not in agents.columns:\n",
    "                    raise ValueError(f\"[{topo_name}/{ep_name}/{scen_name}] agents.csv missing 'agent_id'.\")\n",
    "\n",
    "                # Ensure agent_id is contiguous & sorted (0..N_agents-1)\n",
    "                agents = agents.sort_values(\"agent_id\").reset_index(drop=True)\n",
    "                expected_n = int(bundle[\"dataset\"][\"episodes\"][\"N_agents\"].iloc[0])\n",
    "                if agents[\"agent_id\"].min() != 0 or agents[\"agent_id\"].max() != expected_n - 1:\n",
    "                    raise ValueError(f\"[{topo_name}/{ep_name}/{scen_name}] agent_id range not 0..N_agents-1.\")\n",
    "\n",
    "                # Mapping\n",
    "                mec_ids = (agents[\"agent_id\"].astype(int) % K).astype(int)\n",
    "                agents[\"mec_id\"] = mec_ids\n",
    "\n",
    "                # Store: dataset copy + Series with index=agent_id\n",
    "                ds[\"agents\"] = agents\n",
    "                bundle[\"agent_to_mec\"] = pd.Series(\n",
    "                    data=mec_ids.values,\n",
    "                    index=agents[\"agent_id\"].values,\n",
    "                    name=\"mec_id\"\n",
    "                )\n",
    "\n",
    "    return pairs_by_topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lam_sec</th>\n",
       "      <th>mec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>0.228174</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>0.310369</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local      m_local   lam_sec  mec_id\n",
       "0         0  1.741183e+09  5713.849721  0.708673       0\n",
       "1         1  1.352326e+09  4566.428755  0.234989       1\n",
       "2         2  1.726668e+09  5815.120004  0.228174       2\n",
       "3         3  1.543616e+09  3539.850245  0.310369       3\n",
       "4         4  1.130883e+09  4161.367769  0.548990       4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply mapping\n",
    "pairs_by_topology = assign_agents_to_mecs(pairs_by_topology)\n",
    "\n",
    "# Quick sanity peek\n",
    "pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"dataset\"][\"agents\"].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.6. Environment Configuration </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we build a unified env_config for each scenario–topology pair.\n",
    "It bundles all required information for the MDP/RL environment—such as compute capacities, the Agent→MEC mapping, connection matrix, initial queue states, and action/state specifications—into a single consistent configuration used by the RL training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_core_from_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    required = [\"dataset\", \"topology_data\", \"connection_matrix_df\", \"Delta\", \"K\"]\n",
    "    for k in required:\n",
    "        if k not in bundle:\n",
    "            raise ValueError(f\"Bundle missing required key: '{k}'\")\n",
    "\n",
    "    ds   = bundle[\"dataset\"]\n",
    "    topo = bundle[\"topology_data\"]\n",
    "    Mdf  = bundle[\"connection_matrix_df\"]\n",
    "\n",
    "    private_cpu = np.asarray(topo[\"private_cpu_capacities\"], dtype=float)\n",
    "    public_cpu  = np.asarray(topo[\"public_cpu_capacities\"],  dtype=float)\n",
    "    cloud_cpu   = float(topo[\"cloud_computational_capacity\"])\n",
    "    M           = Mdf.to_numpy(dtype=float)  # (K, K+1), last col MEC→Cloud (MB/slot)\n",
    "\n",
    "    return dict(\n",
    "        Delta=float(bundle[\"Delta\"]),\n",
    "        K=int(bundle[\"K\"]),\n",
    "        episodes=ds[\"episodes\"],\n",
    "        agents=ds[\"agents\"],\n",
    "        arrivals=ds[\"arrivals\"],\n",
    "        tasks=ds[\"tasks\"],\n",
    "        private_cpu=private_cpu,\n",
    "        public_cpu=public_cpu,\n",
    "        cloud_cpu=cloud_cpu,\n",
    "        connection_matrix=M,\n",
    "        topology_type=topo.get(\"topology_type\", \"unknown\"),\n",
    "    )\n",
    "\n",
    "def _build_default_queues(K: int) -> Dict[str, np.ndarray]:\n",
    "    return {\n",
    "        \"mec_local_cycles\":   np.zeros(K, dtype=float),\n",
    "        \"mec_public_cycles\":  np.zeros(K, dtype=float),\n",
    "        \"mec_bytes_in_transit\": np.zeros(K, dtype=float),\n",
    "        \"cloud_cycles\":       np.array([0.0], dtype=float),\n",
    "    }\n",
    "\n",
    "def _derive_action_space() -> Dict[str, Any]:\n",
    "    return {\"type\": \"discrete\", \"n\": 3, \"labels\": {0: \"LOCAL\", 1: \"MEC\", 2: \"CLOUD\"}}\n",
    "\n",
    "def _derive_state_spec(K: int) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"components\": {\n",
    "            \"queues\": {\n",
    "                \"mec_local_cycles\":  {\"shape\": (K,),   \"dtype\": \"float\"},\n",
    "                \"mec_public_cycles\": {\"shape\": (K,),   \"dtype\": \"float\"},\n",
    "                \"cloud_cycles\":      {\"shape\": (1,),   \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"links\": {\n",
    "                \"connection_matrix\": {\"shape\": (K, K+1), \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"capacities\": {\n",
    "                \"private_cpu\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"public_cpu\":  {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"cloud_cpu\":   {\"shape\": (1,), \"dtype\": \"float\"},\n",
    "            }\n",
    "        },\n",
    "        \"note\": \"Declarative spec; tensor assembly happens in the Env at each step.\"\n",
    "    }\n",
    "\n",
    "def build_env_config_for_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    core = _extract_core_from_bundle(bundle)\n",
    "\n",
    "    if \"agent_to_mec\" not in bundle:\n",
    "        raise ValueError(\"Bundle has no 'agent_to_mec' mapping. Run Stage 5 first.\")\n",
    "\n",
    "    agent_to_mec = bundle[\"agent_to_mec\"]\n",
    "    if isinstance(agent_to_mec, pd.Series):\n",
    "        # reorder by agent_id if needed\n",
    "        if agent_to_mec.index.name != \"agent_id\":\n",
    "            agent_to_mec.index.name = \"agent_id\"\n",
    "        idx = core[\"agents\"].sort_values(\"agent_id\")[\"agent_id\"].to_numpy()\n",
    "        agent_to_mec = agent_to_mec.reindex(idx)\n",
    "        agent_to_mec_arr = agent_to_mec.to_numpy(dtype=int)\n",
    "    else:\n",
    "        agent_to_mec_arr = np.asarray(agent_to_mec, dtype=int)\n",
    "\n",
    "    N_agents = int(core[\"episodes\"][\"N_agents\"].iloc[0])\n",
    "    if len(agent_to_mec_arr) != N_agents:\n",
    "        raise ValueError(f\"agent_to_mec length ({len(agent_to_mec_arr)}) != N_agents ({N_agents}).\")\n",
    "\n",
    "    queues_init  = _build_default_queues(core[\"K\"])\n",
    "    action_space = _derive_action_space()\n",
    "    state_spec   = _derive_state_spec(core[\"K\"])\n",
    "\n",
    "    env_config = {\n",
    "        \"Delta\": core[\"Delta\"],\n",
    "        \"K\": core[\"K\"],\n",
    "        \"topology_type\": core[\"topology_type\"],\n",
    "        \"connection_matrix\": core[\"connection_matrix\"],\n",
    "\n",
    "        \"private_cpu\": core[\"private_cpu\"],\n",
    "        \"public_cpu\":  core[\"public_cpu\"],\n",
    "        \"cloud_cpu\":   core[\"cloud_cpu\"],\n",
    "\n",
    "        \"N_agents\": N_agents,\n",
    "        \"agent_to_mec\": agent_to_mec_arr,\n",
    "\n",
    "        # aligned dataframes\n",
    "        \"episodes\": core[\"episodes\"],\n",
    "        \"agents\":   core[\"agents\"],\n",
    "        \"arrivals\": core[\"arrivals\"],\n",
    "        \"tasks\":    core[\"tasks\"],\n",
    "\n",
    "        \"queues_initial\": queues_init,\n",
    "        \"action_space\": action_space,\n",
    "        \"state_spec\": state_spec,\n",
    "\n",
    "        \"checks\": bundle.get(\"checks\", {\"delta_match\": True, \"message\": \"n/a\"}),\n",
    "    }\n",
    "    return env_config\n",
    "\n",
    "\n",
    "def build_all_env_configs(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Build env_config for every (topology / episode / scenario) bundle.\n",
    "\n",
    "    Desired output shape (EPISODE-first):\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "\n",
    "    So you can access:\n",
    "        env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"agent_to_mec\"]\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]] = {}\n",
    "    # pairs_by_topology: topo -> ep -> scen -> bundle\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            # ensure episode level exists\n",
    "            if ep_name not in out:\n",
    "                out[ep_name] = {}\n",
    "            # ensure topology level under this episode exists\n",
    "            if topo_name not in out[ep_name]:\n",
    "                out[ep_name][topo_name] = {}\n",
    "            for scen_name, bundle in by_scen.items():\n",
    "                if \"agent_to_mec\" not in bundle:\n",
    "                    raise RuntimeError(\n",
    "                        f\"[{topo_name}/{ep_name}/{scen_name}] missing 'agent_to_mec'. Run Stage 5 first.\"\n",
    "                    )\n",
    "                env_cfg = build_env_config_for_bundle(bundle)\n",
    "                out[ep_name][topo_name][scen_name] = env_cfg\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build\n",
    "env_configs = build_all_env_configs(pairs_by_topology)\n",
    "\n",
    "# Example access:\n",
    "env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"agent_to_mec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.7. Sanity Checks </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we verify that each env_config is internally consistent (queue shapes, capacities, agent→MEC mapping, and connection matrix are valid and ready for simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]   ep_000/clustered/heavy\n",
      "[OK]   ep_000/clustered/light\n",
      "[OK]   ep_000/clustered/moderate\n",
      "[OK]   ep_000/full_mesh/heavy\n",
      "[OK]   ep_000/full_mesh/light\n",
      "[OK]   ep_000/full_mesh/moderate\n",
      "[OK]   ep_000/sparse_ring/heavy\n",
      "[OK]   ep_000/sparse_ring/light\n",
      "[OK]   ep_000/sparse_ring/moderate\n"
     ]
    }
   ],
   "source": [
    "def sanity_check_env_config(env_config):\n",
    "    errors = []\n",
    "\n",
    "    # 1) Agent → MEC alignment\n",
    "    N_agents = env_config[\"N_agents\"]\n",
    "    if len(env_config[\"agent_to_mec\"]) != N_agents:\n",
    "        errors.append(\"Length of agent_to_mec does not match N_agents.\")\n",
    "\n",
    "    # 2) Queue initial state shapes\n",
    "    K = env_config[\"K\"]\n",
    "    q = env_config[\"queues_initial\"]\n",
    "    if q[\"mec_local_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_local_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_public_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_public_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_bytes_in_transit\"].shape != (K,):\n",
    "        errors.append(\"mec_bytes_in_transit queue shape mismatch.\")\n",
    "    if q[\"cloud_cycles\"].shape != (1,):\n",
    "        errors.append(\"cloud_cycles shape mismatch (should be (1,)).\")\n",
    "\n",
    "    # 3) Non-negative compute capacities\n",
    "    if (env_config[\"private_cpu\"] < 0).any():\n",
    "        errors.append(\"private_cpu has negative values.\")\n",
    "    if (env_config[\"public_cpu\"] < 0).any():\n",
    "        errors.append(\"public_cpu has negative values.\")\n",
    "    if env_config[\"cloud_cpu\"] < 0:\n",
    "        errors.append(\"cloud_cpu is negative.\")\n",
    "\n",
    "    # 4) Connection matrix dimension (K x K+1)\n",
    "    M = env_config[\"connection_matrix\"]\n",
    "    if M.shape != (K, K+1):\n",
    "        errors.append(\"connection_matrix shape mismatch.\")\n",
    "\n",
    "    # 5) Action space correctness\n",
    "    if env_config[\"action_space\"][\"type\"] != \"discrete\":\n",
    "        errors.append(\"Action space must be discrete (LOCAL/MEC/CLOUD).\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "def sanity_check_all(env_configs):\n",
    "    for topo_name, by_ep in env_configs.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                errs = sanity_check_env_config(env_cfg)\n",
    "                if errs:\n",
    "                    print(f\"[FAIL] {topo_name}/{ep_name}/{scen_name}:\")\n",
    "                    for e in errs:\n",
    "                        print(\"   -\", e)\n",
    "                else:\n",
    "                    print(f\"[OK]   {topo_name}/{ep_name}/{scen_name}\")\n",
    "\n",
    "\n",
    "# Run all sanity checks\n",
    "sanity_check_all(env_configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Step 1, we have loaded the data, aligned the units, assigned agents to MECs, and prepared the environment configuration. Finally, we have performed consistency checks to ensure the data is correct. Next, we can move on to task labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 2: Task Labeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Basic Task Labeling (buckets, urgency, atomicity, ...) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers: quantile-based cut points ----------\n",
    "def _quantile_cutpoints(s: pd.Series, q_low=0.33, q_high=0.66) -> Tuple[float, float]:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(s.quantile(q_low)), float(s.quantile(q_high)))\n",
    "\n",
    "def _bucketize(value: float, q1: float, q2: float) -> str:\n",
    "    # Returns 'S', 'M', 'L' based on two cut points (q1<=q2)\n",
    "    if not np.isfinite(value) or not np.isfinite(q1) or not np.isfinite(q2):\n",
    "        return \"U\"  # Unknown\n",
    "    if value <= q1: return \"S\"\n",
    "    if value <= q2: return \"M\"\n",
    "    return \"L\"\n",
    "\n",
    "# ---------- threshold builder (adaptive to each tasks DF) ----------\n",
    "def build_task_label_thresholds(tasks_df: pd.DataFrame,\n",
    "                                q_low=0.33, q_high=0.66,\n",
    "                                urgent_slots_cap: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build adaptive thresholds from the data itself (per-episode/senario),\n",
    "    so 'light/moderate/heavy' are handled robustly.\n",
    "    \"\"\"\n",
    "    q_b_mb   = _quantile_cutpoints(tasks_df[\"b_mb\"], q_low, q_high) if \"b_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_rho    = _quantile_cutpoints(tasks_df[\"rho_cyc_per_mb\"], q_low, q_high) if \"rho_cyc_per_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_mem    = _quantile_cutpoints(tasks_df[\"mem_mb\"], q_low, q_high) if \"mem_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_split  = _quantile_cutpoints(tasks_df.loc[tasks_df.get(\"non_atomic\", 0)==1, \"split_ratio\"], q_low, q_high) \\\n",
    "               if \"split_ratio\" in tasks_df else (np.nan, np.nan)\n",
    "\n",
    "    return {\n",
    "        \"b_mb\":   {\"q1\": q_b_mb[0],  \"q2\": q_b_mb[1]},\n",
    "        \"rho\":    {\"q1\": q_rho[0],   \"q2\": q_rho[1]},\n",
    "        \"mem\":    {\"q1\": q_mem[0],   \"q2\": q_mem[1]},\n",
    "        \"split\":  {\"q1\": q_split[0], \"q2\": q_split[1]},\n",
    "        # If deadline_slots ≤ urgent_slots_cap → 'hard' (latency sensitive)\n",
    "        \"urgent_slots_cap\": int(urgent_slots_cap),\n",
    "    }\n",
    "\n",
    "# ---------- main labeling for a single tasks DF ----------\n",
    "def label_tasks_df(tasks_df: pd.DataFrame, Delta: float, thresholds: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add label columns to tasks_df (returns a COPY).\n",
    "    Columns added:\n",
    "      - size_bucket, compute_bucket, mem_bucket\n",
    "      - deadline_slots (if missing), urgency (none/soft/hard)\n",
    "      - atomicity, split_bucket\n",
    "      - latency_sensitive, compute_heavy, io_heavy, memory_heavy (bools)\n",
    "      - routing_hint (LOCAL/MEC/CLOUD)\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    # --- ensure numeric types\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\", \"deadline_s\", \"split_ratio\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # --- deadline_slots (if not precomputed in Units Alignment)\n",
    "    if \"deadline_slots\" not in df.columns:\n",
    "        if \"has_deadline\" in df.columns and \"deadline_s\" in df.columns:\n",
    "            df[\"deadline_slots\"] = np.where(\n",
    "                (df[\"has_deadline\"] == 1) & np.isfinite(df[\"deadline_s\"]),\n",
    "                np.ceil(df[\"deadline_s\"] / float(Delta)).astype(\"float\"),\n",
    "                np.nan\n",
    "            )\n",
    "        else:\n",
    "            df[\"deadline_slots\"] = np.nan\n",
    "\n",
    "    # --- bucketize size/compute/memory\n",
    "    b_q1, b_q2   = thresholds[\"b_mb\"][\"q1\"], thresholds[\"b_mb\"][\"q2\"]\n",
    "    rho_q1, rho_q2 = thresholds[\"rho\"][\"q1\"], thresholds[\"rho\"][\"q2\"]\n",
    "    mem_q1, mem_q2 = thresholds[\"mem\"][\"q1\"], thresholds[\"mem\"][\"q2\"]\n",
    "\n",
    "    df[\"size_bucket\"]    = df[\"b_mb\"].apply(lambda x: _bucketize(x, b_q1, b_q2)) if \"b_mb\" in df else \"U\"\n",
    "    df[\"compute_bucket\"] = df[\"rho_cyc_per_mb\"].apply(lambda x: _bucketize(x, rho_q1, rho_q2)) if \"rho_cyc_per_mb\" in df else \"U\"\n",
    "    df[\"mem_bucket\"]     = df[\"mem_mb\"].apply(lambda x: _bucketize(x, mem_q1, mem_q2)) if \"mem_mb\" in df else \"U\"\n",
    "\n",
    "    # --- atomicity & split buckets\n",
    "    if \"non_atomic\" in df.columns:\n",
    "        df[\"atomicity\"] = np.where(df[\"non_atomic\"] == 1, \"splittable\", \"atomic\")\n",
    "    else:\n",
    "        df[\"atomicity\"] = \"atomic\"\n",
    "\n",
    "    if \"split_ratio\" in df.columns:\n",
    "        sp_q1, sp_q2 = thresholds[\"split\"][\"q1\"], thresholds[\"split\"][\"q2\"]\n",
    "        df[\"split_bucket\"] = np.where(\n",
    "            df[\"atomicity\"] == \"splittable\",\n",
    "            df[\"split_ratio\"].apply(lambda v: _bucketize(v, sp_q1, sp_q2)),\n",
    "            \"NA\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"split_bucket\"] = \"NA\"\n",
    "\n",
    "    # --- urgency levels\n",
    "    urgent_cap = int(thresholds.get(\"urgent_slots_cap\", 2))\n",
    "    def _urg(row):\n",
    "        if int(row.get(\"has_deadline\", 0)) != 1 or not np.isfinite(row.get(\"deadline_slots\", np.nan)):\n",
    "            return \"none\"\n",
    "        slots = int(row[\"deadline_slots\"])\n",
    "        if slots <= urgent_cap:  # very tight deadline\n",
    "            return \"hard\"\n",
    "        return \"soft\"\n",
    "    df[\"urgency\"] = df.apply(_urg, axis=1)\n",
    "\n",
    "    # --- boolean convenience labels\n",
    "    df[\"latency_sensitive\"] = (df[\"urgency\"] == \"hard\")\n",
    "    df[\"compute_heavy\"]     = (df[\"compute_bucket\"] == \"L\")\n",
    "    df[\"io_heavy\"]          = (df[\"size_bucket\"] == \"L\")\n",
    "    df[\"memory_heavy\"]      = (df[\"mem_bucket\"] == \"L\")\n",
    "\n",
    "    # --- a very simple routing hint (only for debugging/EDA; not used by the RL policy)\n",
    "    def _hint(row):\n",
    "        if row[\"compute_heavy\"] or row[\"memory_heavy\"]:\n",
    "            return \"CLOUD\"\n",
    "        if row[\"latency_sensitive\"]:\n",
    "            return \"MEC\"\n",
    "        return \"LOCAL\"\n",
    "    df[\"routing_hint\"] = df.apply(_hint, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- batch apply to env_configs (topology → episode → scenario) ----------\n",
    "def label_all_tasks_in_env_configs(env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "                                   q_low=0.33, q_high=0.66, urgent_slots_cap=2,\n",
    "                                   verbose=True) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    For each env_config:\n",
    "      - build thresholds from its own tasks DF\n",
    "      - label tasks\n",
    "      - put labeled DF back into env_config[\"tasks\"]\n",
    "      - return a concise summary per bundle\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "\n",
    "    for topo_name, by_ep in env_configs.items():\n",
    "        summary[topo_name] = {}\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            summary[topo_name][ep_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                Delta = float(env_cfg[\"Delta\"])\n",
    "\n",
    "                # thresholds adaptive to this bundle\n",
    "                th = build_task_label_thresholds(tasks, q_low=q_low, q_high=q_high,\n",
    "                                                 urgent_slots_cap=urgent_slots_cap)\n",
    "                labeled = label_tasks_df(tasks, Delta=Delta, thresholds=th)\n",
    "                env_cfg[\"tasks\"] = labeled  # write back\n",
    "\n",
    "                # tiny summary\n",
    "                cnt = {\n",
    "                    \"n\": len(labeled),\n",
    "                    \"urg_hard\": int((labeled[\"urgency\"] == \"hard\").sum()),\n",
    "                    \"splittable\": int((labeled[\"atomicity\"] == \"splittable\").sum()),\n",
    "                    \"size_L\": int((labeled[\"size_bucket\"] == \"L\").sum()),\n",
    "                    \"compute_L\": int((labeled[\"compute_bucket\"] == \"L\").sum()),\n",
    "                    \"mem_L\": int((labeled[\"mem_bucket\"] == \"L\").sum()),\n",
    "                }\n",
    "                summary[topo_name][ep_name][scen_name] = cnt\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[label] {topo_name}/{ep_name}/{scen_name} -> \"\n",
    "                          f\"n={cnt['n']}, hard={cnt['urg_hard']}, split={cnt['splittable']}, \"\n",
    "                          f\"sizeL={cnt['size_L']}, compL={cnt['compute_L']}, memL={cnt['mem_L']}\")\n",
    "\n",
    "    return env_configs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[label] ep_000/clustered/heavy -> n=30636, hard=10673, split=13766, sizeL=10416, compL=10416, memL=10416\n",
      "[label] ep_000/clustered/light -> n=2113, hard=322, split=539, sizeL=719, compL=719, memL=719\n",
      "[label] ep_000/clustered/moderate -> n=8262, hard=2117, split=2890, sizeL=2809, compL=2809, memL=2809\n",
      "[label] ep_000/full_mesh/heavy -> n=30636, hard=10673, split=13766, sizeL=10416, compL=10416, memL=10416\n",
      "[label] ep_000/full_mesh/light -> n=2113, hard=322, split=539, sizeL=719, compL=719, memL=719\n",
      "[label] ep_000/full_mesh/moderate -> n=8262, hard=2117, split=2890, sizeL=2809, compL=2809, memL=2809\n",
      "[label] ep_000/sparse_ring/heavy -> n=30636, hard=10673, split=13766, sizeL=10416, compL=10416, memL=10416\n",
      "[label] ep_000/sparse_ring/light -> n=2113, hard=322, split=539, sizeL=719, compL=719, memL=719\n",
      "[label] ep_000/sparse_ring/moderate -> n=8262, hard=2117, split=2890, sizeL=2809, compL=2809, memL=2809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>...</th>\n",
       "      <th>compute_bucket</th>\n",
       "      <th>mem_bucket</th>\n",
       "      <th>atomicity</th>\n",
       "      <th>split_bucket</th>\n",
       "      <th>urgency</th>\n",
       "      <th>latency_sensitive</th>\n",
       "      <th>compute_heavy</th>\n",
       "      <th>io_heavy</th>\n",
       "      <th>memory_heavy</th>\n",
       "      <th>routing_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>L</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>CLOUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>...</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>splittable</td>\n",
       "      <td>S</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>CLOUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MEC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ... compute_bucket  \\\n",
       "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...              S   \n",
       "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...              M   \n",
       "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...              L   \n",
       "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...              M   \n",
       "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...              S   \n",
       "\n",
       "   mem_bucket   atomicity  split_bucket  urgency  latency_sensitive  \\\n",
       "0           M      atomic            NA     hard               True   \n",
       "1           L      atomic            NA     hard               True   \n",
       "2           M  splittable             S     hard               True   \n",
       "3           M      atomic            NA     hard               True   \n",
       "4           M      atomic            NA     hard               True   \n",
       "\n",
       "  compute_heavy  io_heavy memory_heavy routing_hint  \n",
       "0         False      True        False          MEC  \n",
       "1         False     False         True        CLOUD  \n",
       "2          True      True        False        CLOUD  \n",
       "3         False      True        False          MEC  \n",
       "4         False      True        False          MEC  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env_configs: Produced in Step 6 (structure: episode → topology → scenario)\n",
    "env_configs, label_summary = label_all_tasks_in_env_configs(\n",
    "    env_configs,\n",
    "    q_low=0.33, q_high=0.66, urgent_slots_cap=2,  # tunable thresholds\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example access:\n",
    "labeled_tasks = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"]\n",
    "labeled_tasks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Task Type Classification </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Chapter-4 Task Typing (priority rules) ===\n",
    "# Pre-req: tasks already labeled by your previous step: \n",
    "#   size_bucket, compute_bucket, mem_bucket, urgency, atomicity, split_bucket, routing_hint, etc.\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "def _derive_task_type_row(row: pd.Series) -> tuple[str, str, str, list]:\n",
    "    \"\"\"\n",
    "    Returns (task_type, task_subtype, type_reason, multi_flags)\n",
    "      task_type   ∈ {\"deadline_hard\",\"latency_sensitive\",\"compute_intensive\",\"data_intensive\",\"general\"}\n",
    "      task_subtype: finer note (e.g., \"deadline_hard\", \"deadline_soft\", ...)\n",
    "      type_reason: short human-readable reason\n",
    "      multi_flags: list of boolean tags that were true (for auditing)\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect boolean flags consistent with your earlier labeling:\n",
    "    urgency        = str(row.get(\"urgency\", \"none\"))         # \"hard\" | \"soft\" | \"none\"\n",
    "    latency_flag   = (urgency == \"hard\") or (urgency == \"soft\")\n",
    "    hard_deadline  = (urgency == \"hard\")\n",
    "\n",
    "    compute_heavy  = bool(row.get(\"compute_heavy\", False))   # compute_bucket == \"L\"\n",
    "    memory_heavy   = bool(row.get(\"memory_heavy\", False))    # mem_bucket == \"L\"\n",
    "    io_heavy       = bool(row.get(\"io_heavy\", False))        # size_bucket == \"L\"\n",
    "    non_atomic     = bool(row.get(\"atomicity\", \"atomic\") == \"splittable\")\n",
    "\n",
    "    # Keep all active signals for audit:\n",
    "    multi_flags = []\n",
    "    if hard_deadline:  multi_flags.append(\"deadline_hard\")\n",
    "    elif latency_flag: multi_flags.append(\"deadline_soft\")\n",
    "    if compute_heavy:  multi_flags.append(\"compute_heavy\")\n",
    "    if memory_heavy:   multi_flags.append(\"memory_heavy\")\n",
    "    if io_heavy:       multi_flags.append(\"io_heavy\")\n",
    "    if non_atomic:     multi_flags.append(\"splittable\")\n",
    "\n",
    "    # --- Priority resolution (Chapter 4) ---\n",
    "    # 1) Hard deadline dominates everything\n",
    "    if hard_deadline:\n",
    "        return (\"deadline_hard\", \"deadline_hard\", \"hard deadline (tight slots)\", multi_flags)\n",
    "\n",
    "    # 2) Latency-sensitive (soft deadlines / delay-sensitive)\n",
    "    if latency_flag:\n",
    "        return (\"latency_sensitive\", \"deadline_soft\", \"delay-sensitive (soft deadline)\", multi_flags)\n",
    "\n",
    "    # 3) Compute-intensive (c or rho or mem heavy)\n",
    "    #    You may decide whether memory_heavy alone pushes to compute_intensive or creates a separate class.\n",
    "    #    Based on Chapter 4 text we map memory_heavy into compute_intensive family.\n",
    "    if compute_heavy or memory_heavy:\n",
    "        return (\"compute_intensive\", \"compute_or_memory_heavy\", \"high compute/memory demand\", multi_flags)\n",
    "\n",
    "    # 4) Data-intensive (mainly large input size / high IO pressure)\n",
    "    if io_heavy:\n",
    "        return (\"data_intensive\", \"large_input_bandwidth\", \"large data volume / IO heavy\", multi_flags)\n",
    "\n",
    "    # 5) Otherwise general\n",
    "    return (\"general\", \"general\", \"no dominant constraint\", multi_flags)\n",
    "\n",
    "\n",
    "def apply_ch4_task_typing(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds Chapter-4 level task classes with priority rules into tasks_df (returns a COPY).\n",
    "    Columns added:\n",
    "      - task_type            (5-way class)\n",
    "      - task_subtype         (finer descriptor)\n",
    "      - type_reason          (short textual rationale)\n",
    "      - multi_flags          (list of all active boolean traits)\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    # Ensure the expected helper columns exist (created in your previous labeling step).\n",
    "    required_cols = [\"urgency\", \"compute_heavy\", \"memory_heavy\", \"io_heavy\", \"atomicity\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"apply_ch4_task_typing: missing label columns: {missing}\")\n",
    "\n",
    "    out_type, out_sub, out_reason, out_flags = [], [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        t, s, msg, flags = _derive_task_type_row(r)\n",
    "        out_type.append(t)\n",
    "        out_sub.append(s)\n",
    "        out_reason.append(msg)\n",
    "        out_flags.append(flags)\n",
    "\n",
    "    df[\"task_type\"]   = out_type\n",
    "    df[\"task_subtype\"]= out_sub\n",
    "    df[\"type_reason\"] = out_reason\n",
    "    df[\"multi_flags\"] = out_flags\n",
    "    # For convenience: one-hot view (optional)\n",
    "    df[\"is_general\"]            = (df[\"task_type\"] == \"general\")\n",
    "    df[\"is_deadline_hard\"]      = (df[\"task_type\"] == \"deadline_hard\")\n",
    "    df[\"is_latency_sensitive\"]  = (df[\"task_type\"] == \"latency_sensitive\")\n",
    "    df[\"is_compute_intensive\"]  = (df[\"task_type\"] == \"compute_intensive\")\n",
    "    df[\"is_data_intensive\"]     = (df[\"task_type\"] == \"data_intensive\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_task_typing_in_env_configs(env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "                                     verbose: bool = True) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    env_configs structure (as we fixed earlier):\n",
    "      env_configs[ep_name][topology_name][scenario_name][\"tasks\"] -> DataFrame\n",
    "\n",
    "    This function:\n",
    "      - applies Chapter-4 task typing to every tasks DF\n",
    "      - writes back the enriched DataFrame\n",
    "      - prints a short summary if verbose=True\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                enriched = apply_ch4_task_typing(tasks)\n",
    "                env_cfg[\"tasks\"] = enriched\n",
    "\n",
    "                if verbose:\n",
    "                    n = len(enriched)\n",
    "                    counts = enriched[\"task_type\"].value_counts().to_dict()\n",
    "                    print(f\"[typing] {ep_name}/{topo_name}/{scen_name}  n={n}  → {counts}\")\n",
    "    return env_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[typing] ep_000/clustered/heavy  n=30636  → {'compute_intensive': 11370, 'deadline_hard': 10673, 'general': 5671, 'data_intensive': 2922}\n",
      "[typing] ep_000/clustered/light  n=2113  → {'compute_intensive': 1009, 'general': 502, 'deadline_hard': 322, 'data_intensive': 280}\n",
      "[typing] ep_000/clustered/moderate  n=8262  → {'compute_intensive': 3395, 'deadline_hard': 2117, 'general': 1829, 'data_intensive': 921}\n",
      "[typing] ep_000/full_mesh/heavy  n=30636  → {'compute_intensive': 11370, 'deadline_hard': 10673, 'general': 5671, 'data_intensive': 2922}\n",
      "[typing] ep_000/full_mesh/light  n=2113  → {'compute_intensive': 1009, 'general': 502, 'deadline_hard': 322, 'data_intensive': 280}\n",
      "[typing] ep_000/full_mesh/moderate  n=8262  → {'compute_intensive': 3395, 'deadline_hard': 2117, 'general': 1829, 'data_intensive': 921}\n",
      "[typing] ep_000/sparse_ring/heavy  n=30636  → {'compute_intensive': 11370, 'deadline_hard': 10673, 'general': 5671, 'data_intensive': 2922}\n",
      "[typing] ep_000/sparse_ring/light  n=2113  → {'compute_intensive': 1009, 'general': 502, 'deadline_hard': 322, 'data_intensive': 280}\n",
      "[typing] ep_000/sparse_ring/moderate  n=8262  → {'compute_intensive': 3395, 'deadline_hard': 2117, 'general': 1829, 'data_intensive': 921}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task_id      task_type   task_subtype                  type_reason  \\\n",
       "0        0  deadline_hard  deadline_hard  hard deadline (tight slots)   \n",
       "1        1  deadline_hard  deadline_hard  hard deadline (tight slots)   \n",
       "2        2  deadline_hard  deadline_hard  hard deadline (tight slots)   \n",
       "3        3  deadline_hard  deadline_hard  hard deadline (tight slots)   \n",
       "4        4  deadline_hard  deadline_hard  hard deadline (tight slots)   \n",
       "\n",
       "                                         multi_flags  \n",
       "0                          [deadline_hard, io_heavy]  \n",
       "1                      [deadline_hard, memory_heavy]  \n",
       "2  [deadline_hard, compute_heavy, io_heavy, split...  \n",
       "3                          [deadline_hard, io_heavy]  \n",
       "4                          [deadline_hard, io_heavy]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Run typing on your current env_configs (episode → topology → scenario) ----\n",
    "env_configs = apply_task_typing_in_env_configs(env_configs, verbose=True)\n",
    "\n",
    "\n",
    "# Example access:\n",
    "env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"][[\"task_id\",\"task_type\",\"task_subtype\",\"type_reason\",\"multi_flags\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 3: Agent Profiling </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper 1: build per-agent per-slot arrival counts ----\n",
    "def _per_agent_slot_counts(arrivals_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count how many tasks each agent generates in each time slot.\n",
    "    This is used to estimate lambda (arrival rate) statistics.\n",
    "    \"\"\"\n",
    "    if not {\"agent_id\", \"t_slot\"}.issubset(arrivals_df.columns):\n",
    "        raise ValueError(\"arrivals must contain 'agent_id' and 't_slot'.\")\n",
    "    grp = arrivals_df.groupby([\"agent_id\", \"t_slot\"], as_index=False).size()\n",
    "    grp.rename(columns={\"size\": \"count\"}, inplace=True)\n",
    "    return grp\n",
    "\n",
    "# ---- Helper 2: estimate λ-mean and λ-variance per agent ----\n",
    "def _lambda_stats_from_counts(counts_df: pd.DataFrame, Delta: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert per-slot counts to rate statistics:\n",
    "        lambda_mean = mean(count_per_slot) / Delta\n",
    "        lambda_var  = var(count_per_slot)  / Delta^2\n",
    "    \"\"\"\n",
    "    if counts_df.empty:\n",
    "        return pd.DataFrame(columns=[\"agent_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"])\n",
    "\n",
    "    agg = counts_df.groupby(\"agent_id\")[\"count\"].agg(\n",
    "        lambda_mean_slot=\"mean\",\n",
    "        lambda_var_slot=\"var\",\n",
    "        slots_observed=\"count\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # If only one observation exists, variance becomes NaN → treat as zero.\n",
    "    agg[\"lambda_var_slot\"] = agg[\"lambda_var_slot\"].fillna(0.0)\n",
    "\n",
    "    # Convert to per-second rates\n",
    "    agg[\"lambda_mean\"] = agg[\"lambda_mean_slot\"] / float(Delta)\n",
    "    agg[\"lambda_var\"]  = agg[\"lambda_var_slot\"]  / float(Delta**2)\n",
    "    return agg[[\"agent_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"]]\n",
    "\n",
    "# ---- Helper 3: compute per-agent task-type distributions ----\n",
    "_TASK_TYPES = [\"general\", \"latency_sensitive\", \"deadline_hard\", \"data_intensive\", \"compute_intensive\"]\n",
    "\n",
    "def _task_distribution_per_agent(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute distribution of task types per agent.\n",
    "    Each distribution sums to 1 for agents with at least 1 task.\n",
    "    \"\"\"\n",
    "    if \"task_type\" not in tasks_df.columns or \"agent_id\" not in tasks_df.columns:\n",
    "        raise ValueError(\"tasks must contain 'agent_id' and 'task_type'.\")\n",
    "\n",
    "    cnt = tasks_df.groupby([\"agent_id\", \"task_type\"], as_index=False).size()\n",
    "    piv = cnt.pivot(index=\"agent_id\", columns=\"task_type\", values=\"size\").fillna(0.0)\n",
    "\n",
    "    # Ensure all expected labels exist\n",
    "    for t in _TASK_TYPES:\n",
    "        piv[t] = piv.get(t, 0.0)\n",
    "\n",
    "    piv[\"n_tasks_agent\"] = piv[_TASK_TYPES].sum(axis=1)\n",
    "\n",
    "    # Normalize to probability distribution\n",
    "    for t in _TASK_TYPES:\n",
    "        piv[f\"P_{t}\"] = np.where(piv[\"n_tasks_agent\"] > 0, piv[t] / piv[\"n_tasks_agent\"], 0.0)\n",
    "\n",
    "    keep = [\"n_tasks_agent\"] + [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    return piv[keep].reset_index()\n",
    "\n",
    "# ---- Helper 4: fraction of non-atomic (splittable) tasks ----\n",
    "def _non_atomic_share_per_agent(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the share of splittable (non-atomic) tasks per agent.\n",
    "    \"\"\"\n",
    "    if not {\"agent_id\", \"non_atomic\"}.issubset(tasks_df.columns):\n",
    "        # If missing, assume no splittable tasks\n",
    "        agents = tasks_df.get(\"agent_id\")\n",
    "        if agents is None or len(agents) == 0:\n",
    "            return pd.DataFrame(columns=[\"agent_id\", \"non_atomic_share\"])\n",
    "        return pd.DataFrame({\"agent_id\": agents.unique(), \"non_atomic_share\": 0.0})\n",
    "\n",
    "    grp = tasks_df.groupby(\"agent_id\")[\"non_atomic\"].agg(\n",
    "        non_atomic_share=lambda s: float((s == 1).mean())\n",
    "    ).reset_index()\n",
    "    return grp\n",
    "\n",
    "# ---- Build agent profiles for ONE env_config ----\n",
    "def build_agent_profiles_for_env(env_cfg: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct per-agent profiles combining:\n",
    "      - Local resource capacity (f_local, m_local)\n",
    "      - Arrival rate statistics (lambda_mean, lambda_var)\n",
    "      - Task type distribution (P_general, P_latency_sensitive, ...)\n",
    "      - Splittability share (non_atomic_share)\n",
    "      - MEC mapping if available\n",
    "    \"\"\"\n",
    "    episodes = env_cfg[\"episodes\"]\n",
    "    agents   = env_cfg[\"agents\"].copy()\n",
    "    arrivals = env_cfg[\"arrivals\"]\n",
    "    tasks    = env_cfg[\"tasks\"]\n",
    "    Delta    = float(env_cfg[\"Delta\"])\n",
    "\n",
    "    # Compute cycles per slot if missing (should exist from Step 1)\n",
    "    if \"f_local_slot\" not in agents.columns and \"f_local\" in agents.columns:\n",
    "        agents[\"f_local_slot\"] = agents[\"f_local\"].astype(float) * Delta\n",
    "\n",
    "    # 1) Arrival statistics\n",
    "    counts_df = _per_agent_slot_counts(arrivals)\n",
    "    lam_df    = _lambda_stats_from_counts(counts_df, Delta=Delta)\n",
    "\n",
    "    # 2) Task type distribution\n",
    "    dist_df   = _task_distribution_per_agent(tasks)\n",
    "\n",
    "    # 3) Splittable-task share\n",
    "    na_df     = _non_atomic_share_per_agent(tasks)\n",
    "\n",
    "    # 4) Agent→MEC mapping (optional)\n",
    "    mec_map = None\n",
    "    if \"agent_to_mec\" in env_cfg:\n",
    "        a2m = env_cfg[\"agent_to_mec\"]\n",
    "        if isinstance(a2m, pd.Series):\n",
    "            mec_map = a2m.rename(\"mec_id\").reset_index()\n",
    "        else:\n",
    "            mec_map = pd.DataFrame({\n",
    "                \"agent_id\": np.arange(len(a2m), dtype=int),\n",
    "                \"mec_id\": np.asarray(a2m, dtype=int)\n",
    "            })\n",
    "\n",
    "    # Merge all profile components\n",
    "    base = agents[[\"agent_id\", \"f_local\", \"f_local_slot\", \"m_local\"]].copy()\n",
    "    prof = base.merge(lam_df, on=\"agent_id\", how=\"left\") \\\n",
    "               .merge(dist_df, on=\"agent_id\", how=\"left\") \\\n",
    "               .merge(na_df, on=\"agent_id\", how=\"left\")\n",
    "\n",
    "    if mec_map is not None:\n",
    "        prof = prof.merge(mec_map, on=\"agent_id\", how=\"left\")\n",
    "\n",
    "    # Fill missing values for agents that had no tasks/arrivals\n",
    "    fill_zero = [\n",
    "        \"lambda_mean\", \"lambda_var\", \"slots_observed\",\n",
    "        \"n_tasks_agent\", \"non_atomic_share\"\n",
    "    ] + [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    prof[fill_zero] = prof[fill_zero].fillna(0.0)\n",
    "\n",
    "    # Add task-distribution sum (useful diagnostic)\n",
    "    prob_cols = [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    prof[\"TaskDist_sum\"] = prof[prob_cols].sum(axis=1)\n",
    "\n",
    "    return prof\n",
    "\n",
    "# ---- Batch profiling for ALL env_configs ----\n",
    "def build_all_agent_profiles(env_configs: Dict[str, Dict[str, Dict[str, Any]]]):\n",
    "    \"\"\"\n",
    "    Compute profiles for every (episode → topology → scenario) environment.\n",
    "    Stores result both in return dict AND env_configs[...] for convenience.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        out[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            out[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                prof = build_agent_profiles_for_env(env_cfg)\n",
    "                out[ep_name][topo_name][scen_name] = prof\n",
    "                env_cfg[\"agent_profiles\"] = prof  # attach back for direct access\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>f_local_slot</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_agent</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>1.400325</td>\n",
       "      <td>0.474341</td>\n",
       "      <td>1846</td>\n",
       "      <td>2585.0</td>\n",
       "      <td>0.182592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343907</td>\n",
       "      <td>0.093617</td>\n",
       "      <td>0.379884</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>1.125348</td>\n",
       "      <td>0.140472</td>\n",
       "      <td>718</td>\n",
       "      <td>808.0</td>\n",
       "      <td>0.169554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353960</td>\n",
       "      <td>0.092822</td>\n",
       "      <td>0.383663</td>\n",
       "      <td>0.433168</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>1.134063</td>\n",
       "      <td>0.138167</td>\n",
       "      <td>731</td>\n",
       "      <td>829.0</td>\n",
       "      <td>0.184560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334138</td>\n",
       "      <td>0.086852</td>\n",
       "      <td>0.394451</td>\n",
       "      <td>0.434258</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>1.139918</td>\n",
       "      <td>0.147241</td>\n",
       "      <td>972</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>0.182310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371841</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.359206</td>\n",
       "      <td>0.435921</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>1.277666</td>\n",
       "      <td>0.287951</td>\n",
       "      <td>1491</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>0.185302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344882</td>\n",
       "      <td>0.095538</td>\n",
       "      <td>0.374278</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local  f_local_slot      m_local  lambda_mean  lambda_var  \\\n",
       "0         0  1.741183e+09  1.741183e+09  5713.849721     1.400325    0.474341   \n",
       "1         1  1.352326e+09  1.352326e+09  4566.428755     1.125348    0.140472   \n",
       "2         2  1.726668e+09  1.726668e+09  5815.120004     1.134063    0.138167   \n",
       "3         3  1.543616e+09  1.543616e+09  3539.850245     1.139918    0.147241   \n",
       "4         4  1.130883e+09  1.130883e+09  4161.367769     1.277666    0.287951   \n",
       "\n",
       "   slots_observed  n_tasks_agent  P_general  P_latency_sensitive  \\\n",
       "0            1846         2585.0   0.182592                  0.0   \n",
       "1             718          808.0   0.169554                  0.0   \n",
       "2             731          829.0   0.184560                  0.0   \n",
       "3             972         1108.0   0.182310                  0.0   \n",
       "4            1491         1905.0   0.185302                  0.0   \n",
       "\n",
       "   P_deadline_hard  P_data_intensive  P_compute_intensive  non_atomic_share  \\\n",
       "0         0.343907          0.093617             0.379884          0.467311   \n",
       "1         0.353960          0.092822             0.383663          0.433168   \n",
       "2         0.334138          0.086852             0.394451          0.434258   \n",
       "3         0.371841          0.086643             0.359206          0.435921   \n",
       "4         0.344882          0.095538             0.374278          0.440945   \n",
       "\n",
       "   mec_id  TaskDist_sum  \n",
       "0       0           1.0  \n",
       "1       1           1.0  \n",
       "2       2           1.0  \n",
       "3       3           1.0  \n",
       "4       4           1.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build agent profiles for all (episode → topology → scenario) environments\n",
    "agent_profiles = build_all_agent_profiles(env_configs)\n",
    "\n",
    "# Example: Access the profile table for a specific episode / topology / scenario\n",
    "agent_profiles[\"ep_000\"][\"clustered\"][\"heavy\"].head()\n",
    "\n",
    "# Alternatively, the profile is also stored directly inside env_configs for convenience:\n",
    "env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"agent_profiles\"].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
