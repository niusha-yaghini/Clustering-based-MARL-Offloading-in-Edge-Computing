{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Prepare data and configure the environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Data Uploading (Data I/O) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Loading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "dataset_dir = '../Data_Generator/datasets'\n",
    "topology_dir = '../Topology_Generator/topologies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for datasets and topologies\n",
    "datasets = {}\n",
    "topologies = {}\n",
    "\n",
    "# Function to load datasets\n",
    "def load_datasets_from_directory(dataset_dir):\n",
    "    for dataset_name in os.listdir(dataset_dir):\n",
    "        dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "        if os.path.isdir(dataset_path):  # Only process directories\n",
    "            # Assuming the dataset files are named episodes.csv, agents.csv, etc.\n",
    "            episodes = os.path.join(dataset_path, \"episodes.csv\")\n",
    "            agents = os.path.join(dataset_path, \"agents.csv\")\n",
    "            arrivals = os.path.join(dataset_path, \"arrivals.csv\")\n",
    "            tasks = os.path.join(dataset_path, \"tasks.csv\")\n",
    "            \n",
    "            datasets[dataset_name] = {\n",
    "                \"episodes\": episodes,\n",
    "                \"agents\": agents,\n",
    "                \"arrivals\": arrivals,\n",
    "                \"tasks\": tasks\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded: {'heavy': {'episodes': '../Data_Generator/datasets\\\\heavy\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\heavy\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\heavy\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\heavy\\\\tasks.csv'}, 'light': {'episodes': '../Data_Generator/datasets\\\\light\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\light\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\light\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\light\\\\tasks.csv'}, 'moderate': {'episodes': '../Data_Generator/datasets\\\\moderate\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\moderate\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\moderate\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\moderate\\\\tasks.csv'}}\n",
      "../Data_Generator/datasets\\heavy\\agents.csv\n"
     ]
    }
   ],
   "source": [
    "# Call the functions to load data and topologies\n",
    "load_datasets_from_directory(dataset_dir)\n",
    "# load_topologies_from_directory(topology_dir)\n",
    "\n",
    "# Print the dictionaries to verify the output\n",
    "print(\"Datasets loaded:\", datasets)\n",
    "# print(\"Topologies loaded:\", topologies)\n",
    "\n",
    "print(datasets['heavy']['agents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded: {'heavy': {'episodes': '../Data_Generator/datasets\\\\heavy\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\heavy\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\heavy\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\heavy\\\\tasks.csv'}, 'light': {'episodes': '../Data_Generator/datasets\\\\light\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\light\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\light\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\light\\\\tasks.csv'}, 'moderate': {'episodes': '../Data_Generator/datasets\\\\moderate\\\\episodes.csv', 'agents': '../Data_Generator/datasets\\\\moderate\\\\agents.csv', 'arrivals': '../Data_Generator/datasets\\\\moderate\\\\arrivals.csv', 'tasks': '../Data_Generator/datasets\\\\moderate\\\\tasks.csv'}}\n",
      "Topologies loaded: {'clustered': {'topology_json': '../Topology_Generator/topologies\\\\clustered\\\\topology.json', 'meta_json': '../Topology_Generator/topologies\\\\clustered\\\\topology_meta.json'}, 'full_mesh': {'topology_json': '../Topology_Generator/topologies\\\\full_mesh\\\\topology.json', 'meta_json': '../Topology_Generator/topologies\\\\full_mesh\\\\topology_meta.json'}, 'sparse_ring': {'topology_json': '../Topology_Generator/topologies\\\\sparse_ring\\\\topology.json', 'meta_json': '../Topology_Generator/topologies\\\\sparse_ring\\\\topology_meta.json'}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for datasets and topologies\n",
    "datasets = {}\n",
    "topologies = {}\n",
    "\n",
    "# Function to load datasets\n",
    "def load_datasets_from_directory(dataset_dir):\n",
    "    for dataset_name in os.listdir(dataset_dir):\n",
    "        dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "        if os.path.isdir(dataset_path):  # Only process directories\n",
    "            # Assuming the dataset files are named episodes.csv, agents.csv, etc.\n",
    "            episodes = os.path.join(dataset_path, \"episodes.csv\")\n",
    "            agents = os.path.join(dataset_path, \"agents.csv\")\n",
    "            arrivals = os.path.join(dataset_path, \"arrivals.csv\")\n",
    "            tasks = os.path.join(dataset_path, \"tasks.csv\")\n",
    "            \n",
    "            datasets[dataset_name] = {\n",
    "                \"episodes\": episodes,\n",
    "                \"agents\": agents,\n",
    "                \"arrivals\": arrivals,\n",
    "                \"tasks\": tasks\n",
    "            }\n",
    "            \n",
    "            \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load datasets from a directory\n",
    "def load_datasets_from_directory(dataset_dir):\n",
    "    datasets = {}\n",
    "    \n",
    "    # Loop through each folder in the datasets directory\n",
    "    for dataset_name in os.listdir(dataset_dir):\n",
    "        dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "        \n",
    "        # Only process directories\n",
    "        if os.path.isdir(dataset_path):\n",
    "            episodes = os.path.join(dataset_path, \"episodes.csv\")\n",
    "            agents = os.path.join(dataset_path, \"agents.csv\")\n",
    "            arrivals = os.path.join(dataset_path, \"arrivals.csv\")\n",
    "            tasks = os.path.join(dataset_path, \"tasks.csv\")\n",
    "            \n",
    "            # Load the datasets using pandas (if they exist)\n",
    "            dataset = {}\n",
    "            # if os.path.exists(episodes):\n",
    "            dataset['episodes'] = pd.read_csv(episodes)\n",
    "            # else:\n",
    "            #     dataset['episodes'] = None\n",
    "            \n",
    "            # if os.path.exists(agents):\n",
    "            dataset['agents'] = pd.read_csv(agents)\n",
    "            # else:\n",
    "            #     dataset['agents'] = None\n",
    "            \n",
    "            # if os.path.exists(arrivals):\n",
    "            dataset['arrivals'] = pd.read_csv(arrivals)\n",
    "            # else:\n",
    "            #     dataset['arrivals'] = None\n",
    "            \n",
    "            # if os.path.exists(tasks):\n",
    "            dataset['tasks'] = pd.read_csv(tasks)\n",
    "            # else:\n",
    "            #     dataset['tasks'] = None\n",
    "            \n",
    "            # Store the datasets for the current folder\n",
    "            datasets[dataset_name] = dataset\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Usage example\n",
    "dataset_dir = 'path_to_your_datasets_directory'  # Replace with your directory path\n",
    "datasets = load_datasets_from_directory(dataset_dir)\n",
    "\n",
    "# Example of how to access the 'arrivals' DataFrame for a specific dataset\n",
    "dataset_name = 'light'  # Replace with the dataset name you want\n",
    "if dataset_name in datasets:\n",
    "    arrivals_df = datasets[dataset_name]['arrivals']\n",
    "    if arrivals_df is not None:\n",
    "        print(arrivals_df.head())  # Print the first few rows of the arrivals DataFrame\n",
    "    else:\n",
    "        print(f\"Arrivals data for {dataset_name} is missing.\")\n",
    "\n",
    "\n",
    "# # Function to load topologies\n",
    "# def load_topologies_from_directory(topology_dir):\n",
    "#     for topology_name in os.listdir(topology_dir):\n",
    "#         topology_path = os.path.join(topology_dir, topology_name)\n",
    "#         if os.path.isdir(topology_path):  # Only process directories\n",
    "#             # Assuming topology files are named topology.json and topology_meta.json\n",
    "#             topology_json = os.path.join(topology_path, \"topology.json\")\n",
    "#             meta_json = os.path.join(topology_path, \"topology_meta.json\")\n",
    "            \n",
    "#             topologies[topology_name] = {\n",
    "#                 \"topology_json\": topology_json,\n",
    "#                 \"meta_json\": meta_json\n",
    "#             }\n",
    "            \n",
    "# Function to load topologies including connection_matrix.csv\n",
    "def load_topologies_from_directory(topology_dir):\n",
    "    for topology_name in os.listdir(topology_dir):\n",
    "        topology_path = os.path.join(topology_dir, topology_name)\n",
    "        if os.path.isdir(topology_path):  # Only process directories\n",
    "            # Assuming topology files are named topology.json, topology_meta.json, and connection_matrix.csv\n",
    "            topology_json = os.path.join(topology_path, \"topology.json\")\n",
    "            meta_json = os.path.join(topology_path, \"topology_meta.json\")\n",
    "            connection_matrix_csv = os.path.join(topology_path, \"connection_matrix.csv\")\n",
    "            \n",
    "            topologies[topology_name] = {\n",
    "                \"topology_json\": topology_json,\n",
    "                \"meta_json\": meta_json,\n",
    "                \"connection_matrix_csv\": connection_matrix_csv\n",
    "            }            \n",
    "            \n",
    "\n",
    "# Call the functions to load data and topologies\n",
    "load_datasets_from_directory(dataset_dir)\n",
    "load_topologies_from_directory(topology_dir)\n",
    "\n",
    "# Print the dictionaries to verify the output\n",
    "print(\"Datasets loaded:\", datasets)\n",
    "print(\"Topologies loaded:\", topologies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads five input files and returns the data in the form of data frames (for CSV files) and dictionaries (for JSON files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "episodes, agents, arrivals, tasks, topology = load_data('episodes.csv', 'agents.csv', 'arrivals.csv', 'tasks.csv', 'topology.json')\n",
    "\n",
    "# Step 2: Validate Data\n",
    "validate_data(episodes, agents, arrivals, tasks, topology)\n",
    "\n",
    "# Step 3: Align Units\n",
    "Delta = 1.0  # Example value for Delta\n",
    "align_units(topology, agents, tasks, Delta)\n",
    "\n",
    "# Step 4: Map Agents to MEC\n",
    "K_MEC = len(topology[\"private_cpu_capacities\"])  # Number of MEC servers\n",
    "map_agents_to_mec(agents, K_MEC)\n",
    "\n",
    "# Step 5: Configure Environment\n",
    "env_config = configure_environment(topology, agents, tasks, Delta)\n",
    "\n",
    "# Step 6: Perform Sanity Checks\n",
    "sanity_checks(topology, agents, tasks, Delta)\n",
    "\n",
    "# Step 7: Smoke Test\n",
    "smoke_test(agents, topology)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
