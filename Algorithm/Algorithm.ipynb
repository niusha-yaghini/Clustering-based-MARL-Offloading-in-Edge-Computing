{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Prepare data and configure the environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Data Loading (Data I/O) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "dataset_dir = '../Data_Generator/datasets'\n",
    "topology_dir = '../Topology_Generator/topologies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for datasets and topologies\n",
    "datasets = {}\n",
    "\n",
    "# Function to load datasets from a directory\n",
    "def load_datasets_from_directory(dataset_dir):\n",
    "    \n",
    "    # Loop through each folder in the datasets directory\n",
    "    for dataset_name in os.listdir(dataset_dir):\n",
    "        dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "        \n",
    "        # Only process directories\n",
    "        if os.path.isdir(dataset_path):\n",
    "            episodes = os.path.join(dataset_path, \"episodes.csv\")\n",
    "            agents = os.path.join(dataset_path, \"agents.csv\")\n",
    "            arrivals = os.path.join(dataset_path, \"arrivals.csv\")\n",
    "            tasks = os.path.join(dataset_path, \"tasks.csv\")\n",
    "            \n",
    "            dataset = {}\n",
    "            dataset['episodes'] = pd.read_csv(episodes)\n",
    "            dataset['agents'] = pd.read_csv(agents)\n",
    "            dataset['arrivals'] = pd.read_csv(arrivals)\n",
    "            dataset['tasks'] = pd.read_csv(tasks)\n",
    "            datasets[dataset_name] = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset heavy -> agents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lam_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5.713850e+09</td>\n",
       "      <td>0.708673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4.566429e+09</td>\n",
       "      <td>0.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5.815120e+09</td>\n",
       "      <td>0.228174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3.539850e+09</td>\n",
       "      <td>0.310369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4.161368e+09</td>\n",
       "      <td>0.548990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local       m_local   lam_sec\n",
       "0         0  1.741183e+09  5.713850e+09  0.708673\n",
       "1         1  1.352326e+09  4.566429e+09  0.234989\n",
       "2         2  1.726668e+09  5.815120e+09  0.228174\n",
       "3         3  1.543616e+09  3.539850e+09  0.310369\n",
       "4         4  1.130883e+09  4.161368e+09  0.548990"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   agent_id  18 non-null     int64  \n",
      " 1   f_local   18 non-null     float64\n",
      " 2   m_local   18 non-null     float64\n",
      " 3   lam_sec   18 non-null     float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 704.0 bytes\n",
      "\n",
      "dataset heavy -> arrivals\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>t_slot</th>\n",
       "      <th>t_time</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>task_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  t_slot  t_time  agent_id  task_id\n",
       "0    heavy           0       0     0.0         0        0\n",
       "1    heavy           0       0     0.0         1        1\n",
       "2    heavy           0       0     0.0         4        2\n",
       "3    heavy           0       0     0.0         7        3\n",
       "4    heavy           0       0     0.0        10        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30636 entries, 0 to 30635\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    30636 non-null  object \n",
      " 1   episode_id  30636 non-null  int64  \n",
      " 2   t_slot      30636 non-null  int64  \n",
      " 3   t_time      30636 non-null  float64\n",
      " 4   agent_id    30636 non-null  int64  \n",
      " 5   task_id     30636 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 1.4+ MB\n",
      "\n",
      "dataset heavy -> episodes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>Delta</th>\n",
       "      <th>T_slots</th>\n",
       "      <th>hours</th>\n",
       "      <th>N_agents</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  Delta  T_slots  hours  N_agents  seed\n",
       "0    heavy           0    1.0     3600    1.0        18   345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    1 non-null      object \n",
      " 1   episode_id  1 non-null      int64  \n",
      " 2   Delta       1 non-null      float64\n",
      " 3   T_slots     1 non-null      int64  \n",
      " 4   hours       1 non-null      float64\n",
      " 5   N_agents    1 non-null      int64  \n",
      " 6   seed        1 non-null      int64  \n",
      "dtypes: float64(2), int64(4), object(1)\n",
      "memory usage: 184.0+ bytes\n",
      "\n",
      "dataset heavy -> tasks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000003</td>\n",
       "      <td>1.499999e+09</td>\n",
       "      <td>7.499998e+09</td>\n",
       "      <td>64.000006</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>1.500000e+09</td>\n",
       "      <td>7.499999e+09</td>\n",
       "      <td>64.000031</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000005</td>\n",
       "      <td>1.500002e+09</td>\n",
       "      <td>7.500016e+09</td>\n",
       "      <td>64.000021</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539704</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000002</td>\n",
       "      <td>1.500001e+09</td>\n",
       "      <td>7.500006e+09</td>\n",
       "      <td>63.999980</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000008</td>\n",
       "      <td>1.499999e+09</td>\n",
       "      <td>7.500006e+09</td>\n",
       "      <td>64.000022</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb modality  has_deadline  \\\n",
       "0  5.000003    1.499999e+09  7.499998e+09  64.000006   sensor             1   \n",
       "1  5.000001    1.500000e+09  7.499999e+09  64.000031    image             1   \n",
       "2  5.000005    1.500002e+09  7.500016e+09  64.000021     text             1   \n",
       "3  5.000002    1.500001e+09  7.500006e+09  63.999980   sensor             1   \n",
       "4  5.000008    1.499999e+09  7.500006e+09  64.000022   sensor             1   \n",
       "\n",
       "   deadline_s  deadline_time  non_atomic  split_ratio action_space_hint  \n",
       "0    0.800726       0.800726           0     0.000000          discrete  \n",
       "1    0.615113       0.615113           0     0.000000          discrete  \n",
       "2    0.323007       0.323007           1     0.539704        continuous  \n",
       "3    0.481587       0.481587           0     0.000000          discrete  \n",
       "4    0.594564       0.594564           0     0.000000          discrete  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30636 entries, 0 to 30635\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   scenario           30636 non-null  object \n",
      " 1   episode_id         30636 non-null  int64  \n",
      " 2   task_id            30636 non-null  int64  \n",
      " 3   agent_id           30636 non-null  int64  \n",
      " 4   t_arrival_slot     30636 non-null  int64  \n",
      " 5   t_arrival_time     30636 non-null  float64\n",
      " 6   b_mb               30636 non-null  float64\n",
      " 7   rho_cyc_per_mb     30636 non-null  float64\n",
      " 8   c_cycles           30636 non-null  float64\n",
      " 9   mem_mb             30636 non-null  float64\n",
      " 10  modality           30636 non-null  object \n",
      " 11  has_deadline       30636 non-null  int64  \n",
      " 12  deadline_s         10673 non-null  float64\n",
      " 13  deadline_time      10673 non-null  float64\n",
      " 14  non_atomic         30636 non-null  int64  \n",
      " 15  split_ratio        30636 non-null  float64\n",
      " 16  action_space_hint  30636 non-null  object \n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "load_datasets_from_directory(dataset_dir)\n",
    "\n",
    "print(\"dataset heavy -> agents\")\n",
    "display(datasets['heavy']['agents'].head())\n",
    "datasets['heavy']['agents'].info()\n",
    "\n",
    "print(\"\\ndataset heavy -> arrivals\")\n",
    "display(datasets['heavy']['arrivals'].head())\n",
    "datasets['heavy']['arrivals'].info()\n",
    "\n",
    "print(\"\\ndataset heavy -> episodes\")\n",
    "display(datasets['heavy']['episodes'].head())\n",
    "datasets['heavy']['episodes'].info()\n",
    "\n",
    "print(\"\\ndataset heavy -> tasks\")\n",
    "display(datasets['heavy']['tasks'].head())\n",
    "datasets['heavy']['tasks'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topologies = {}\n",
    "\n",
    "def load_topologies_from_directory(topology_dir):\n",
    "    \n",
    "    for topology_name in os.listdir(topology_dir):\n",
    "        topology_path = os.path.join(topology_dir, topology_name)\n",
    "        \n",
    "        # Only process directories\n",
    "        if os.path.isdir(topology_path):\n",
    "            topology_json_path = os.path.join(topology_path, \"topology.json\")\n",
    "            meta_json_path = os.path.join(topology_path, \"topology_meta.json\")\n",
    "            connection_matrix_csv_path = os.path.join(topology_path, \"connection_matrix.csv\")\n",
    "            \n",
    "             # --- Load JSON & CSV files ---\n",
    "            topology_data = None\n",
    "            meta_data = None\n",
    "            with open(topology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                topology_data = json.load(f)\n",
    "            with open(meta_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta_data = json.load(f)\n",
    "            \n",
    "            # The first column is just for displaying row names, not part of the capacity matrix. \n",
    "            # So the best way is to index the first column. (index_col=0)\n",
    "            connection_matrix = pd.read_csv(connection_matrix_csv_path, index_col=0)\n",
    "            \n",
    "            # Store the topology details and the loaded CSV\n",
    "            topologies[topology_name] = {\n",
    "                \"topology_data\": topology_data,\n",
    "                \"meta_data\": meta_data,\n",
    "                \"connection_matrix\": connection_matrix  # Store the loaded CSV data\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topology clustered -> connection_matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_0</th>\n",
       "      <th>mec_1</th>\n",
       "      <th>mec_2</th>\n",
       "      <th>mec_3</th>\n",
       "      <th>mec_4</th>\n",
       "      <th>mec_5</th>\n",
       "      <th>mec_6</th>\n",
       "      <th>mec_7</th>\n",
       "      <th>mec_8</th>\n",
       "      <th>mec_9</th>\n",
       "      <th>mec_10</th>\n",
       "      <th>mec_11</th>\n",
       "      <th>mec_12</th>\n",
       "      <th>mec_13</th>\n",
       "      <th>mec_14</th>\n",
       "      <th>mec_15</th>\n",
       "      <th>mec_16</th>\n",
       "      <th>mec_17</th>\n",
       "      <th>cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mec_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.798801</td>\n",
       "      <td>11.958295</td>\n",
       "      <td>11.470404</td>\n",
       "      <td>9.980195</td>\n",
       "      <td>8.814050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_1</th>\n",
       "      <td>9.798801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.136893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.609202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_2</th>\n",
       "      <td>11.958295</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>9.170700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.055348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_3</th>\n",
       "      <td>11.470404</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>11.920252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.851664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_4</th>\n",
       "      <td>9.980195</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.105383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.076301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mec_0      mec_1      mec_2      mec_3      mec_4      mec_5  \\\n",
       "mec_0   0.000000   9.798801  11.958295  11.470404   9.980195   8.814050   \n",
       "mec_1   9.798801   0.000000   8.375143  11.535600  10.702584   9.136893   \n",
       "mec_2  11.958295   8.375143   0.000000   8.814632   9.637714   9.170700   \n",
       "mec_3  11.470404  11.535600   8.814632   0.000000  10.377054  11.920252   \n",
       "mec_4   9.980195  10.702584   9.637714  10.377054   0.000000   8.105383   \n",
       "\n",
       "       mec_6  mec_7  mec_8  mec_9  mec_10  mec_11  mec_12  mec_13  mec_14  \\\n",
       "mec_0    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_1    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_2    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_3    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_4    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       mec_15  mec_16  mec_17       cloud  \n",
       "mec_0     0.0     0.0     0.0   98.912800  \n",
       "mec_1     0.0     0.0     0.0  104.609202  \n",
       "mec_2     0.0     0.0     0.0   92.055348  \n",
       "mec_3     0.0     0.0     0.0   84.851664  \n",
       "mec_4     0.0     0.0     0.0   99.076301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18 entries, mec_0 to mec_17\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   mec_0   18 non-null     float64\n",
      " 1   mec_1   18 non-null     float64\n",
      " 2   mec_2   18 non-null     float64\n",
      " 3   mec_3   18 non-null     float64\n",
      " 4   mec_4   18 non-null     float64\n",
      " 5   mec_5   18 non-null     float64\n",
      " 6   mec_6   18 non-null     float64\n",
      " 7   mec_7   18 non-null     float64\n",
      " 8   mec_8   18 non-null     float64\n",
      " 9   mec_9   18 non-null     float64\n",
      " 10  mec_10  18 non-null     float64\n",
      " 11  mec_11  18 non-null     float64\n",
      " 12  mec_12  18 non-null     float64\n",
      " 13  mec_13  18 non-null     float64\n",
      " 14  mec_14  18 non-null     float64\n",
      " 15  mec_15  18 non-null     float64\n",
      " 16  mec_16  18 non-null     float64\n",
      " 17  mec_17  18 non-null     float64\n",
      " 18  cloud   18 non-null     float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 2.8+ KB\n",
      "\n",
      "topology clustered -> topology_data\n",
      "{'number_of_servers': 18, 'private_cpu_capacities': [1417026512.123894, 1465835517.1380253, 1400296152.8672054, 1219863115.6758468, 1270376631.237273, 1348019534.1092064, 1786161289.9722981, 1380932007.7716885, 1739680378.9314957, 1606151704.3138125, 1722792016.067419, 1452119690.3718696, 1575323717.1732426, 1675176049.772993, 1465269239.5654364, 1565743740.4168038, 1715259832.5839007, 1225832428.456285], 'public_cpu_capacities': [768946447.3896191, 753418540.8443304, 702715947.6715652, 545197668.8935852, 670994513.1813464, 680525151.7601619, 811159071.5491732, 782943570.2288362, 501997671.50295806, 685198791.9005744, 822519853.1111488, 716138690.9607652, 822355072.296565, 556428866.3646028, 805978940.2755054, 790274917.5537901, 602437544.1146357, 709495765.381235], 'cloud_computational_capacity': 30000000000.0, 'connection_matrix': [[0.0, 9.798800609449902, 11.958295388571093, 11.470404384171585, 9.980195439399376, 8.8140500666217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.91279981562592], [9.798800609449902, 0.0, 8.375143239602235, 11.535600106180025, 10.702584376249934, 9.136893120492793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.60920207554163], [11.958295388571093, 8.375143239602235, 0.0, 8.814631907625701, 9.637714071509851, 9.170700424264968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 92.05534806322484], [11.470404384171585, 11.535600106180025, 8.814631907625701, 0.0, 10.37705391001423, 11.920252079923895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.85166371762908], [9.980195439399376, 10.702584376249934, 9.637714071509851, 10.37705391001423, 0.0, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0763009651688], [8.8140500666217, 9.136893120492793, 9.170700424264968, 11.920252079923895, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.0792791055983], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 9.566567838535661, 11.01158338193066, 10.132206014485726, 11.731761213672993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.87637059619391], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 0.0, 8.17265481777462, 11.317250164676846, 9.944330066939582, 11.698994881774604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.49730491373893], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.566567838535661, 8.17265481777462, 0.0, 11.919271797879857, 9.162931925532463, 11.22074688068535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.13111014716404], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.01158338193066, 11.317250164676846, 11.919271797879857, 0.0, 11.8680001020066, 11.393785145055949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 87.20532264682294], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.132206014485726, 9.944330066939582, 9.162931925532463, 11.8680001020066, 0.0, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.89380625765318], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.731761213672993, 11.698994881774604, 11.22074688068535, 11.393785145055949, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.16242880696842], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 10.155633210306574, 11.202595781660758, 10.69123291440641, 11.439129446848145, 94.68338717140213], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 0.0, 10.795539716597093, 9.382485758460357, 8.156515169162681, 10.108953651086749, 86.2148806997884], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.155633210306574, 10.795539716597093, 0.0, 8.183423253938845, 9.055460579237527, 8.857865582666564, 100.51802149006446], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.202595781660758, 9.382485758460357, 8.183423253938845, 0.0, 10.922031113226154, 8.777507498932518, 85.56329833615376], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.69123291440641, 8.156515169162681, 9.055460579237527, 10.922031113226154, 0.0, 11.48804541896262, 109.44902761831553], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.439129446848145, 10.108953651086749, 8.857865582666564, 8.777507498932518, 11.48804541896262, 0.0, 119.21419244342577]], 'time_step': 1.0, 'topology_type': 'clustered', 'skip_k': 5, 'symmetric': True, 'num_clusters': 3}\n",
      "\n",
      "topology clustered -> meta_data\n",
      "{'generated_at_utc': '2025-10-28T10:47:04Z', 'fingerprint': '9a3a6cc2768b6834', 'env': {'python': '3.10.9', 'user': 'niush'}, 'units': {'compute': 'CPU cycles per slot', 'links': 'MB per slot', 'time_step': 'seconds'}, 'notes': {'inputs_unit': {'compute': 'CPU cycles per second', 'links': 'MB per second'}, 'conversion': 'per_slot = per_second * time_step'}}\n"
     ]
    }
   ],
   "source": [
    "load_topologies_from_directory(topology_dir)\n",
    "\n",
    "print('topology clustered -> connection_matrix')\n",
    "display(topologies['clustered']['connection_matrix'].head())\n",
    "topologies['clustered']['connection_matrix'].info()\n",
    "\n",
    "print('\\ntopology clustered -> topology_data')\n",
    "print(topologies['clustered']['topology_data'])\n",
    "\n",
    "print('\\ntopology clustered -> meta_data')\n",
    "print(topologies['clustered']['meta_data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Data Validation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data, we must validate that required columns exist and that IDs match properly.\n",
    "\n",
    "**The code below performs three layers of checks:** \n",
    "\n",
    "- Validate each dataset (episodes/agents/arrivals/tasks)\n",
    "\n",
    "- Validate each topology (JSON and connection matrix)\n",
    "\n",
    "- Validate dataset–topology pairs for unit alignment and overall consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Generic helpers ----------\n",
    "def _require(cond: bool, msg: str, errors: list):\n",
    "    # Collect errors instead of stopping at first failure\n",
    "    if not cond:\n",
    "        errors.append(msg)\n",
    "\n",
    "def _has_cols(df: pd.DataFrame, cols: list) -> bool:\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# ---------- Dataset-level validation ----------\n",
    "def validate_one_dataset(dataset_name: str, ds: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single dataset scenario (light/moderate/heavy).\n",
    "    Expects keys: episodes, agents, arrivals, tasks  (each is a DataFrame)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds.get(\"episodes\")\n",
    "    agents   = ds.get(\"agents\")\n",
    "    arrivals = ds.get(\"arrivals\")\n",
    "    tasks    = ds.get(\"tasks\")\n",
    "\n",
    "    # 1) Presence checks\n",
    "    _require(isinstance(episodes, pd.DataFrame), f\"[{dataset_name}] episodes missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(agents,   pd.DataFrame), f\"[{dataset_name}] agents missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(arrivals, pd.DataFrame), f\"[{dataset_name}] arrivals missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(tasks,    pd.DataFrame), f\"[{dataset_name}] tasks missing or not a DataFrame\", errors)\n",
    "\n",
    "    if errors:\n",
    "        return errors  # can't continue without frames\n",
    "\n",
    "    # 2) Required columns based on your generator outputs\n",
    "    req_ep_cols  = [\"scenario\", \"episode_id\", \"Delta\", \"T_slots\", \"hours\", \"N_agents\", \"seed\"]\n",
    "    req_ag_cols  = [\"agent_id\", \"f_local\", \"m_local\", \"lam_sec\"]\n",
    "    req_ar_cols  = [\"scenario\", \"episode_id\", \"t_slot\", \"t_time\", \"agent_id\", \"task_id\"]\n",
    "    req_tk_cols  = [\n",
    "        \"scenario\",\"episode_id\",\"task_id\",\"agent_id\",\"t_arrival_slot\",\"t_arrival_time\",\n",
    "        \"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\",\"modality\",\n",
    "        \"has_deadline\",\"deadline_s\",\"deadline_time\",\"non_atomic\",\"split_ratio\",\"action_space_hint\"\n",
    "    ]\n",
    "\n",
    "    _require(_has_cols(episodes, req_ep_cols), f\"[{dataset_name}] episodes missing required columns\", errors)\n",
    "    _require(_has_cols(agents,   req_ag_cols), f\"[{dataset_name}] agents missing required columns\", errors)\n",
    "    _require(_has_cols(arrivals, req_ar_cols), f\"[{dataset_name}] arrivals missing required columns\", errors)\n",
    "    _require(_has_cols(tasks,    req_tk_cols), f\"[{dataset_name}] tasks missing required columns\", errors)\n",
    "\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 3) Basic integrity checks\n",
    "    # 3.1 Unique task_id\n",
    "    _require(tasks[\"task_id\"].is_unique, f\"[{dataset_name}] task_id is not unique\", errors)\n",
    "\n",
    "    # 3.2 Agent id range and count\n",
    "    if len(agents):\n",
    "        min_id = agents[\"agent_id\"].min()\n",
    "        max_id = agents[\"agent_id\"].max()\n",
    "        expected_n = episodes[\"N_agents\"].iloc[0]\n",
    "        _require(min_id == 0, f\"[{dataset_name}] agent_id should start at 0 (got {min_id})\", errors)\n",
    "        _require(max_id == expected_n - 1, f\"[{dataset_name}] agent_id max should be N_agents-1 ({expected_n-1}), got {max_id}\", errors)\n",
    "\n",
    "    # 3.3 Cross references (arrivals/tasks refer to valid agents)\n",
    "    valid_agents = set(agents[\"agent_id\"].tolist())\n",
    "    bad_arr_agents = set(arrivals[\"agent_id\"]) - valid_agents\n",
    "    bad_task_agents = set(tasks[\"agent_id\"]) - valid_agents\n",
    "    _require(len(bad_arr_agents) == 0, f\"[{dataset_name}] arrivals contain unknown agent_id(s): {sorted(bad_arr_agents)}\", errors)\n",
    "    _require(len(bad_task_agents) == 0, f\"[{dataset_name}] tasks contain unknown agent_id(s): {sorted(bad_task_agents)}\", errors)\n",
    "\n",
    "    # 3.4 Non-negative and sane numeric values\n",
    "    for col in [\"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            _require((tasks[col] >= 0).all(), f\"[{dataset_name}] tasks.{col} has negative values\", errors)\n",
    "\n",
    "    # 3.5 Deadline coherence\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        bad_deadline = tasks[(tasks[\"has_deadline\"] == 1) & ((tasks[\"deadline_s\"].isna()) | (tasks[\"deadline_s\"] <= 0))]\n",
    "        _require(len(bad_deadline) == 0, f\"[{dataset_name}] tasks with deadline have invalid deadline_s\", errors)\n",
    "\n",
    "    # 3.6 Episode-level parameters\n",
    "    _require(episodes[\"Delta\"].nunique() == 1, f\"[{dataset_name}] multiple Delta values in episodes\", errors)\n",
    "    _require(episodes[\"T_slots\"].nunique() == 1, f\"[{dataset_name}] multiple T_slots in episodes\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Topology-level validation ----------\n",
    "def validate_one_topology(topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single topology pack (topology.json + meta + connection_matrix.csv).\n",
    "    topo_entry expected keys:\n",
    "      - \"topology_data\": dict (loaded JSON)\n",
    "      - \"meta_data\": dict (loaded JSON)\n",
    "      - \"connection_matrix\": DataFrame (loaded)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    topo = topo_entry.get(\"topology_data\")\n",
    "    meta = topo_entry.get(\"meta_data\")\n",
    "    Mdf  = topo_entry.get(\"connection_matrix\")\n",
    "\n",
    "    # Presence checks\n",
    "    _require(isinstance(topo, dict), f\"[{topology_name}] topology_data missing or not a dict\", errors)\n",
    "    _require(isinstance(meta, dict), f\"[{topology_name}] meta_data missing or not a dict\", errors)\n",
    "    _require(isinstance(Mdf,  pd.DataFrame), f\"[{topology_name}] connection_matrix CSV missing or not a DataFrame\", errors)\n",
    "\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # Required keys in topology.json\n",
    "    req_keys = [\n",
    "        \"number_of_servers\",\"private_cpu_capacities\",\"public_cpu_capacities\",\n",
    "        \"cloud_computational_capacity\",\"connection_matrix\",\"time_step\"\n",
    "    ]\n",
    "    for k in req_keys:\n",
    "        _require(k in topo, f\"[{topology_name}] topology.json missing key: {k}\", errors)\n",
    "\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K = int(topo[\"number_of_servers\"])\n",
    "    # Length checks for capacities\n",
    "    _require(len(topo[\"private_cpu_capacities\"]) == K, f\"[{topology_name}] private_cpu_capacities length != K\", errors)\n",
    "    _require(len(topo[\"public_cpu_capacities\"])  == K, f\"[{topology_name}] public_cpu_capacities length != K\", errors)\n",
    "\n",
    "    # Connection matrix shape (from JSON and CSV)\n",
    "    Mjson = topo[\"connection_matrix\"]\n",
    "    _require(isinstance(Mjson, list) and len(Mjson) == K and len(Mjson[0]) == K+1,\n",
    "             f\"[{topology_name}] connection_matrix in JSON must be K x (K+1)\", errors)\n",
    "    _require(Mdf.shape == (K, K+1), f\"[{topology_name}] connection_matrix.csv shape must be K x (K+1)\", errors)\n",
    "\n",
    "    # Positive vertical capacities (MEC->Cloud)\n",
    "    vert_csv = Mdf.iloc[:, K]\n",
    "    _require((vert_csv > 0).all(), f\"[{topology_name}] MEC->Cloud capacities must be > 0\", errors)\n",
    "\n",
    "    # Non-negative horizontal capacities\n",
    "    horiz_csv = Mdf.iloc[:, :K]\n",
    "    _require((horiz_csv.values >= 0).all(), f\"[{topology_name}] MEC<->MEC capacities contain negatives\", errors)\n",
    "\n",
    "    # Per-slot note: time_step exists\n",
    "    _require(\"time_step\" in topo, f\"[{topology_name}] missing time_step\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Pairwise validation (dataset <-> topology) ----------\n",
    "def validate_dataset_topology_pair(dataset_name: str, ds: dict, topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate alignment between one dataset scenario and one topology.\n",
    "    Ensures Delta == time_step and basic feasibility checks.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds[\"episodes\"]\n",
    "    topo     = topo_entry[\"topology_data\"]\n",
    "    K        = int(topo[\"number_of_servers\"])\n",
    "\n",
    "    # Delta vs time_step\n",
    "    Delta = float(episodes[\"Delta\"].iloc[0])\n",
    "    time_step = float(topo[\"time_step\"])\n",
    "    _require(abs(Delta - time_step) < 1e-9,\n",
    "             f\"[{dataset_name} x {topology_name}] Delta ({Delta}) != time_step ({time_step})\", errors)\n",
    "\n",
    "    # Basic feasibility: at least one capacity positive\n",
    "    priv = topo[\"private_cpu_capacities\"]\n",
    "    pub  = topo[\"public_cpu_capacities\"]\n",
    "    cloud = topo[\"cloud_computational_capacity\"]\n",
    "    _require(all(x >= 0 for x in priv) and all(x >= 0 for x in pub) and cloud >= 0,\n",
    "             f\"[{dataset_name} x {topology_name}] negative compute capacities detected\", errors)\n",
    "\n",
    "    # Optional sanity: arrivals span within T_slots\n",
    "    T_slots = int(episodes[\"T_slots\"].iloc[0])\n",
    "    tasks = ds[\"tasks\"]\n",
    "    _require(tasks[\"t_arrival_slot\"].max() <= T_slots - 1,\n",
    "             f\"[{dataset_name} x {topology_name}] t_arrival_slot exceeds T_slots-1\", errors)\n",
    "\n",
    "    # Optional: map agents to MEC (simple modulo mapping) to check index bounds\n",
    "    N_agents = int(episodes[\"N_agents\"].iloc[0])\n",
    "    mapped = [(aid % K) for aid in range(N_agents)]\n",
    "    _require(all(0 <= m < K for m in mapped),\n",
    "             f\"[{dataset_name} x {topology_name}] agent->MEC mapping out of bounds\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Orchestrator over ALL datasets and ALL topologies ----------\n",
    "def validate_everything(datasets: dict, topologies: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Iterate through all dataset scenarios and all topologies, run validations,\n",
    "    and return a structured report.\n",
    "    'datasets' example shape:\n",
    "        {\n",
    "          \"light\": {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "          \"moderate\": {...},\n",
    "          \"heavy\": {...}\n",
    "        }\n",
    "    'topologies' example shape from your loader:\n",
    "        {\n",
    "          \"full_mesh\": {\"topology_data\": dict, \"meta_data\": dict, \"connection_matrix\": df, \"paths\": {...}},\n",
    "          \"clustered\": {...},\n",
    "          \"sparse_ring\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    report = {\"datasets\": {}, \"topologies\": {}, \"pairs\": {}}\n",
    "\n",
    "    # Validate each dataset\n",
    "    for dname, dpack in datasets.items():\n",
    "        errs = validate_one_dataset(dname, dpack)\n",
    "        report[\"datasets\"][dname] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # Validate each topology\n",
    "    for tname, tpack in topologies.items():\n",
    "        errs = validate_one_topology(tname, tpack)\n",
    "        report[\"topologies\"][tname] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # Pairwise validation only if both sides are OK\n",
    "    for dname, dres in report[\"datasets\"].items():\n",
    "        for tname, tres in report[\"topologies\"].items():\n",
    "            key = f\"{dname}__{tname}\"\n",
    "            if dres[\"ok\"] and tres[\"ok\"]:\n",
    "                errs = validate_dataset_topology_pair(dname, datasets[dname], tname, topologies[tname])\n",
    "                report[\"pairs\"][key] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "            else:\n",
    "                # Skip pair if either side invalid\n",
    "                report[\"pairs\"][key] = {\"ok\": False, \"errors\": [\"Skipped due to upstream invalid dataset/topology.\"]}\n",
    "\n",
    "    return report\n",
    "\n",
    "# ---------- Pretty printer ----------\n",
    "def print_validation_report(report: dict):\n",
    "    # Datasets\n",
    "    print(\"=== DATASETS ===\")\n",
    "    for name, info in report[\"datasets\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    # Topologies\n",
    "    print(\"\\n=== TOPOLOGIES ===\")\n",
    "    for name, info in report[\"topologies\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    # Pairs\n",
    "    print(\"\\n=== DATASET × TOPOLOGY PAIRS ===\")\n",
    "    for key, info in report[\"pairs\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {key}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS ===\n",
      "[OK] heavy\n",
      "[OK] light\n",
      "[OK] moderate\n",
      "\n",
      "=== TOPOLOGIES ===\n",
      "[OK] clustered\n",
      "[OK] full_mesh\n",
      "[OK] sparse_ring\n",
      "\n",
      "=== DATASET × TOPOLOGY PAIRS ===\n",
      "[OK] heavy__clustered\n",
      "[OK] heavy__full_mesh\n",
      "[OK] heavy__sparse_ring\n",
      "[OK] light__clustered\n",
      "[OK] light__full_mesh\n",
      "[OK] light__sparse_ring\n",
      "[OK] moderate__clustered\n",
      "[OK] moderate__full_mesh\n",
      "[OK] moderate__sparse_ring\n"
     ]
    }
   ],
   "source": [
    "report = validate_everything(datasets, topologies)\n",
    "print_validation_report(report)\n",
    "\n",
    "# If you need programmatic checks:\n",
    "all_ok = all(v[\"ok\"] for v in report[\"datasets\"].values()) \\\n",
    "         and all(v[\"ok\"] for v in report[\"topologies\"].values()) \\\n",
    "         and all(v[\"ok\"] for v in report[\"pairs\"].values())\n",
    "if not all_ok:\n",
    "    raise RuntimeError(\"Validation failed. See printed report for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Units Alignment </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we align units for all dataset scenarios (light / moderate / heavy) and all topologies (full_mesh / clustered / sparse_ring), and run several consistency checks. \n",
    "\n",
    "\n",
    "- In datasets: we take Delta from episodes.csv, create deadline_slots for tasks, and add helper per-slot columns for agents (f_local_slot)\n",
    "\n",
    "- In topologies: capacities are already per-slot (you multiplied by Δ in the generator), so we only verify that time_step == Delta and capacities are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports =====\n",
    "from typing import Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# ===== Helpers: safe getters =====\n",
    "def _get_delta(episodes_df: pd.DataFrame) -> float:\n",
    "    # Expect a single Delta value in episodes; take the first row\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain a 'Delta' column.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _ensure_numeric_positive(name: str, arr: np.ndarray):\n",
    "    # Basic sanity: no negative and finite values for capacities/links\n",
    "    if not np.isfinite(arr).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "    if (arr < 0).any():\n",
    "        raise ValueError(f\"{name} contains negative values.\")\n",
    "\n",
    "# ===== Alignment: per-dataset =====\n",
    "def align_units_for_dataset(dataset: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given one dataset dict {\"episodes\",\"agents\",\"arrivals\",\"tasks\"},\n",
    "    return a copy with aligned/derived columns (per-slot helpers).\n",
    "    \"\"\"\n",
    "    episodes = dataset[\"episodes\"].copy()\n",
    "    agents   = dataset[\"agents\"].copy()\n",
    "    arrivals = dataset[\"arrivals\"].copy()\n",
    "    tasks    = dataset[\"tasks\"].copy()\n",
    "\n",
    "    Delta = _get_delta(episodes)\n",
    "\n",
    "    # --- Agents: add per-slot compute capacity helper (cycles/slot) ---\n",
    "    # f_local is Hz (cycles per second) in generator; we add f_local_slot = f_local * Delta\n",
    "    if \"f_local\" in agents.columns:\n",
    "        agents[\"f_local_slot\"] = agents[\"f_local\"].astype(float) * Delta\n",
    "    else:\n",
    "        raise ValueError(\"agents.csv must contain 'f_local'.\")\n",
    "\n",
    "    # Memory is MB (not per second), so we keep m_local as-is. Add safe dtype.\n",
    "    if \"m_local\" in agents.columns:\n",
    "        agents[\"m_local\"] = agents[\"m_local\"].astype(float)\n",
    "\n",
    "    # --- Tasks: add deadline_slots; keep arrival slot as integer ---\n",
    "    if \"t_arrival_slot\" not in tasks.columns:\n",
    "        raise ValueError(\"tasks.csv must contain 't_arrival_slot'.\")\n",
    "    tasks[\"t_arrival_slot\"] = tasks[\"t_arrival_slot\"].astype(int)\n",
    "\n",
    "    # Build deadline_slots = ceil(deadline_s / Delta) when has_deadline == 1, else NaN\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        def _to_deadline_slots(row):\n",
    "            if int(row[\"has_deadline\"]) == 1 and np.isfinite(row[\"deadline_s\"]):\n",
    "                return int(math.ceil(float(row[\"deadline_s\"]) / Delta))\n",
    "            return np.nan\n",
    "        tasks[\"deadline_slots\"] = tasks.apply(_to_deadline_slots, axis=1)\n",
    "\n",
    "    # Ensure key numeric task fields are floats\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            tasks[col] = tasks[col].astype(float)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes,\n",
    "        \"agents\":   agents,\n",
    "        \"arrivals\": arrivals,\n",
    "        \"tasks\":    tasks,\n",
    "    }\n",
    "\n",
    "# ===== Verification: per-topology against a target Delta =====\n",
    "def verify_topology_units(topology: Dict[str, Any], target_Delta: float) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ensure topology capacities are per-slot and consistent with dataset Delta:\n",
    "    - time_step == target_Delta\n",
    "    - shapes are valid (K x (K+1))\n",
    "    - capacities non-negative\n",
    "    Returns (ok, message).\n",
    "    \"\"\"\n",
    "    # time_step check\n",
    "    ts = float(topology.get(\"time_step\", -1.0))\n",
    "    if not np.isclose(ts, target_Delta, atol=1e-9):\n",
    "        return (False, f\"time_step mismatch (topology={ts}, dataset Delta={target_Delta})\")\n",
    "\n",
    "    # K and lists\n",
    "    K = int(topology.get(\"number_of_servers\", -1))\n",
    "    priv = np.array(topology.get(\"private_cpu_capacities\", []), dtype=float)\n",
    "    pub  = np.array(topology.get(\"public_cpu_capacities\", []), dtype=float)\n",
    "    cloud = float(topology.get(\"cloud_computational_capacity\", -1.0))\n",
    "    M = np.array(topology.get(\"connection_matrix\", []), dtype=float)\n",
    "\n",
    "    if K <= 0:\n",
    "        return (False, \"Invalid 'number_of_servers' (K<=0).\")\n",
    "    if priv.shape[0] != K or pub.shape[0] != K:\n",
    "        return (False, \"private/public capacities must have length K.\")\n",
    "    if M.shape != (K, K+1):\n",
    "        return (False, f\"connection_matrix shape must be (K, K+1), got {M.shape}.\")\n",
    "\n",
    "    # Non-negative checks\n",
    "    _ensure_numeric_positive(\"private_cpu_capacities\", priv)\n",
    "    _ensure_numeric_positive(\"public_cpu_capacities\",  pub)\n",
    "    if not np.isfinite(cloud) or cloud < 0:\n",
    "        return (False, \"cloud_computational_capacity must be non-negative and finite.\")\n",
    "    _ensure_numeric_positive(\"connection_matrix\", M)\n",
    "\n",
    "    return (True, \"topology verified (per-slot, consistent).\")\n",
    "\n",
    "# ===== Batch alignment for ALL datasets & ALL topologies =====\n",
    "def align_all_units(datasets_by_name: Dict[str, Dict[str, pd.DataFrame]],\n",
    "                    topologies_by_name: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - Align each dataset variant independently.\n",
    "    - Verify each topology variant against each dataset's Delta (they should match).\n",
    "    - Return a structured dict containing aligned datasets and topology checks.\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"datasets_aligned\": {},   # scenario_name -> aligned dataset dict\n",
    "        \"topology_checks\":  {},   # topology_name -> per-dataset Delta check results\n",
    "    }\n",
    "\n",
    "    # Align datasets\n",
    "    for scen, ds in datasets_by_name.items():\n",
    "        try:\n",
    "            out[\"datasets\"][scen] = align_units_for_dataset(ds)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"[{scen}] dataset alignment failed: {e}\") from e\n",
    "\n",
    "    # Verify topologies against each dataset's Delta\n",
    "    for topo_name, topo_bundle in topologies_by_name.items():\n",
    "        # Expect topology_data already loaded into dict\n",
    "        topo_obj = topo_bundle.get(\"topology_data\", None)\n",
    "        if not isinstance(topo_obj, dict):\n",
    "            raise RuntimeError(f\"[{topo_name}] 'topology_data' missing or not a dict.\")\n",
    "\n",
    "        per_dataset_results = {}\n",
    "        for scen, aligned in out[\"datasets\"].items():\n",
    "            Delta = _get_delta(aligned[\"episodes\"])\n",
    "            ok, msg = verify_topology_units(topo_obj, Delta)\n",
    "            per_dataset_results[scen] = {\"ok\": bool(ok), \"message\": msg}\n",
    "        out[\"topology_checks\"][topo_name] = per_dataset_results\n",
    "\n",
    "    return out\n",
    "\n",
    "# ===== Pretty printer (optional) =====\n",
    "def print_alignment_summary(result: Dict[str, Any]):\n",
    "    # Datasets\n",
    "    print(\"=== DATASETS (aligned) ===\")\n",
    "    for scen, ds in result[\"datasets\"].items():\n",
    "        Delta = _get_delta(ds[\"episodes\"])\n",
    "        n_tasks = len(ds[\"tasks\"])\n",
    "        n_agents = len(ds[\"agents\"])\n",
    "        print(f\"[{scen}] Delta={Delta}  tasks={n_tasks}  agents={n_agents}\")\n",
    "\n",
    "    # Topologies\n",
    "    print(\"\\n=== TOPOLOGIES (checks) ===\")\n",
    "    for topo_name, checks in result[\"topology_checks\"].items():\n",
    "        print(f\"Topology: {topo_name}\")\n",
    "        for scen, r in checks.items():\n",
    "            flag = \"OK\" if r[\"ok\"] else \"FAIL\"\n",
    "            print(f\"  - vs {scen}: {flag}  -> {r['message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (aligned) ===\n",
      "[heavy] Delta=1.0  tasks=30636  agents=18\n",
      "[light] Delta=1.0  tasks=2113  agents=18\n",
      "[moderate] Delta=1.0  tasks=8262  agents=18\n",
      "\n",
      "=== TOPOLOGIES (checks) ===\n",
      "Topology: clustered\n",
      "  - vs heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs light: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: full_mesh\n",
      "  - vs heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs light: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: sparse_ring\n",
      "  - vs heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs light: OK  -> topology verified (per-slot, consistent).\n",
      "  - vs moderate: OK  -> topology verified (per-slot, consistent).\n"
     ]
    }
   ],
   "source": [
    "# Example driver (after your loading step)\n",
    "# datasets: { \"light\": {...}, \"moderate\": {...}, \"heavy\": {...} }\n",
    "# topologies: { \"full_mesh\": {...}, \"clustered\": {...}, \"sparse_ring\": {...} }\n",
    "\n",
    "result_align = align_all_units(datasets_by_name=datasets, topologies_by_name=topologies)\n",
    "print_alignment_summary(result_align)\n",
    "\n",
    "# Access aligned data for a specific scenario:\n",
    "aligned_light = result_align[\"datasets_aligned\"][\"light\"]\n",
    "episodes_light = aligned_light[\"episodes\"]\n",
    "agents_light   = aligned_light[\"agents\"]   # has f_local_slot\n",
    "tasks_light    = aligned_light[\"tasks\"]    # has deadline_slots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. Build Scenario–Topology Pairs </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all datasets are paired with all topologies (Cartesian product). Each pair is checked for matching time parameters, then a basic bundle is created for further enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def _delta_from_episodes(episodes_df) -> float:\n",
    "    # Get single Delta value from episodes table\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain 'Delta'.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _topology_time_step(topo_json: Dict[str, Any]) -> float:\n",
    "    # Extract topology time_step\n",
    "    ts = topo_json.get(\"time_step\", None)\n",
    "    if ts is None:\n",
    "        raise ValueError(\"topology.json must contain 'time_step'.\")\n",
    "    return float(ts)\n",
    "\n",
    "def build_topology_scenario_pairs(\n",
    "    datasets: Dict[str, Dict[str, Any]],\n",
    "    topologies: Dict[str, Dict[str, Any]],\n",
    "    strict_delta_match: bool = True\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build a nested dict organized as:\n",
    "      pairs_by_topology[topology_name][scenario_name] = {\n",
    "        'scenario': <scenario_name>,\n",
    "        'topology': <topology_name>,\n",
    "        'Delta': <float>,\n",
    "        'K': <int>,\n",
    "        'dataset': {episodes, agents, arrivals, tasks},\n",
    "        'topology_data': <dict>,\n",
    "        'topology_meta_data': <dict or None>,\n",
    "        'connection_matrix_df': <pd.DataFrame>,  # shape (K, K+1)\n",
    "        'checks': {'delta_match': bool, 'message': str}\n",
    "      }\n",
    "\n",
    "    If strict_delta_match=True, a mismatch between dataset Delta and topology time_step raises an error.\n",
    "    \"\"\"\n",
    "    pairs_by_topology: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    # Iterate over all topologies first (topology-centric)\n",
    "    for topo_name, topo_bundle in topologies.items():\n",
    "        topo_data = topo_bundle.get(\"topology_data\", None)\n",
    "        meta_data = topo_bundle.get(\"meta_data\", None)\n",
    "        cm_df     = topo_bundle.get(\"connection_matrix\", None)\n",
    "\n",
    "        if not isinstance(topo_data, dict):\n",
    "            raise ValueError(f\"[{topo_name}] topology_data missing or not a dict.\")\n",
    "        if cm_df is None:\n",
    "            raise ValueError(f\"[{topo_name}] connection_matrix DataFrame is missing.\")\n",
    "\n",
    "        # Extract K for shape checks\n",
    "        K = int(topo_data.get(\"number_of_servers\", -1))\n",
    "        if K <= 0:\n",
    "            raise ValueError(f\"[{topo_name}] invalid 'number_of_servers' in topology.json\")\n",
    "\n",
    "        # Validate connection_matrix dimension K x (K+1)\n",
    "        if not (cm_df.shape[0] == K and cm_df.shape[1] == K + 1):\n",
    "            raise ValueError(\n",
    "                f\"[{topo_name}] connection_matrix shape must be (K, K+1); got {cm_df.shape}\"\n",
    "            )\n",
    "\n",
    "        # Prepare nested dict for this topology\n",
    "        if topo_name not in pairs_by_topology:\n",
    "            pairs_by_topology[topo_name] = {}\n",
    "\n",
    "        # Compare with every scenario\n",
    "        topo_ts = _topology_time_step(topo_data)\n",
    "        for scen_name, ds in datasets.items():\n",
    "            ds_Delta = _delta_from_episodes(ds[\"episodes\"])\n",
    "            delta_ok = bool(np.isclose(ds_Delta, topo_ts, atol=1e-12))\n",
    "            msg = \"OK\" if delta_ok else (\n",
    "                f\"time_step mismatch (dataset Delta={ds_Delta}, topology time_step={topo_ts})\"\n",
    "            )\n",
    "            if (not delta_ok) and strict_delta_match:\n",
    "                raise ValueError(f\"[{topo_name} × {scen_name}] {msg}\")\n",
    "\n",
    "            # Store a topology→scenario bundle\n",
    "            pairs_by_topology[topo_name][scen_name] = {\n",
    "                \"scenario\": scen_name,\n",
    "                \"topology\": topo_name,\n",
    "                \"Delta\": ds_Delta,\n",
    "                \"K\": K,\n",
    "                \"dataset\": ds,\n",
    "                \"topology_data\": topo_data,\n",
    "                \"topology_meta_data\": meta_data,\n",
    "                \"connection_matrix_df\": cm_df,\n",
    "                \"checks\": {\"delta_match\": delta_ok, \"message\": msg}\n",
    "            }\n",
    "\n",
    "    return pairs_by_topology\n",
    "\n",
    "def print_pairs_summary_topology_first(pairs_by_topology: Dict[str, Dict[str, Any]]) -> None:\n",
    "    # Pretty-print a compact summary in topology→scenario order\n",
    "    print(\"=== TOPOLOGY PAIRS × SCENARIO ===\")\n",
    "    for topo_name, scen_map in pairs_by_topology.items():\n",
    "        print(f\"[TOPOLOGY] {topo_name}\")\n",
    "        for scen_name, bundle in scen_map.items():\n",
    "            flag  = \"OK\" if bundle[\"checks\"][\"delta_match\"] else \"FAIL\"\n",
    "            K     = bundle[\"K\"]\n",
    "            Delta = bundle[\"Delta\"]\n",
    "            msg   = bundle[\"checks\"][\"message\"]\n",
    "            print(f\"  - [{flag}] {topo_name} :: {scen_name}  |  K={K}  Delta={Delta}  -> {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPOLOGY PAIRS × SCENARIO ===\n",
      "[TOPOLOGY] clustered\n",
      "  - [OK] clustered :: heavy  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] clustered :: light  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] clustered :: moderate  |  K=18  Delta=1.0  -> OK\n",
      "[TOPOLOGY] full_mesh\n",
      "  - [OK] full_mesh :: heavy  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] full_mesh :: light  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] full_mesh :: moderate  |  K=18  Delta=1.0  -> OK\n",
      "[TOPOLOGY] sparse_ring\n",
      "  - [OK] sparse_ring :: heavy  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] sparse_ring :: light  |  K=18  Delta=1.0  -> OK\n",
      "  - [OK] sparse_ring :: moderate  |  K=18  Delta=1.0  -> OK\n"
     ]
    }
   ],
   "source": [
    "# Assume you already have:\n",
    "# datasets: Dict[str, Dict[str, Any]]  # e.g., {\"light\": {...}, \"moderate\": {...}, \"heavy\": {...}}\n",
    "# topologies:       Dict[str, Dict[str, Any]]  # e.g., {\"full_mesh\": {...}, \"clustered\": {...}, \"sparse_ring\": {...}}\n",
    "\n",
    "pairs_by_topology = build_topology_scenario_pairs(\n",
    "    datasets=datasets,\n",
    "    topologies=topologies,\n",
    "    strict_delta_match=True  # set False if you want to allow mismatches but mark them as FAIL\n",
    ")\n",
    "\n",
    "print_pairs_summary_topology_first(pairs_by_topology)\n",
    "\n",
    "# Access pattern:\n",
    "# pairs_by_topology[\"full_mesh\"][\"light\"][\"dataset\"][\"tasks\"]  -> tasks DataFrame for (full_mesh, light)\n",
    "# pairs_by_topology[\"clustered\"][\"heavy\"][\"connection_matrix_df\"] -> K x (K+1) matrix for that topology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. Agent→MEC mapping (for all pairs) </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
