{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Prepare data and configure the environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1. Data Loading (Data I/O) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "dataset_dir = '../Data_Generator/datasets'\n",
    "topology_dir = '../Topology_Generator/topologies'\n",
    "environment_dir = '../Environment_Generator/simulation_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_directory(dataset_dir: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Episode-first loader for the structure:\n",
    "\n",
    "        dataset_dir/\n",
    "          ep_000/\n",
    "            light/\n",
    "              episodes.csv\n",
    "              arrivals.csv\n",
    "              tasks.csv\n",
    "              summary_stats.csv      (optional for this loader)\n",
    "            moderate/\n",
    "              ...\n",
    "            heavy/\n",
    "              ...\n",
    "            dataset_metadata.json   (optional, per-episode metadata)\n",
    "\n",
    "    Returns:\n",
    "        datasets = {\n",
    "            \"ep_000\": {\n",
    "                \"light\":   {\"episodes\": df, \"arrivals\": df, \"tasks\": df},\n",
    "                \"moderate\":{\"...\"},\n",
    "                \"heavy\":   {\"...\"},\n",
    "                \"_meta\":   {...}  # if dataset_metadata.json exists\n",
    "            },\n",
    "            \"ep_001\": { ... },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        raise ValueError(f\"dataset_dir does not exist or is not a directory: {dataset_dir}\")\n",
    "\n",
    "    # Step 1 — find ep_* directories\n",
    "    ep_dirs = sorted([\n",
    "        name for name in os.listdir(dataset_dir)\n",
    "        if os.path.isdir(os.path.join(dataset_dir, name)) and name.startswith(\"ep_\")\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        if not ep_dirs:\n",
    "            print(f\"[warn] no ep_* folders found under root '{dataset_dir}'\")\n",
    "        else:\n",
    "            print(f\"[info] detected episodes: {ep_dirs}\")\n",
    "\n",
    "    # Step 2 — for each episode, detect scenarios and load CSVs\n",
    "    for ep_name in ep_dirs:\n",
    "        ep_path = os.path.join(dataset_dir, ep_name)\n",
    "        datasets[ep_name] = {}\n",
    "\n",
    "        # Scenario names (e.g., light / moderate / heavy)\n",
    "        scenario_names = sorted([\n",
    "            name for name in os.listdir(ep_path)\n",
    "            if os.path.isdir(os.path.join(ep_path, name))\n",
    "        ])\n",
    "\n",
    "        if verbose:\n",
    "            if not scenario_names:\n",
    "                print(f\"[warn] no scenario folders found under episode '{ep_name}'\")\n",
    "            else:\n",
    "                print(f\"[info] {ep_name}: scenarios detected -> {scenario_names}\")\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            scn_path = os.path.join(ep_path, scenario)\n",
    "            try:\n",
    "                episodes_csv = os.path.join(scn_path, \"episodes.csv\")\n",
    "                arrivals_csv = os.path.join(scn_path, \"arrivals.csv\")\n",
    "                tasks_csv    = os.path.join(scn_path, \"tasks.csv\")\n",
    "\n",
    "                dfs = {\n",
    "                    \"episodes\": pd.read_csv(episodes_csv),\n",
    "                    \"arrivals\": pd.read_csv(arrivals_csv),\n",
    "                    \"tasks\":    pd.read_csv(tasks_csv),\n",
    "                }\n",
    "                datasets[ep_name][scenario] = dfs\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                if verbose:\n",
    "                    print(f\"[error] missing CSV in {scn_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Step 3 — load per-episode metadata if present\n",
    "        meta_path = os.path.join(ep_path, \"dataset_metadata.json\")\n",
    "        if os.path.isfile(meta_path):\n",
    "            try:\n",
    "                with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    meta = json.load(f)\n",
    "                datasets[ep_name][\"_meta\"] = meta\n",
    "                if verbose:\n",
    "                    print(f\"[info] loaded metadata for {ep_name} from {meta_path}\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"[warn] could not load metadata for {ep_name}: {e}\")\n",
    "\n",
    "    # Optional summary printing\n",
    "    if verbose:\n",
    "        print(\"\\n=== Dataset Summary ===\")\n",
    "        print(f\"episodes detected: {len(datasets)}\")\n",
    "        for ep_name in sorted(datasets.keys()):\n",
    "            keys_here = sorted(datasets[ep_name].keys())\n",
    "            scenarios_here = [k for k in keys_here if not k.startswith(\"_\")]\n",
    "            print(f\"  - {ep_name}: scenarios = {scenarios_here}\")\n",
    "            for scn in scenarios_here:\n",
    "                dfs = datasets[ep_name][scn]\n",
    "                n_ep   = len(dfs[\"episodes\"])\n",
    "                n_arr  = len(dfs[\"arrivals\"])\n",
    "                n_task = len(dfs[\"tasks\"])\n",
    "                print(\n",
    "                    f\"      {scn:9s} → \"\n",
    "                    f\"episodes:{n_ep:3d}  \"\n",
    "                    f\"arrivals:{n_arr:6d}  \"\n",
    "                    f\"tasks:{n_task:6d}\"\n",
    "                )\n",
    "            if \"_meta\" in datasets[ep_name]:\n",
    "                print(f\"      meta: dataset_metadata.json loaded\")\n",
    "        print(\"=======================================\\n\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] detected episodes: ['ep_000']\n",
      "[info] ep_000: scenarios detected -> ['heavy', 'light', 'moderate']\n",
      "[info] loaded metadata for ep_000 from ../Data_Generator/datasets\\ep_000\\dataset_metadata.json\n",
      "\n",
      "=== Dataset Summary ===\n",
      "episodes detected: 1\n",
      "  - ep_000: scenarios = ['heavy', 'light', 'moderate']\n",
      "      heavy     → episodes:  1  arrivals:  7929  tasks:  7929\n",
      "      light     → episodes:  1  arrivals:  2008  tasks:  2008\n",
      "      moderate  → episodes:  1  arrivals:  4969  tasks:  4969\n",
      "      meta: dataset_metadata.json loaded\n",
      "=======================================\n",
      "\n",
      "\n",
      "[info] printing from episode='ep_000', scenario='heavy'\n",
      "\n",
      "arrivals:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>t_slot</th>\n",
       "      <th>t_time</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>task_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  t_slot  t_time  mec_id  task_id\n",
       "0    heavy           0       0     0.0       0        0\n",
       "1    heavy           0       0     0.0       0        1\n",
       "2    heavy           0       0     0.0       0        2\n",
       "3    heavy           0       0     0.0       0        3\n",
       "4    heavy           0       0     0.0       0        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    7929 non-null   object \n",
      " 1   episode_id  7929 non-null   int64  \n",
      " 2   t_slot      7929 non-null   int64  \n",
      " 3   t_time      7929 non-null   float64\n",
      " 4   mec_id      7929 non-null   int64  \n",
      " 5   task_id     7929 non-null   int64  \n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 371.8+ KB\n",
      "\n",
      "episodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>Delta</th>\n",
       "      <th>T_slots</th>\n",
       "      <th>T_decision</th>\n",
       "      <th>T_drain</th>\n",
       "      <th>hours</th>\n",
       "      <th>N_mecs</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>20</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  Delta  T_slots  T_decision  T_drain     hours  N_mecs  \\\n",
       "0    heavy           0    1.0      110         100       10  0.030556      20   \n",
       "\n",
       "   seed  \n",
       "0   345  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    1 non-null      object \n",
      " 1   episode_id  1 non-null      int64  \n",
      " 2   Delta       1 non-null      float64\n",
      " 3   T_slots     1 non-null      int64  \n",
      " 4   T_decision  1 non-null      int64  \n",
      " 5   T_drain     1 non-null      int64  \n",
      " 6   hours       1 non-null      float64\n",
      " 7   N_mecs      1 non-null      int64  \n",
      " 8   seed        1 non-null      int64  \n",
      "dtypes: float64(2), int64(6), object(1)\n",
      "memory usage: 200.0+ bytes\n",
      "\n",
      "tasks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.588426e+09</td>\n",
       "      <td>7.942128e+09</td>\n",
       "      <td>76.217380</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.578404</td>\n",
       "      <td>0.578404</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408449</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.486160e+09</td>\n",
       "      <td>5.944639e+09</td>\n",
       "      <td>96.590225</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.441022e+09</td>\n",
       "      <td>4.882044e+09</td>\n",
       "      <td>95.325920</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.564350e+09</td>\n",
       "      <td>1.282175e+10</td>\n",
       "      <td>75.990030</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495647</td>\n",
       "      <td>0.495647</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800898</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.821414e+09</td>\n",
       "      <td>7.285658e+09</td>\n",
       "      <td>74.964060</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516735</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  mec_id  t_arrival_slot  t_arrival_time  b_mb  \\\n",
       "0    heavy           0        0       0               0             0.0   5.0   \n",
       "1    heavy           0        1       0               0             0.0   4.0   \n",
       "2    heavy           0        2       0               0             0.0   2.0   \n",
       "3    heavy           0        3       0               0             0.0   5.0   \n",
       "4    heavy           0        4       0               0             0.0   4.0   \n",
       "\n",
       "   rho_cyc_per_mb      c_cycles     mem_mb modality  has_deadline  deadline_s  \\\n",
       "0    1.588426e+09  7.942128e+09  76.217380     text             1    0.578404   \n",
       "1    1.486160e+09  5.944639e+09  96.590225     text             0         NaN   \n",
       "2    2.441022e+09  4.882044e+09  95.325920   sensor             0         NaN   \n",
       "3    2.564350e+09  1.282175e+10  75.990030    image             1    0.495647   \n",
       "4    1.821414e+09  7.285658e+09  74.964060     text             0         NaN   \n",
       "\n",
       "   deadline_time  non_atomic  split_ratio action_space_hint  \n",
       "0       0.578404           1     0.408449        continuous  \n",
       "1            NaN           0     0.000000          discrete  \n",
       "2            NaN           0     0.000000          discrete  \n",
       "3       0.495647           1     0.800898        continuous  \n",
       "4            NaN           1     0.516735        continuous  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   scenario           7929 non-null   object \n",
      " 1   episode_id         7929 non-null   int64  \n",
      " 2   task_id            7929 non-null   int64  \n",
      " 3   mec_id             7929 non-null   int64  \n",
      " 4   t_arrival_slot     7929 non-null   int64  \n",
      " 5   t_arrival_time     7929 non-null   float64\n",
      " 6   b_mb               7929 non-null   float64\n",
      " 7   rho_cyc_per_mb     7929 non-null   float64\n",
      " 8   c_cycles           7929 non-null   float64\n",
      " 9   mem_mb             7929 non-null   float64\n",
      " 10  modality           7929 non-null   object \n",
      " 11  has_deadline       7929 non-null   int64  \n",
      " 12  deadline_s         2762 non-null   float64\n",
      " 13  deadline_time      2762 non-null   float64\n",
      " 14  non_atomic         7929 non-null   int64  \n",
      " 15  split_ratio        7929 non-null   float64\n",
      " 16  action_space_hint  7929 non-null   object \n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      "arrivals per mec_id:\n",
      "0     537\n",
      "1     414\n",
      "2     400\n",
      "3     314\n",
      "4     368\n",
      "5     358\n",
      "6     315\n",
      "7     409\n",
      "8     428\n",
      "9     341\n",
      "10    393\n",
      "11    326\n",
      "12    426\n",
      "13    358\n",
      "14    360\n",
      "15    425\n",
      "16    448\n",
      "17    509\n",
      "18    471\n",
      "19    329\n",
      "Name: mec_id, dtype: int64\n",
      "\n",
      "meta (dataset_metadata.json):\n",
      "{\n",
      "  \"schema_version\": \"1.0.0\",\n",
      "  \"generated_at_utc\": \"2025-11-30T09:24:21Z\",\n",
      "  \"env\": {\n",
      "    \"python\": \"3.10.9\",\n",
      "    \"user\": \"niush\"\n",
      "  },\n",
      "  \"episodes_root\": \"d:\\\\Karamozi\\\\Meet 0.5\\\\Implementation\\\\Clustering-based-MARL-Offloading-in-Edge-Computing\\\\Data_Generator\\\\datasets\\\\ep_000\",\n",
      "  \"notes\": {\n",
      "    \"policy_agnostic\": true,\n",
      "    \"queueing\": \"not simulated here\",\n",
      "    \"timing\": {\n",
      "      \"description\": \"HOODIE-style: T_decision slots with arrivals + T_drain slots without arrivals.\"\n",
      "    },\n",
      "    \"clipping\": {\n",
      "      \"enabled\": true,\n",
      "      \"method\": \"lognormal analytic quantile cap\",\n",
      "      \"qcap\": 0.99,\n",
      "      \"z_table_keys\": [\n",
      "        0.9,\n",
      "        0.95,\n",
      "        0.975,\n",
      "        0.99,\n",
      "        0.995,\n",
      "        0.999\n",
      "      ]\n",
      "    },\n",
      "    \"action_space_hint\": \"derived from non_atomic; final decision belongs to environment\"\n",
      "  },\n",
      "  \"scenarios\": [\n",
      "    {\n",
      "      \"name\": \"light\",\n",
      "      \"fingerprint\": \"79dbc753ca88eaf1\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 143\n",
      "      },\n",
      "      \"N_mecs\": 20,\n",
      "      \"ArrivalRanges\": {\n",
      "        \"lam_sec_min\": 1.0,\n",
      "        \"lam_sec_max\": 1.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 2.0,\n",
      "        \"b_sigma_g\": 1.55,\n",
      "        \"rho_median\": 1000000000.0,\n",
      "        \"rho_sigma_g\": 1.45,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.4,\n",
      "        \"p_deadline\": 0.15,\n",
      "        \"deadline_min\": 0.8,\n",
      "        \"deadline_max\": 2.0,\n",
      "        \"p_non_atomic\": 0.25,\n",
      "        \"split_ratio_min\": 0.25,\n",
      "        \"split_ratio_max\": 0.75,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per MEC)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"moderate\",\n",
      "      \"fingerprint\": \"1831d840d6e39cdb\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 244\n",
      "      },\n",
      "      \"N_mecs\": 20,\n",
      "      \"ArrivalRanges\": {\n",
      "        \"lam_sec_min\": 2.0,\n",
      "        \"lam_sec_max\": 3.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 3.0,\n",
      "        \"b_sigma_g\": 1.6,\n",
      "        \"rho_median\": 1200000000.0,\n",
      "        \"rho_sigma_g\": 1.5,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.45,\n",
      "        \"p_deadline\": 0.25,\n",
      "        \"deadline_min\": 0.5,\n",
      "        \"deadline_max\": 1.5,\n",
      "        \"p_non_atomic\": 0.35,\n",
      "        \"split_ratio_min\": 0.3,\n",
      "        \"split_ratio_max\": 0.8,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per MEC)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"heavy\",\n",
      "      \"fingerprint\": \"a9ed7d1eec239c00\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 345\n",
      "      },\n",
      "      \"N_mecs\": 20,\n",
      "      \"ArrivalRanges\": {\n",
      "        \"lam_sec_min\": 3.0,\n",
      "        \"lam_sec_max\": 5.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 5.0,\n",
      "        \"b_sigma_g\": 1.7,\n",
      "        \"rho_median\": 1500000000.0,\n",
      "        \"rho_sigma_g\": 1.55,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.5,\n",
      "        \"p_deadline\": 0.35,\n",
      "        \"deadline_min\": 0.3,\n",
      "        \"deadline_max\": 1.0,\n",
      "        \"p_non_atomic\": 0.45,\n",
      "        \"split_ratio_min\": 0.4,\n",
      "        \"split_ratio_max\": 0.85,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per MEC)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets_from_directory(dataset_dir, verbose=True)\n",
    "\n",
    "# Pick an episode and a scenario for inspection\n",
    "ep_name = sorted(datasets.keys())[0] if datasets else None\n",
    "scenario = \"heavy\"  # or \"light\" / \"moderate\"\n",
    "\n",
    "if ep_name is not None and scenario in datasets[ep_name]:\n",
    "    print(f\"\\n[info] printing from episode='{ep_name}', scenario='{scenario}'\")\n",
    "\n",
    "    dfs = datasets[ep_name][scenario]\n",
    "\n",
    "    print(\"\\narrivals:\")\n",
    "    display(dfs[\"arrivals\"].head())\n",
    "    dfs[\"arrivals\"].info()\n",
    "\n",
    "    print(\"\\nepisodes:\")\n",
    "    display(dfs[\"episodes\"].head())\n",
    "    dfs[\"episodes\"].info()\n",
    "\n",
    "    print(\"\\ntasks:\")\n",
    "    display(dfs[\"tasks\"].head())\n",
    "    dfs[\"tasks\"].info()\n",
    "\n",
    "    # Example: check how many arrivals per mec_id\n",
    "    if \"mec_id\" in dfs[\"arrivals\"].columns:\n",
    "        print(\"\\narrivals per mec_id:\")\n",
    "        print(dfs[\"arrivals\"][\"mec_id\"].value_counts().sort_index())\n",
    "\n",
    "    # Show metadata if available\n",
    "    if \"_meta\" in datasets[ep_name]:\n",
    "        print(\"\\nmeta (dataset_metadata.json):\")\n",
    "        print(json.dumps(datasets[ep_name][\"_meta\"], ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(\"[error] no datasets found or requested scenario is missing for the chosen episode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_environment_from_directory(\n",
    "    environment_dir: str,\n",
    "    mec_filename: str = \"environment.csv\",\n",
    "    cloud_filename: str = \"cloud_info.csv\",\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load MEC & Cloud environment from CSV files generated by Environment_Generator.\n",
    "\n",
    "    Expected structure:\n",
    "        environment_dir/\n",
    "          environment.csv   # servers: Server ID, Private CPU Capacity, Public CPU Capacity\n",
    "          cloud_info.csv    # cloud:   id, computational_capacity\n",
    "\n",
    "    Returns:\n",
    "        environment = {\n",
    "            \"servers_df\":    DataFrame,\n",
    "            \"cloud_df\":      DataFrame,\n",
    "            \"num_servers\":   int,\n",
    "            \"num_clouds\":    int,\n",
    "            \"private_cpu\":   np.ndarray,\n",
    "            \"public_cpu\":    np.ndarray,\n",
    "            \"cloud_capacity\": np.ndarray,\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(environment_dir):\n",
    "        raise ValueError(f\"environment_dir does not exist or is not a directory: {environment_dir}\")\n",
    "\n",
    "    mec_path = os.path.join(environment_dir, mec_filename)\n",
    "    cloud_path = os.path.join(environment_dir, cloud_filename)\n",
    "\n",
    "    if not os.path.isfile(mec_path):\n",
    "        raise FileNotFoundError(f\"MEC environment CSV not found: {mec_path}\")\n",
    "    if not os.path.isfile(cloud_path):\n",
    "        raise FileNotFoundError(f\"Cloud info CSV not found: {cloud_path}\")\n",
    "\n",
    "    # --- Load CSVs ---\n",
    "    servers_df = pd.read_csv(mec_path)\n",
    "    cloud_df   = pd.read_csv(cloud_path)\n",
    "\n",
    "    # --- Basic sanity on required columns ---\n",
    "    required_server_cols = {\"Server ID\", \"Private CPU Capacity\", \"Public CPU Capacity\"}\n",
    "    if not required_server_cols.issubset(servers_df.columns):\n",
    "        missing = required_server_cols - set(servers_df.columns)\n",
    "        raise ValueError(f\"servers_df is missing required columns: {missing}\")\n",
    "\n",
    "    required_cloud_cols = {\"id\", \"computational_capacity\"}\n",
    "    if not required_cloud_cols.issubset(cloud_df.columns):\n",
    "        missing = required_cloud_cols - set(cloud_df.columns)\n",
    "        raise ValueError(f\"cloud_df is missing required columns: {missing}\")\n",
    "\n",
    "    # --- Basic shapes and arrays ---\n",
    "    num_servers = len(servers_df)\n",
    "    num_clouds  = len(cloud_df)\n",
    "\n",
    "    private_cpu = servers_df[\"Private CPU Capacity\"].to_numpy()\n",
    "    public_cpu  = servers_df[\"Public CPU Capacity\"].to_numpy()\n",
    "    cloud_cap   = cloud_df[\"computational_capacity\"].to_numpy()\n",
    "\n",
    "    # --- Optional: enforce that Server ID are 0..num_servers-1 (consistent with mec_id) ---\n",
    "    server_ids = servers_df[\"Server ID\"].to_numpy()\n",
    "    expected_ids = np.arange(num_servers, dtype=server_ids.dtype)\n",
    "    if not np.array_equal(server_ids, expected_ids):\n",
    "        raise ValueError(\n",
    "            \"Server ID column is not a simple 0..num_servers-1 sequence. \"\n",
    "            \"This may break consistency with mec_id in datasets/topology. \"\n",
    "            f\"Found IDs: {server_ids}\"\n",
    "        )\n",
    "\n",
    "    environment = {\n",
    "        \"servers_df\": servers_df,\n",
    "        \"cloud_df\": cloud_df,\n",
    "        \"num_servers\": num_servers,\n",
    "        \"num_clouds\": num_clouds,\n",
    "        \"private_cpu\": private_cpu,\n",
    "        \"public_cpu\": public_cpu,\n",
    "        \"cloud_capacity\": cloud_cap,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[info] loaded environment from '{environment_dir}'\")\n",
    "        print(f\"  - num_servers: {num_servers}\")\n",
    "        print(f\"  - num_clouds : {num_clouds}\")\n",
    "        print(\"  - private_cpu (first 5):\", private_cpu[:5])\n",
    "        print(\"  - public_cpu  (first 5):\", public_cpu[:5])\n",
    "        print(\"  - cloud_capacity:\", cloud_cap)\n",
    "\n",
    "    return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loaded environment from '../Environment_Generator/simulation_output'\n",
      "  - num_servers: 20\n",
      "  - num_clouds : 1\n",
      "  - private_cpu (first 5): [5. 5. 5. 5. 5.]\n",
      "  - public_cpu  (first 5): [5. 5. 5. 5. 5.]\n",
      "  - cloud_capacity: [30.]\n",
      "\n",
      "[environment] servers_df.head():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Server ID</th>\n",
       "      <th>Private CPU Capacity</th>\n",
       "      <th>Public CPU Capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Server ID  Private CPU Capacity  Public CPU Capacity\n",
       "0          0                   5.0                  5.0\n",
       "1          1                   5.0                  5.0\n",
       "2          2                   5.0                  5.0\n",
       "3          3                   5.0                  5.0\n",
       "4          4                   5.0                  5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Server ID             20 non-null     int64  \n",
      " 1   Private CPU Capacity  20 non-null     float64\n",
      " 2   Public CPU Capacity   20 non-null     float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 608.0 bytes\n",
      "\n",
      "[environment] cloud_df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>computational_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  computational_capacity\n",
       "0   0                    30.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1 non-null      int64  \n",
      " 1   computational_capacity  1 non-null      float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 144.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# ---- Load environment (MEC + Cloud) and quick inspection ----\n",
    "environment = load_environment_from_directory(environment_dir, verbose=True)\n",
    "\n",
    "print(\"\\n[environment] servers_df.head():\")\n",
    "display(environment[\"servers_df\"].head())\n",
    "environment[\"servers_df\"].info()\n",
    "\n",
    "print(\"\\n[environment] cloud_df:\")\n",
    "display(environment[\"cloud_df\"])\n",
    "environment[\"cloud_df\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topologies_from_directory(topology_dir: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Load all topologies from a root directory.\n",
    "\n",
    "    Expected structure:\n",
    "        topology_dir/\n",
    "          <topology_name>/\n",
    "            topology.json\n",
    "            topology_meta.json\n",
    "            connection_matrix.csv\n",
    "    \"\"\"\n",
    "    topologies = {}\n",
    "\n",
    "    if not os.path.isdir(topology_dir):\n",
    "        raise ValueError(f\"topology_dir does not exist or is not a directory: {topology_dir}\")\n",
    "\n",
    "    # Iterate over subdirectories (each representing a topology variant)\n",
    "    for topology_name in os.listdir(topology_dir):\n",
    "        topology_path = os.path.join(topology_dir, topology_name)\n",
    "\n",
    "        # Only process directories\n",
    "        if not os.path.isdir(topology_path):\n",
    "            continue\n",
    "\n",
    "        topology_json_path = os.path.join(topology_path, \"topology.json\")\n",
    "        meta_json_path = os.path.join(topology_path, \"topology_meta.json\")\n",
    "        connection_matrix_csv_path = os.path.join(topology_path, \"connection_matrix.csv\")\n",
    "\n",
    "        # Check for required files\n",
    "        if not (os.path.isfile(topology_json_path) and\n",
    "                os.path.isfile(meta_json_path) and\n",
    "                os.path.isfile(connection_matrix_csv_path)):\n",
    "            if verbose:\n",
    "                print(f\"[warn] skipping '{topology_name}' — missing one of required files.\")\n",
    "            continue\n",
    "\n",
    "        # --- Load JSON & CSV files ---\n",
    "        with open(topology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            topology_data = json.load(f)\n",
    "        with open(meta_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta_data = json.load(f)\n",
    "\n",
    "        # First column is row labels (mec_i), so we use index_col=0\n",
    "        connection_matrix = pd.read_csv(connection_matrix_csv_path, index_col=0)\n",
    "\n",
    "        # Optional sanity check: match matrix shape with number_of_servers\n",
    "        if \"number_of_servers\" in topology_data:\n",
    "            K = int(topology_data[\"number_of_servers\"])\n",
    "            if connection_matrix.shape[0] != K:\n",
    "                raise ValueError(\n",
    "                    f\"Topology '{topology_name}': number_of_servers={K} \"\n",
    "                    f\"but connection_matrix has {connection_matrix.shape[0]} rows.\"\n",
    "                )\n",
    "            if connection_matrix.shape[1] != K + 1:\n",
    "                raise ValueError(\n",
    "                    f\"Topology '{topology_name}': expected {K+1} columns in \"\n",
    "                    f\"connection_matrix (K MEC + 1 cloud), got {connection_matrix.shape[1]}.\"\n",
    "                )\n",
    "\n",
    "        topologies[topology_name] = {\n",
    "            \"topology_data\": topology_data,\n",
    "            \"meta_data\": meta_data,\n",
    "            \"connection_matrix\": connection_matrix\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[info] loaded topologies: {sorted(topologies.keys())}\")\n",
    "\n",
    "    return topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loaded topologies: ['clustered', 'fully_connected', 'skip_connections']\n",
      "topology clustered -> connection_matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_0</th>\n",
       "      <th>mec_1</th>\n",
       "      <th>mec_2</th>\n",
       "      <th>mec_3</th>\n",
       "      <th>mec_4</th>\n",
       "      <th>mec_5</th>\n",
       "      <th>mec_6</th>\n",
       "      <th>mec_7</th>\n",
       "      <th>mec_8</th>\n",
       "      <th>mec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>mec_11</th>\n",
       "      <th>mec_12</th>\n",
       "      <th>mec_13</th>\n",
       "      <th>mec_14</th>\n",
       "      <th>mec_15</th>\n",
       "      <th>mec_16</th>\n",
       "      <th>mec_17</th>\n",
       "      <th>mec_18</th>\n",
       "      <th>mec_19</th>\n",
       "      <th>cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mec_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mec_0  mec_1  mec_2  mec_3  mec_4  mec_5  mec_6  mec_7  mec_8  mec_9  \\\n",
       "mec_0    0.0    3.0    3.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
       "mec_1    3.0    0.0    3.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
       "mec_2    3.0    3.0    0.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
       "mec_3    3.0    3.0    3.0    0.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
       "mec_4    3.0    3.0    3.0    3.0    0.0    3.0    3.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...  mec_11  mec_12  mec_13  mec_14  mec_15  mec_16  mec_17  mec_18  \\\n",
       "mec_0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_1  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_2  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_3  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_4  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       mec_19  cloud  \n",
       "mec_0     0.0    5.0  \n",
       "mec_1     0.0    5.0  \n",
       "mec_2     0.0    5.0  \n",
       "mec_3     0.0    5.0  \n",
       "mec_4     0.0    5.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20 entries, mec_0 to mec_19\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   mec_0   20 non-null     float64\n",
      " 1   mec_1   20 non-null     float64\n",
      " 2   mec_2   20 non-null     float64\n",
      " 3   mec_3   20 non-null     float64\n",
      " 4   mec_4   20 non-null     float64\n",
      " 5   mec_5   20 non-null     float64\n",
      " 6   mec_6   20 non-null     float64\n",
      " 7   mec_7   20 non-null     float64\n",
      " 8   mec_8   20 non-null     float64\n",
      " 9   mec_9   20 non-null     float64\n",
      " 10  mec_10  20 non-null     float64\n",
      " 11  mec_11  20 non-null     float64\n",
      " 12  mec_12  20 non-null     float64\n",
      " 13  mec_13  20 non-null     float64\n",
      " 14  mec_14  20 non-null     float64\n",
      " 15  mec_15  20 non-null     float64\n",
      " 16  mec_16  20 non-null     float64\n",
      " 17  mec_17  20 non-null     float64\n",
      " 18  mec_18  20 non-null     float64\n",
      " 19  mec_19  20 non-null     float64\n",
      " 20  cloud   20 non-null     float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 3.4+ KB\n",
      "\n",
      "topology clustered -> topology_data\n",
      "{'number_of_servers': 20, 'private_cpu_capacities': [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0], 'public_cpu_capacities': [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0], 'cloud_computational_capacity': 30.0, 'connection_matrix': [[0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 3.0, 3.0, 3.0, 0.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 0.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 3.0, 3.0, 3.0, 3.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 0.0, 3.0, 3.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 0.0, 3.0, 5.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 5.0]], 'time_step': 1.0, 'topology_type': 'clustered', 'skip_k': 3, 'symmetric': True, 'num_clusters': 3}\n",
      "\n",
      "topology clustered -> meta_data\n",
      "{'generated_at_utc': '2025-11-29T13:26:01Z', 'fingerprint': '3c447a62a8adc777', 'env': {'python': '3.10.9', 'user': 'niush'}, 'hyperparameters': {'time_step': 1.0, 'bw_mec_mec': 3.0, 'bw_mec_cloud': 5.0, 'topology_type': 'clustered', 'skip_k': 3, 'symmetric': True, 'num_clusters': 3, 'inter_cluster_frac': 0.0, 'environment_mec_file': '../Environment_Generator/simulation_output/environment.csv', 'environment_cloud_file': '../Environment_Generator/simulation_output/cloud_info.csv', 'seed': 20251229}}\n"
     ]
    }
   ],
   "source": [
    "topologies = load_topologies_from_directory(topology_dir, verbose=True)\n",
    "\n",
    "print('topology clustered -> connection_matrix')\n",
    "display(topologies['clustered']['connection_matrix'].head())\n",
    "topologies['clustered']['connection_matrix'].info()\n",
    "\n",
    "print('\\ntopology clustered -> topology_data')\n",
    "print(topologies['clustered']['topology_data'])\n",
    "\n",
    "print('\\ntopology clustered -> meta_data')\n",
    "print(topologies['clustered']['meta_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2. Data Validation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data, we must validate that required columns exist and that IDs match properly.\n",
    "\n",
    "**The code below performs three layers of checks:** \n",
    "\n",
    "- Validate each dataset (episodes/agents/arrivals/tasks)\n",
    "- Validate each topology (JSON and connection matrix)\n",
    "- Validate dataset–topology pairs for unit alignment and overall consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Generic helpers ----------\n",
    "def _require(cond: bool, msg: str, errors: list):\n",
    "    \"\"\"Collect errors instead of stopping at first failure.\"\"\"\n",
    "    if not cond:\n",
    "        errors.append(msg)\n",
    "\n",
    "def _has_cols(df: pd.DataFrame, cols: list) -> bool:\n",
    "    \"\"\"Check that all required columns exist in a DataFrame.\"\"\"\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# ---------- Dataset-level validation ----------\n",
    "def validate_one_dataset(dataset_key: str, ds: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single dataset pack (episodes/arrivals/tasks) for one (episode, scenario).\n",
    "    'dataset_key' is just a label for error messages, e.g. 'ep_000/heavy'.\n",
    "\n",
    "    Assumes the new dataset structure (no agents.csv, MEC-based):\n",
    "      - episodes.csv: has N_mecs instead of N_agents\n",
    "      - arrivals.csv: uses mec_id\n",
    "      - tasks.csv:    uses mec_id\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds.get(\"episodes\")\n",
    "    arrivals = ds.get(\"arrivals\")\n",
    "    tasks    = ds.get(\"tasks\")\n",
    "\n",
    "    # 1) Presence checks\n",
    "    _require(isinstance(episodes, pd.DataFrame), f\"[{dataset_key}] episodes missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(arrivals, pd.DataFrame), f\"[{dataset_key}] arrivals missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(tasks,    pd.DataFrame), f\"[{dataset_key}] tasks missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 2) Required columns (aligned with new Dataset_Generator)\n",
    "    req_ep_cols  = [\"scenario\", \"episode_id\", \"Delta\",\n",
    "                    \"T_slots\", \"T_decision\", \"T_drain\",\n",
    "                    \"hours\", \"N_mecs\", \"seed\"]\n",
    "    req_ar_cols  = [\"scenario\", \"episode_id\", \"t_slot\", \"t_time\", \"mec_id\", \"task_id\"]\n",
    "    req_tk_cols  = [\n",
    "        \"scenario\", \"episode_id\", \"task_id\", \"mec_id\",\n",
    "        \"t_arrival_slot\", \"t_arrival_time\",\n",
    "        \"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\", \"modality\",\n",
    "        \"has_deadline\", \"deadline_s\", \"deadline_time\",\n",
    "        \"non_atomic\", \"split_ratio\", \"action_space_hint\"\n",
    "    ]\n",
    "\n",
    "    _require(_has_cols(episodes, req_ep_cols),\n",
    "             f\"[{dataset_key}] episodes missing required columns\", errors)\n",
    "    _require(_has_cols(arrivals, req_ar_cols),\n",
    "             f\"[{dataset_key}] arrivals missing required columns\", errors)\n",
    "    _require(_has_cols(tasks,    req_tk_cols),\n",
    "             f\"[{dataset_key}] tasks missing required columns\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 3) Basic integrity checks\n",
    "\n",
    "    # 3.1) unique task_id\n",
    "    _require(tasks[\"task_id\"].is_unique,\n",
    "             f\"[{dataset_key}] task_id is not unique\", errors)\n",
    "\n",
    "    # 3.2) arrivals and tasks should have the same number of rows\n",
    "    _require(len(arrivals) == len(tasks),\n",
    "             f\"[{dataset_key}] arrivals ({len(arrivals)}) != tasks ({len(tasks)})\", errors)\n",
    "\n",
    "    # 3.3) mec_id range against N_mecs\n",
    "    N_mecs = int(episodes[\"N_mecs\"].iloc[0])\n",
    "    _require(N_mecs > 0, f\"[{dataset_key}] N_mecs must be > 0 (got {N_mecs})\", errors)\n",
    "\n",
    "    if len(tasks):\n",
    "        mec_min = int(tasks[\"mec_id\"].min())\n",
    "        mec_max = int(tasks[\"mec_id\"].max())\n",
    "        _require(mec_min >= 0,\n",
    "                 f\"[{dataset_key}] mec_id minimum must be >= 0 (got {mec_min})\", errors)\n",
    "        _require(mec_max <= N_mecs - 1,\n",
    "                 f\"[{dataset_key}] mec_id maximum must be <= N_mecs-1 ({N_mecs-1}), got {mec_max}\", errors)\n",
    "\n",
    "    if len(arrivals):\n",
    "        mec_min_a = int(arrivals[\"mec_id\"].min())\n",
    "        mec_max_a = int(arrivals[\"mec_id\"].max())\n",
    "        _require(mec_min_a >= 0,\n",
    "                 f\"[{dataset_key}] arrivals.mec_id minimum must be >= 0 (got {mec_min_a})\", errors)\n",
    "        _require(mec_max_a <= N_mecs - 1,\n",
    "                 f\"[{dataset_key}] arrivals.mec_id maximum must be <= N_mecs-1 ({N_mecs-1}), got {mec_max_a}\", errors)\n",
    "\n",
    "    # 3.4) non-negative task numerics\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            _require((tasks[col] >= 0).all(),\n",
    "                     f\"[{dataset_key}] tasks.{col} has negative values\", errors)\n",
    "\n",
    "    # 3.5) deadline coherence\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        bad_deadline = tasks[\n",
    "            (tasks[\"has_deadline\"] == 1) &\n",
    "            ((tasks[\"deadline_s\"].isna()) | (tasks[\"deadline_s\"] <= 0))\n",
    "        ]\n",
    "        _require(len(bad_deadline) == 0,\n",
    "                 f\"[{dataset_key}] tasks with deadline have invalid deadline_s\", errors)\n",
    "\n",
    "    # 3.6) single Delta / T_slots / T_decision / T_drain inside this (episode, scenario)\n",
    "    _require(episodes[\"Delta\"].nunique() == 1,\n",
    "             f\"[{dataset_key}] multiple Delta values in episodes\", errors)\n",
    "    _require(episodes[\"T_slots\"].nunique() == 1,\n",
    "             f\"[{dataset_key}] multiple T_slots in episodes\", errors)\n",
    "    _require(episodes[\"T_decision\"].nunique() == 1,\n",
    "             f\"[{dataset_key}] multiple T_decision values in episodes\", errors)\n",
    "    _require(episodes[\"T_drain\"].nunique() == 1,\n",
    "             f\"[{dataset_key}] multiple T_drain values in episodes\", errors)\n",
    "\n",
    "    # 3.7) arrivals inside slot range [0, T_slots-1] and only in decision horizon\n",
    "    T_slots    = int(episodes[\"T_slots\"].iloc[0])\n",
    "    T_decision = int(episodes[\"T_decision\"].iloc[0])\n",
    "\n",
    "    if len(tasks):\n",
    "        _require(int(tasks[\"t_arrival_slot\"].max()) <= T_slots - 1,\n",
    "                 f\"[{dataset_key}] t_arrival_slot exceeds T_slots-1\", errors)\n",
    "\n",
    "    if len(arrivals):\n",
    "        _require(int(arrivals[\"t_slot\"].max()) <= T_slots - 1,\n",
    "                 f\"[{dataset_key}] arrivals.t_slot exceeds T_slots-1\", errors)\n",
    "        _require(int(arrivals[\"t_slot\"].max()) <= T_decision - 1,\n",
    "                 f\"[{dataset_key}] arrivals.t_slot exceeds T_decision-1 (should only arrive during decision horizon)\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Environment-level validation ----------\n",
    "def validate_environment(environment: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate the MEC + Cloud environment loaded from CSVs.\n",
    "\n",
    "    Expected structure (from load_environment_from_directory):\n",
    "\n",
    "        environment = {\n",
    "            \"servers_df\":    DataFrame,\n",
    "            \"cloud_df\":      DataFrame,\n",
    "            \"num_servers\":   int,\n",
    "            \"num_clouds\":    int,\n",
    "            \"private_cpu\":   np.ndarray,\n",
    "            \"public_cpu\":    np.ndarray,\n",
    "            \"cloud_capacity\": np.ndarray\n",
    "        }\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    servers_df  = environment.get(\"servers_df\")\n",
    "    cloud_df    = environment.get(\"cloud_df\")\n",
    "    num_servers = environment.get(\"num_servers\")\n",
    "    num_clouds  = environment.get(\"num_clouds\")\n",
    "    private_cpu = environment.get(\"private_cpu\")\n",
    "    public_cpu  = environment.get(\"public_cpu\")\n",
    "    cloud_cap   = environment.get(\"cloud_capacity\")\n",
    "\n",
    "    # --- presence / type checks ---\n",
    "    _require(isinstance(servers_df, pd.DataFrame),\n",
    "             \"[env] servers_df missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(cloud_df, pd.DataFrame),\n",
    "             \"[env] cloud_df missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # --- required columns (According to Environment_Generator) ---\n",
    "    required_server_cols = {\"Server ID\", \"Private CPU Capacity\", \"Public CPU Capacity\"}\n",
    "    _require(required_server_cols.issubset(servers_df.columns),\n",
    "             f\"[env] servers_df missing required columns: \"\n",
    "             f\"{required_server_cols - set(servers_df.columns)}\", errors)\n",
    "\n",
    "    required_cloud_cols = {\"id\", \"computational_capacity\"}\n",
    "    _require(required_cloud_cols.issubset(cloud_df.columns),\n",
    "             f\"[env] cloud_df missing required columns: \"\n",
    "             f\"{required_cloud_cols - set(cloud_df.columns)}\", errors)\n",
    "\n",
    "    # --- num_servers / num_clouds consistency ---\n",
    "    if num_servers is not None:\n",
    "        _require(num_servers == len(servers_df),\n",
    "                 f\"[env] num_servers ({num_servers}) != len(servers_df) ({len(servers_df)})\", errors)\n",
    "    else:\n",
    "        num_servers = len(servers_df)\n",
    "\n",
    "    if num_clouds is not None:\n",
    "        _require(num_clouds == len(cloud_df),\n",
    "                 f\"[env] num_clouds ({num_clouds}) != len(cloud_df) ({len(cloud_df)})\", errors)\n",
    "    else:\n",
    "        num_clouds = len(cloud_df)\n",
    "\n",
    "    # --- Server ID sanity (unique and from 0 to num_servers-1) ---\n",
    "    server_ids = servers_df[\"Server ID\"].to_numpy()\n",
    "    _require(len(np.unique(server_ids)) == len(server_ids),\n",
    "             \"[env] duplicate Server ID values detected\", errors)\n",
    "\n",
    "    if np.issubdtype(server_ids.dtype, np.number):\n",
    "        _require(server_ids.min() == 0,\n",
    "                 f\"[env] Server ID should start from 0 (got {server_ids.min()})\", errors)\n",
    "        _require(server_ids.max() == num_servers - 1,\n",
    "                 f\"[env] Server ID max should be num_servers-1 ({num_servers-1}), \"\n",
    "                 f\"got {server_ids.max()}\", errors)\n",
    "\n",
    "    # --- arrays shapes ---\n",
    "    if private_cpu is not None:\n",
    "        _require(len(private_cpu) == num_servers,\n",
    "                 \"[env] private_cpu length != num_servers\", errors)\n",
    "    if public_cpu is not None:\n",
    "        _require(len(public_cpu) == num_servers,\n",
    "                 \"[env] public_cpu length != num_servers\", errors)\n",
    "    if cloud_cap is not None:\n",
    "        _require(len(cloud_cap) == num_clouds,\n",
    "                 \"[env] cloud_capacity length != num_clouds\", errors)\n",
    "\n",
    "    # --- non-negativity ---\n",
    "    if private_cpu is not None:\n",
    "        _require((private_cpu >= 0).all(),\n",
    "                 \"[env] private_cpu contains negative values\", errors)\n",
    "    if public_cpu is not None:\n",
    "        _require((public_cpu >= 0).all(),\n",
    "                 \"[env] public_cpu contains negative values\", errors)\n",
    "    if cloud_cap is not None:\n",
    "        _require((cloud_cap >= 0).all(),\n",
    "                 \"[env] cloud_capacity contains negative values\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Topology-level validation ----------\n",
    "def validate_one_topology(topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single topology pack: topology.json + topology_meta.json + connection_matrix.csv.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    topo = topo_entry.get(\"topology_data\")\n",
    "    meta = topo_entry.get(\"meta_data\")\n",
    "    Mdf  = topo_entry.get(\"connection_matrix\")\n",
    "\n",
    "    _require(isinstance(topo, dict), f\"[{topology_name}] topology_data missing or not a dict\", errors)\n",
    "    _require(isinstance(meta, dict), f\"[{topology_name}] meta_data missing or not a dict\", errors)\n",
    "    _require(isinstance(Mdf,  pd.DataFrame), f\"[{topology_name}] connection_matrix CSV missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    req_keys = [\n",
    "        \"number_of_servers\", \"private_cpu_capacities\", \"public_cpu_capacities\",\n",
    "        \"cloud_computational_capacity\", \"connection_matrix\", \"time_step\"\n",
    "    ]\n",
    "    for k in req_keys:\n",
    "        _require(k in topo, f\"[{topology_name}] topology.json missing key: {k}\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K = int(topo[\"number_of_servers\"])\n",
    "    _require(len(topo[\"private_cpu_capacities\"]) == K,\n",
    "             f\"[{topology_name}] private_cpu_capacities length != K\", errors)\n",
    "    _require(len(topo[\"public_cpu_capacities\"])  == K,\n",
    "             f\"[{topology_name}] public_cpu_capacities length != K\", errors)\n",
    "\n",
    "    Mjson = topo[\"connection_matrix\"]\n",
    "    _require(\n",
    "        isinstance(Mjson, list)\n",
    "        and len(Mjson) == K\n",
    "        and (K == 0 or len(Mjson[0]) == K + 1),\n",
    "        f\"[{topology_name}] connection_matrix in JSON must be K x (K+1)\",\n",
    "        errors\n",
    "    )\n",
    "    _require(Mdf.shape == (K, K + 1),\n",
    "             f\"[{topology_name}] connection_matrix.csv shape must be K x (K+1)\", errors)\n",
    "\n",
    "    # MEC->Cloud capacities (last column) must be > 0\n",
    "    vert_csv = Mdf.iloc[:, K]\n",
    "    _require((vert_csv > 0).all(),\n",
    "             f\"[{topology_name}] MEC->Cloud capacities must be > 0\", errors)\n",
    "\n",
    "    # MEC<->MEC capacities (first K columns) must be >= 0\n",
    "    horiz_csv = Mdf.iloc[:, :K]\n",
    "    _require((horiz_csv.values >= 0).all(),\n",
    "             f\"[{topology_name}] MEC<->MEC capacities contain negatives\", errors)\n",
    "\n",
    "    _require(\"time_step\" in topo, f\"[{topology_name}] missing time_step\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Pairwise validation (environment <-> topology) ----------\n",
    "def validate_environment_topology_pair(environment: dict,\n",
    "                                       topology_name: str,\n",
    "                                       topo_entry: dict,\n",
    "                                       atol: float = 1e-6) -> list:\n",
    "    \"\"\"\n",
    "    Validate alignment between environment (MEC/Cloud CSVs)\n",
    "    and one topology (topology.json + connection_matrix).\n",
    "\n",
    "    Checks:\n",
    "      - number_of_servers == env.num_servers\n",
    "      - private/public CPU capacities match (up to tolerance)\n",
    "      - cloud_computational_capacity matches env.cloud_capacity\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    num_servers  = environment.get(\"num_servers\")\n",
    "    private_cpu  = environment.get(\"private_cpu\")\n",
    "    public_cpu   = environment.get(\"public_cpu\")\n",
    "    cloud_cap    = environment.get(\"cloud_capacity\")\n",
    "\n",
    "    topo = topo_entry.get(\"topology_data\")\n",
    "\n",
    "    _require(isinstance(topo, dict),\n",
    "             f\"[env x {topology_name}] topology_data missing or not a dict\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K = int(topo[\"number_of_servers\"])\n",
    "    _require(K == num_servers,\n",
    "             f\"[env x {topology_name}] number_of_servers ({K}) != env.num_servers ({num_servers})\", errors)\n",
    "\n",
    "    topo_priv  = np.array(topo[\"private_cpu_capacities\"], dtype=float)\n",
    "    topo_pub   = np.array(topo[\"public_cpu_capacities\"], dtype=float)\n",
    "    topo_cloud = float(topo[\"cloud_computational_capacity\"])\n",
    "\n",
    "    if private_cpu is not None:\n",
    "        _require(topo_priv.shape == private_cpu.shape,\n",
    "                 f\"[env x {topology_name}] shape mismatch in private CPU capacities \"\n",
    "                 f\"topo:{topo_priv.shape}, env:{private_cpu.shape}\", errors)\n",
    "        if topo_priv.shape == private_cpu.shape:\n",
    "            _require(np.allclose(topo_priv, private_cpu, atol=atol),\n",
    "                     f\"[env x {topology_name}] private CPU capacities differ (topology vs environment)\", errors)\n",
    "\n",
    "    if public_cpu is not None:\n",
    "        _require(topo_pub.shape == public_cpu.shape,\n",
    "                 f\"[env x {topology_name}] shape mismatch in public CPU capacities \"\n",
    "                 f\"topo:{topo_pub.shape}, env:{public_cpu.shape}\", errors)\n",
    "        if topo_pub.shape == public_cpu.shape:\n",
    "            _require(np.allclose(topo_pub, public_cpu, atol=atol),\n",
    "                     f\"[env x {topology_name}] public CPU capacities differ (topology vs environment)\", errors)\n",
    "\n",
    "    if cloud_cap is not None and len(cloud_cap) > 0:\n",
    "        env_cloud_val = float(cloud_cap[0])   # currently we have 1 cloud\n",
    "        _require(abs(topo_cloud - env_cloud_val) <= atol,\n",
    "                 f\"[env x {topology_name}] cloud capacity differs: \"\n",
    "                 f\"topology={topo_cloud}, env={env_cloud_val}\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Pairwise validation (dataset <-> topology) ----------\n",
    "def validate_dataset_topology_pair(ep_name: str, scenario: str, ds: dict,\n",
    "                                   topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate alignment between one (episode, scenario) dataset and one topology.\n",
    "\n",
    "    Ensures:\n",
    "      - Delta == time_step\n",
    "      - N_mecs == number_of_servers\n",
    "      - mec_id values are within [0, K-1]\n",
    "      - compute capacities are non-negative\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds[\"episodes\"]\n",
    "    arrivals = ds[\"arrivals\"]\n",
    "    tasks    = ds[\"tasks\"]\n",
    "    topo     = topo_entry[\"topology_data\"]\n",
    "    K        = int(topo[\"number_of_servers\"])\n",
    "\n",
    "    # Delta vs time_step\n",
    "    Delta     = float(episodes[\"Delta\"].iloc[0])\n",
    "    time_step = float(topo[\"time_step\"])\n",
    "    _require(abs(Delta - time_step) < 1e-9,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] Delta ({Delta}) != time_step ({time_step})\", errors)\n",
    "\n",
    "    # N_mecs vs number_of_servers\n",
    "    N_mecs = int(episodes[\"N_mecs\"].iloc[0])\n",
    "    _require(N_mecs == K,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] N_mecs ({N_mecs}) != number_of_servers ({K})\", errors)\n",
    "\n",
    "    # mec_id range inside dataset vs topology K\n",
    "    if len(tasks):\n",
    "        min_mec_t = int(tasks[\"mec_id\"].min())\n",
    "        max_mec_t = int(tasks[\"mec_id\"].max())\n",
    "        _require(min_mec_t >= 0 and max_mec_t <= K - 1,\n",
    "                 f\"[{ep_name}/{scenario} x {topology_name}] tasks.mec_id out of range [0, {K-1}]\", errors)\n",
    "\n",
    "    if len(arrivals):\n",
    "        min_mec_a = int(arrivals[\"mec_id\"].min())\n",
    "        max_mec_a = int(arrivals[\"mec_id\"].max())\n",
    "        _require(min_mec_a >= 0 and max_mec_a <= K - 1,\n",
    "                 f\"[{ep_name}/{scenario} x {topology_name}] arrivals.mec_id out of range [0, {K-1}]\", errors)\n",
    "\n",
    "    # Non-negative compute capacities in topology\n",
    "    priv  = topo[\"private_cpu_capacities\"]\n",
    "    pub   = topo[\"public_cpu_capacities\"]\n",
    "    cloud = topo[\"cloud_computational_capacity\"]\n",
    "    _require(all(x >= 0 for x in priv) and all(x >= 0 for x in pub) and cloud >= 0,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] negative compute capacities detected\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Episode-level Delta consistency across scenarios ----------\n",
    "def validate_episode_delta_consistency(ep_name: str, ep_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    Check that all SCENARIOS (light/moderate/heavy/...) inside one episode\n",
    "    share the same Delta and T_slots.\n",
    "\n",
    "    ep_dict:\n",
    "        {\n",
    "          \"light\":   {\"episodes\": df, ...},\n",
    "          \"moderate\":{...},\n",
    "          \"heavy\":   {...},\n",
    "          \"_meta\":   {...}  # we should ignore this\n",
    "        }\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    deltas = set()\n",
    "    tslots = set()\n",
    "\n",
    "    for scenario, ds in ep_dict.items():\n",
    "        # Ignore metadata or entries without 'episodes'\n",
    "        if not isinstance(ds, dict) or \"episodes\" not in ds:\n",
    "            continue\n",
    "\n",
    "        ep_df = ds[\"episodes\"]\n",
    "        if len(ep_df):\n",
    "            deltas.add(float(ep_df[\"Delta\"].iloc[0]))\n",
    "            tslots.add(int(ep_df[\"T_slots\"].iloc[0]))\n",
    "        else:\n",
    "            errors.append(f\"[{ep_name}/{scenario}] episodes.csv is empty\")\n",
    "\n",
    "    if len(deltas) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple Delta values across scenarios: {sorted(deltas)}\")\n",
    "    if len(tslots) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple T_slots values across scenarios: {sorted(tslots)}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Global validation entrypoint ----------\n",
    "def validate_everything(datasets: dict,\n",
    "                        topologies: dict,\n",
    "                        environment: dict) -> dict:\n",
    "    \"\"\"\n",
    "    'datasets' shape (episode-first, new structure):\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}  # optional per-episode metadata\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"datasets\": {},\n",
    "        \"episodes_consistency\": {},\n",
    "        \"topologies\": {},\n",
    "        \"pairs\": {},\n",
    "        \"environment\": {},\n",
    "        \"env_topology_pairs\": {}\n",
    "    }\n",
    "\n",
    "    # 0) Validate environment (MEC + Cloud)\n",
    "    env_errs = validate_environment(environment)\n",
    "    report[\"environment\"] = {\"ok\": len(env_errs) == 0, \"errors\": env_errs}\n",
    "\n",
    "    # 1) Validate each (episode, scenario) dataset\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        report[\"datasets\"][ep_name] = {}\n",
    "\n",
    "        # Only real scenarios (not _meta)\n",
    "        scenario_names = [\n",
    "            scn for scn, dpack in ep_pack.items()\n",
    "            if isinstance(dpack, dict) and \"episodes\" in dpack\n",
    "        ]\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            dpack = ep_pack[scenario]\n",
    "            key = f\"{ep_name}/{scenario}\"\n",
    "            errs = validate_one_dataset(key, dpack)\n",
    "            report[\"datasets\"][ep_name][scenario] = {\n",
    "                \"ok\": len(errs) == 0,\n",
    "                \"errors\": errs\n",
    "            }\n",
    "\n",
    "    # 2) Episode-level Delta/T_slots consistency across scenarios\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        errs = validate_episode_delta_consistency(ep_name, ep_pack)\n",
    "        report[\"episodes_consistency\"][ep_name] = {\n",
    "            \"ok\": len(errs) == 0,\n",
    "            \"errors\": errs\n",
    "        }\n",
    "\n",
    "    # 3) Validate each topology\n",
    "    for tname, tpack in topologies.items():\n",
    "        errs = validate_one_topology(tname, tpack)\n",
    "        report[\"topologies\"][tname] = {\n",
    "            \"ok\": len(errs) == 0,\n",
    "            \"errors\": errs\n",
    "        }\n",
    "\n",
    "    # 4) Pairwise validation: ENVIRONMENT × each topology\n",
    "    for tname, tpack in topologies.items():\n",
    "        if report[\"environment\"][\"ok\"] and report[\"topologies\"][tname][\"ok\"]:\n",
    "            errs = validate_environment_topology_pair(environment, tname, tpack)\n",
    "            report[\"env_topology_pairs\"][tname] = {\n",
    "                \"ok\": len(errs) == 0,\n",
    "                \"errors\": errs\n",
    "            }\n",
    "        else:\n",
    "            report[\"env_topology_pairs\"][tname] = {\n",
    "                \"ok\": False,\n",
    "                \"errors\": [\"Skipped due to invalid environment or topology.\"]\n",
    "            }\n",
    "\n",
    "    # 5) Pairwise validation for every valid (ep, scenario) × valid topology\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        scenario_names = list(report[\"datasets\"][ep_name].keys())\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            dpack = ep_pack[scenario]\n",
    "            d_ok  = report[\"datasets\"][ep_name][scenario][\"ok\"]\n",
    "            ep_ok = report[\"episodes_consistency\"][ep_name][\"ok\"]\n",
    "\n",
    "            for tname, tres in report[\"topologies\"].items():\n",
    "                key = f\"{ep_name}/{scenario}__{tname}\"\n",
    "                if d_ok and ep_ok and tres[\"ok\"]:\n",
    "                    errs = validate_dataset_topology_pair(\n",
    "                        ep_name, scenario, dpack, tname, topologies[tname]\n",
    "                    )\n",
    "                    report[\"pairs\"][key] = {\n",
    "                        \"ok\": len(errs) == 0,\n",
    "                        \"errors\": errs\n",
    "                    }\n",
    "                else:\n",
    "                    report[\"pairs\"][key] = {\n",
    "                        \"ok\": False,\n",
    "                        \"errors\": [\"Skipped due to upstream invalid dataset/episode/topology.\"]\n",
    "                    }\n",
    "\n",
    "    return report\n",
    "\n",
    "# ---------- Pretty printer ----------\n",
    "def print_validation_report(report: dict):\n",
    "    print(\"=== ENVIRONMENT (MEC + Cloud) ===\")\n",
    "    env_info = report.get(\"environment\")\n",
    "    if env_info:\n",
    "        status = \"OK\" if env_info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] environment\")\n",
    "        for e in env_info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== DATASETS (episode/scenario) ===\")\n",
    "    for ep_name, ep_res in report[\"datasets\"].items():\n",
    "        for scenario, info in ep_res.items():\n",
    "            status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "            print(f\"[{status}] {ep_name}/{scenario}\")\n",
    "            for e in info[\"errors\"]:\n",
    "                print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\")\n",
    "    for ep_name, info in report[\"episodes_consistency\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {ep_name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== TOPOLOGIES ===\")\n",
    "    for name, info in report[\"topologies\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\")\n",
    "    for key, info in report[\"pairs\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {key}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "            \n",
    "    print(\"\\n=== ENVIRONMENT × TOPOLOGY PAIRS ===\")\n",
    "    for tname, info in report.get(\"env_topology_pairs\", {}).items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] env x {tname}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT (MEC + Cloud) ===\n",
      "[OK] environment\n",
      "\n",
      "=== DATASETS (episode/scenario) ===\n",
      "[OK] ep_000/heavy\n",
      "[OK] ep_000/light\n",
      "[OK] ep_000/moderate\n",
      "\n",
      "=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\n",
      "[OK] ep_000\n",
      "\n",
      "=== TOPOLOGIES ===\n",
      "[OK] clustered\n",
      "[OK] fully_connected\n",
      "[OK] skip_connections\n",
      "\n",
      "=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\n",
      "[OK] ep_000/heavy__clustered\n",
      "[OK] ep_000/heavy__fully_connected\n",
      "[OK] ep_000/heavy__skip_connections\n",
      "[OK] ep_000/light__clustered\n",
      "[OK] ep_000/light__fully_connected\n",
      "[OK] ep_000/light__skip_connections\n",
      "[OK] ep_000/moderate__clustered\n",
      "[OK] ep_000/moderate__fully_connected\n",
      "[OK] ep_000/moderate__skip_connections\n",
      "\n",
      "=== ENVIRONMENT × TOPOLOGY PAIRS ===\n",
      "[OK] env x clustered\n",
      "[OK] env x fully_connected\n",
      "[OK] env x skip_connections\n"
     ]
    }
   ],
   "source": [
    "# ---------- Run validation ----------\n",
    "report = validate_everything(datasets, topologies, environment)\n",
    "print_validation_report(report)\n",
    "\n",
    "all_ok = (\n",
    "    # datasets\n",
    "    all(info[\"ok\"] for ep in report[\"datasets\"].values() for info in ep.values())\n",
    "    # per-episode Delta/T_slots consistency\n",
    "    and all(info[\"ok\"] for info in report[\"episodes_consistency\"].values())\n",
    "    # topologies\n",
    "    and all(info[\"ok\"] for info in report[\"topologies\"].values())\n",
    "    # dataset × topology pairs\n",
    "    and all(info[\"ok\"] for info in report[\"pairs\"].values())\n",
    "    # environment itself\n",
    "    and report[\"environment\"][\"ok\"]\n",
    "    # environment × topology pairs\n",
    "    and all(info[\"ok\"] for info in report[\"env_topology_pairs\"].values())\n",
    ")\n",
    "\n",
    "if not all_ok:\n",
    "    raise RuntimeError(\"Validation failed. See printed report for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3. Units Alignment </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we align units for all dataset episodes and scenarios\n",
    "and run consistency checks against all topologies.\n",
    "- Datasets: use Delta from episodes.csv; add per-slot helpers:\n",
    "    agents.f_local_slot (cycles/slot), tasks.deadline_slots (integer or NaN)\n",
    "    \n",
    "- Topologies: capacities are already per-slot (generator multiplied by Δ);\n",
    "    we only verify time_step == Delta and non-negative capacities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the private_cpu, public_cpu and cloud_capacity capacities in terms of \"per second\" and multiply them by Delta to convert them to \"per slot\", which is also consistent with the f_local logic. Only if we later decide to keep all capacities in Environment_Generator \"per-slot\" from the beginning, this multiplication by Delta will not be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks that didn't had the deadline sensitivity or delay sensivity, had these values ​​in the table set to \"NAN\", so here we added the step of converting these to -1 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Helpers: safe getters =====\n",
    "def _get_delta(episodes_df: pd.DataFrame) -> float:\n",
    "    # Expect a single Delta value in episodes; take the first row\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain a 'Delta' column.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _ensure_numeric_positive(name: str, arr: np.ndarray):\n",
    "    # Basic sanity: finite and no negatives for capacities/links\n",
    "    if not np.isfinite(arr).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "    if (arr < 0).any():\n",
    "        raise ValueError(f\"{name} contains negative values.\")\n",
    "\n",
    "# ===== Alignment: per-dataset (one (episode, scenario) pack) =====\n",
    "def align_units_for_dataset(dataset: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given one dataset dict {\"episodes\",\"arrivals\",\"tasks\"},\n",
    "    return a copy with aligned/derived columns (per-slot helpers).\n",
    "\n",
    "    Notes:\n",
    "      - This version assumes MEC-based datasets (no agents.csv).\n",
    "      - Deadline-related NaNs are replaced with -1 so that the\n",
    "        final table is RL-friendly (no NaNs in deadline fields).\n",
    "    \"\"\"\n",
    "    episodes = dataset[\"episodes\"].copy()\n",
    "    arrivals = dataset[\"arrivals\"].copy()\n",
    "    tasks    = dataset[\"tasks\"].copy()\n",
    "\n",
    "    Delta = _get_delta(episodes)\n",
    "\n",
    "    # Tasks: ensure integer arrival slot\n",
    "    if \"t_arrival_slot\" not in tasks.columns:\n",
    "        raise ValueError(\"tasks.csv must contain 't_arrival_slot'.\")\n",
    "    tasks[\"t_arrival_slot\"] = tasks[\"t_arrival_slot\"].astype(int)\n",
    "\n",
    "    # Compute deadline_slots (in slots) and replace missing deadlines with -1\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        # Normalize types first\n",
    "        tasks[\"has_deadline\"] = tasks[\"has_deadline\"].astype(int)\n",
    "        tasks[\"deadline_s\"] = tasks[\"deadline_s\"].astype(\"float32\")\n",
    "\n",
    "        def _to_deadline_slots(row):\n",
    "            # only tasks with has_deadline == 1 and valid deadline_s\n",
    "            if int(row[\"has_deadline\"]) == 1 and np.isfinite(row[\"deadline_s\"]):\n",
    "                return int(math.ceil(float(row[\"deadline_s\"]) / Delta))\n",
    "            # no deadline → use -1 sentinel\n",
    "            return -1\n",
    "\n",
    "        tasks[\"deadline_slots\"] = tasks.apply(_to_deadline_slots, axis=1).astype(\"int32\")\n",
    "\n",
    "        # For tasks that effectively have no valid deadline, set -1 in deadline_s and deadline_time\n",
    "        no_valid_deadline_mask = (tasks[\"has_deadline\"] == 0) | (~np.isfinite(tasks[\"deadline_s\"]))\n",
    "\n",
    "        tasks.loc[no_valid_deadline_mask, \"deadline_s\"] = -1.0\n",
    "\n",
    "        if \"deadline_time\" in tasks.columns:\n",
    "            # deadline_time is absolute time; for 'no deadline' we also put -1\n",
    "            tasks[\"deadline_time\"] = tasks[\"deadline_time\"].astype(\"float32\")\n",
    "            tasks.loc[no_valid_deadline_mask, \"deadline_time\"] = -1.0\n",
    "\n",
    "    # Ensure key numeric task fields are floats\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            tasks[col] = tasks[col].astype(float)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes,\n",
    "        \"arrivals\": arrivals,\n",
    "        \"tasks\":    tasks,\n",
    "    }\n",
    "\n",
    "# ===== Alignment: per-environment against a target Delta =====\n",
    "def align_environment_units(environment: dict, target_Delta: float) -> dict:\n",
    "    \"\"\"\n",
    "    Take raw environment dict from load_environment_from_directory and\n",
    "    return an aligned copy with per-slot capacities.\n",
    "\n",
    "    Adds:\n",
    "      - private_cpu_slot, public_cpu_slot, cloud_capacity_slot (numpy arrays)\n",
    "      - columns in servers_df / cloud_df reflecting per-slot capacities\n",
    "    \"\"\"\n",
    "    if environment is None:\n",
    "        raise ValueError(\"environment is None in align_environment_units.\")\n",
    "\n",
    "    env_aligned = dict(environment)  # shallow copy of dict; we'll copy DFs below\n",
    "\n",
    "    servers_df = env_aligned[\"servers_df\"].copy()\n",
    "    cloud_df   = env_aligned[\"cloud_df\"].copy()\n",
    "\n",
    "    private_cpu = np.asarray(env_aligned[\"private_cpu\"], dtype=float)\n",
    "    public_cpu  = np.asarray(env_aligned[\"public_cpu\"], dtype=float)\n",
    "    cloud_cap   = np.asarray(env_aligned[\"cloud_capacity\"], dtype=float)\n",
    "\n",
    "    # basic sanity (non-finite / negative)\n",
    "    _ensure_numeric_positive(\"env.private_cpu\", private_cpu)\n",
    "    _ensure_numeric_positive(\"env.public_cpu\", public_cpu)\n",
    "    _ensure_numeric_positive(\"env.cloud_capacity\", cloud_cap)\n",
    "\n",
    "    # per-slot capacities\n",
    "    private_cpu_slot = private_cpu * float(target_Delta)\n",
    "    public_cpu_slot  = public_cpu  * float(target_Delta)\n",
    "    cloud_slot       = cloud_cap   * float(target_Delta)\n",
    "\n",
    "    # update numpy arrays in dict\n",
    "    env_aligned[\"private_cpu\"] = private_cpu\n",
    "    env_aligned[\"public_cpu\"]  = public_cpu\n",
    "    env_aligned[\"cloud_capacity\"] = cloud_cap\n",
    "    env_aligned[\"private_cpu_slot\"] = private_cpu_slot\n",
    "    env_aligned[\"public_cpu_slot\"]  = public_cpu_slot\n",
    "    env_aligned[\"cloud_capacity_slot\"] = cloud_slot\n",
    "\n",
    "    # also add columns to DataFrames for convenience\n",
    "    servers_df[\"Private CPU Capacity\"] = private_cpu\n",
    "    servers_df[\"Public CPU Capacity\"]  = public_cpu\n",
    "    servers_df[\"Private CPU per_slot\"] = private_cpu_slot\n",
    "    servers_df[\"Public CPU per_slot\"]  = public_cpu_slot\n",
    "\n",
    "    cloud_df[\"computational_capacity\"] = cloud_cap\n",
    "    cloud_df[\"capacity_per_slot\"]      = cloud_slot\n",
    "\n",
    "    env_aligned[\"servers_df\"] = servers_df\n",
    "    env_aligned[\"cloud_df\"]   = cloud_df\n",
    "\n",
    "    return env_aligned\n",
    "\n",
    "# ===== Verification: per-topology against a target Delta =====\n",
    "def verify_topology_units(topology: Dict[str, Any], target_Delta: float) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ensure topology capacities are per-slot and consistent with dataset Delta:\n",
    "    - time_step == target_Delta\n",
    "    - shapes are valid (K x (K+1))\n",
    "    - capacities non-negative\n",
    "    Returns (ok, message).\n",
    "    \"\"\"\n",
    "    # time_step check\n",
    "    ts = float(topology.get(\"time_step\", -1.0))\n",
    "    if not np.isclose(ts, target_Delta, atol=1e-9):\n",
    "        return (False, f\"time_step mismatch (topology={ts}, dataset Delta={target_Delta})\")\n",
    "\n",
    "    # K and lists\n",
    "    K = int(topology.get(\"number_of_servers\", -1))\n",
    "    priv = np.array(topology.get(\"private_cpu_capacities\", []), dtype=float)\n",
    "    pub  = np.array(topology.get(\"public_cpu_capacities\", []), dtype=float)\n",
    "    cloud = float(topology.get(\"cloud_computational_capacity\", -1.0))\n",
    "    M = np.array(topology.get(\"connection_matrix\", []), dtype=float)\n",
    "\n",
    "    if K <= 0:\n",
    "        return (False, \"Invalid 'number_of_servers' (K<=0).\")\n",
    "    if priv.shape[0] != K or pub.shape[0] != K:\n",
    "        return (False, \"private/public capacities must have length K.\")\n",
    "    if M.shape != (K, K+1):\n",
    "        return (False, f\"connection_matrix shape must be (K, K+1), got {M.shape}.\")\n",
    "\n",
    "    # Non-negative checks\n",
    "    _ensure_numeric_positive(\"private_cpu_capacities\", priv)\n",
    "    _ensure_numeric_positive(\"public_cpu_capacities\",  pub)\n",
    "    if not np.isfinite(cloud) or cloud < 0:\n",
    "        return (False, \"cloud_computational_capacity must be non-negative and finite.\")\n",
    "    _ensure_numeric_positive(\"connection_matrix\", M)\n",
    "\n",
    "    return (True, \"topology verified (per-slot, consistent).\")\n",
    "\n",
    "# ===== Batch alignment for ALL datasets (episode-first) & ALL topologies =====\n",
    "def align_all_units(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    topologies_by_name: Dict[str, Dict[str, Any]],\n",
    "    environment: dict\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input 'datasets_ep_first' shape:\n",
    "\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}   # optional per-episode metadata (NO episodes/tasks)\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"datasets_aligned\": { ep_name: { scenario: aligned_pack_or_meta, ... }, ... },\n",
    "          \"topology_checks\":  { topo_name: { ep_name: { scenario: {ok, message} } } },\n",
    "          \"environment_aligned\": dict,\n",
    "          \"environment_Delta\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"datasets_aligned\": {},\n",
    "        \"topology_checks\":  {},\n",
    "        \"environment_aligned\": None,\n",
    "        \"environment_Delta\": None,\n",
    "    }\n",
    "\n",
    "    # ---- 1) Align datasets (episode/scenario) ----\n",
    "    for ep_name, ep_pack in datasets_ep_first.items():\n",
    "        out[\"datasets_aligned\"][ep_name] = {}\n",
    "\n",
    "        for scenario, ds in ep_pack.items():\n",
    "            # If the dataset is real (has episodes) → align\n",
    "            if isinstance(ds, dict) and \"episodes\" in ds:\n",
    "                try:\n",
    "                    out[\"datasets_aligned\"][ep_name][scenario] = align_units_for_dataset(ds)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"[{ep_name}/{scenario}] dataset alignment failed: {e}\") from e\n",
    "            else:\n",
    "                # For example \"_meta\" or anything else → keep as is\n",
    "                out[\"datasets_aligned\"][ep_name][scenario] = ds\n",
    "\n",
    "    # ---- 2) Infer a reference Delta for environment alignment ----\n",
    "    Delta_ref = None\n",
    "    for ep_name, ep_pack in out[\"datasets_aligned\"].items():\n",
    "        for scenario, ds in ep_pack.items():\n",
    "            if isinstance(ds, dict) and \"episodes\" in ds and len(ds[\"episodes\"]):\n",
    "                Delta_ref = _get_delta(ds[\"episodes\"])\n",
    "                break\n",
    "        if Delta_ref is not None:\n",
    "            break\n",
    "\n",
    "    if Delta_ref is None:\n",
    "        raise RuntimeError(\"Could not infer a reference Delta for environment alignment (no episodes found).\")\n",
    "\n",
    "    # ---- 3) Align environment w.r.t this Delta ----\n",
    "    env_aligned = align_environment_units(environment, Delta_ref)\n",
    "    out[\"environment_aligned\"] = env_aligned\n",
    "    out[\"environment_Delta\"]   = Delta_ref\n",
    "    \n",
    "    # ---- 4) Verify each topology against each (episode, scenario) Delta ----\n",
    "    for topo_name, topo_bundle in topologies_by_name.items():\n",
    "        topo_obj = topo_bundle.get(\"topology_data\", None)\n",
    "        if not isinstance(topo_obj, dict):\n",
    "            raise RuntimeError(f\"[{topo_name}] 'topology_data' missing or not a dict.\")\n",
    "        out[\"topology_checks\"][topo_name] = {}\n",
    "\n",
    "        for ep_name, ep_pack in out[\"datasets_aligned\"].items():\n",
    "            out[\"topology_checks\"][topo_name][ep_name] = {}\n",
    "\n",
    "            for scenario, aligned in ep_pack.items():\n",
    "                # Only check scenarios that have episodes; ignore metadata\n",
    "                if not (isinstance(aligned, dict) and \"episodes\" in aligned):\n",
    "                    continue\n",
    "\n",
    "                Delta = _get_delta(aligned[\"episodes\"])\n",
    "                ok, msg = verify_topology_units(topo_obj, Delta)\n",
    "                out[\"topology_checks\"][topo_name][ep_name][scenario] = {\n",
    "                    \"ok\": bool(ok),\n",
    "                    \"message\": msg\n",
    "                }\n",
    "\n",
    "    return out\n",
    "\n",
    "# ===== Pretty printer =====\n",
    "def print_alignment_summary(result: Dict[str, Any]):\n",
    "    # ===== DATASETS =====\n",
    "    print(\"=== DATASETS (aligned, episode/scenario) ===\")\n",
    "    for ep_name in sorted(result[\"datasets_aligned\"].keys()):\n",
    "        ep_pack = result[\"datasets_aligned\"][ep_name]\n",
    "\n",
    "        for scenario in sorted(ep_pack.keys()):\n",
    "            ds = ep_pack[scenario]\n",
    "            if not (isinstance(ds, dict) and \"episodes\" in ds):\n",
    "                continue\n",
    "\n",
    "            Delta    = _get_delta(ds[\"episodes\"])\n",
    "            n_tasks  = len(ds[\"tasks\"])\n",
    "            print(f\"[{ep_name}/{scenario}] Delta={Delta}  tasks={n_tasks}\")\n",
    "\n",
    "    # ===== TOPOLOGIES =====\n",
    "    print(\"\\n=== TOPOLOGIES (checks vs each episode/scenario) ===\")\n",
    "    for topo_name, by_ep in result[\"topology_checks\"].items():\n",
    "        print(f\"Topology: {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            for scenario, r in sorted(by_ep[ep_name].items()):\n",
    "                flag = \"OK\" if r[\"ok\"] else \"FAIL\"\n",
    "                print(f\"  - {ep_name}/{scenario}: {flag}  -> {r['message']}\")\n",
    "    \n",
    "    # ===== ENVIRONMENT =====\n",
    "    print(\"\\n=== ENVIRONMENT (aligned) ===\")\n",
    "    env_aligned = result.get(\"environment_aligned\")\n",
    "    Delta_env   = result.get(\"environment_Delta\", None)\n",
    "\n",
    "    if env_aligned is None:\n",
    "        print(\"No environment_aligned found in result.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Reference Delta used for environment per-slot capacities: {Delta_env}\")\n",
    "    print(f\"num_servers = {env_aligned['num_servers']}, num_clouds = {env_aligned['num_clouds']}\")\n",
    "\n",
    "    priv = env_aligned[\"private_cpu\"]\n",
    "    pub  = env_aligned[\"public_cpu\"]\n",
    "    priv_slot = env_aligned[\"private_cpu_slot\"]\n",
    "    pub_slot  = env_aligned[\"public_cpu_slot\"]\n",
    "    cloud_cap = env_aligned[\"cloud_capacity\"]\n",
    "    cloud_slot = env_aligned[\"cloud_capacity_slot\"]\n",
    "\n",
    "    print(\"  private_cpu (first 5):      \", priv[:5])\n",
    "    print(\"  private_cpu_per_slot (5):   \", priv_slot[:5])\n",
    "    print(\"  public_cpu (first 5):       \", pub[:5])\n",
    "    print(\"  public_cpu_per_slot (5):    \", pub_slot[:5])\n",
    "    print(\"  cloud_capacity:             \", cloud_cap)\n",
    "    print(\"  cloud_capacity_per_slot:    \", cloud_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (aligned, episode/scenario) ===\n",
      "[ep_000/heavy] Delta=1.0  tasks=7929\n",
      "[ep_000/light] Delta=1.0  tasks=2008\n",
      "[ep_000/moderate] Delta=1.0  tasks=4969\n",
      "\n",
      "=== TOPOLOGIES (checks vs each episode/scenario) ===\n",
      "Topology: clustered\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: fully_connected\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: skip_connections\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "\n",
      "=== ENVIRONMENT (aligned) ===\n",
      "Reference Delta used for environment per-slot capacities: 1.0\n",
      "num_servers = 20, num_clouds = 1\n",
      "  private_cpu (first 5):       [5. 5. 5. 5. 5.]\n",
      "  private_cpu_per_slot (5):    [5. 5. 5. 5. 5.]\n",
      "  public_cpu (first 5):        [5. 5. 5. 5. 5.]\n",
      "  public_cpu_per_slot (5):     [5. 5. 5. 5. 5.]\n",
      "  cloud_capacity:              [30.]\n",
      "  cloud_capacity_per_slot:     [30.]\n",
      "\n",
      " ===EXAMPLE===\n",
      "   task_id  mec_id  t_arrival_slot  deadline_s  deadline_slots\n",
      "0        0       0               0    1.339893               2\n",
      "1        1       0               0   -1.000000              -1\n",
      "2        2       1               0   -1.000000              -1\n",
      "3        3       3               0   -1.000000              -1\n",
      "4        4       3               0   -1.000000              -1\n"
     ]
    }
   ],
   "source": [
    "# ===== Example usage =====\n",
    "result_align = align_all_units(\n",
    "    datasets_ep_first=datasets,\n",
    "    topologies_by_name=topologies,\n",
    "    environment=environment\n",
    ")\n",
    "print_alignment_summary(result_align)\n",
    "\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "aligned_light_ep0 = result_align[\"datasets_aligned\"][\"ep_000\"][\"light\"]\n",
    "tasks_ep0_light   = aligned_light_ep0[\"tasks\"]    # has deadline_slots\n",
    "print(tasks_ep0_light[[\"task_id\", \"mec_id\", \"t_arrival_slot\", \"deadline_s\", \"deadline_slots\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.4. Build Scenario–Topology Pairs </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all datasets are paired with all topologies (Cartesian product). Each pair is checked for matching time parameters, then a basic bundle is created for further enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4. Build Scenario–Topology Pairs\n",
    "\n",
    "# In this step, we create a Cartesian product between:\n",
    "#   - all (episode, scenario) datasets\n",
    "#   - all topologies\n",
    "#\n",
    "# For each (topology, episode, scenario) triple we build a \"bundle\" that contains:\n",
    "#   - the aligned dataset (episodes, arrivals, tasks)\n",
    "#   - the topology (JSON + connection matrix)\n",
    "#   - the aligned environment (MEC + Cloud capacities)\n",
    "#\n",
    "# Output structure (topology-first):\n",
    "# pairs_by_topology = {\n",
    "#   \"<topology_name>\": {\n",
    "#       \"<ep_XXX>\": {\n",
    "#           \"<scenario>\": {\n",
    "#               'scenario': <str>,\n",
    "#               'episode': <str>,\n",
    "#               'topology': <str>,\n",
    "#               'Delta': <float>,\n",
    "#               'K': <int>,  # number_of_servers (MECs)\n",
    "#               'dataset': { 'episodes': df, 'arrivals': df, 'tasks': df },\n",
    "#               'topology_data': <dict>,\n",
    "#               'topology_meta_data': <dict or None>,\n",
    "#               'connection_matrix_df': <pd.DataFrame>,  # shape (K, K+1)\n",
    "#               'environment': <dict>,  # aligned environment (same for all pairs)\n",
    "#               'checks': {\n",
    "#                   'delta_match': bool,\n",
    "#                   'env_servers_match': bool,\n",
    "#                   'message': str\n",
    "#               }\n",
    "#           }, ...\n",
    "#       }, ...\n",
    "#   }, ...\n",
    "# }\n",
    "\n",
    "def _delta_from_episodes(episodes_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Extract a single Delta value from episodes table.\"\"\"\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain a 'Delta' column.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _topology_time_step(topo_json: Dict[str, Any]) -> float:\n",
    "    \"\"\"Extract the topology time_step from topology.json.\"\"\"\n",
    "    ts = topo_json.get(\"time_step\", None)\n",
    "    if ts is None:\n",
    "        raise ValueError(\"topology.json must contain 'time_step'.\")\n",
    "    return float(ts)\n",
    "\n",
    "def build_topology_episode_pairs(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    topologies: Dict[str, Dict[str, Any]],\n",
    "    environment: dict,\n",
    "    strict_delta_match: bool = True,\n",
    "    strict_env_match: bool = True,\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Build pairs between every topology and every (episode, scenario) dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets_ep_first :\n",
    "        Episode-first datasets, shape:\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}  # metadata only (no episodes/arrivals/tasks)\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    topologies :\n",
    "        Dict of topologies as returned by load_topologies_from_directory, e.g.:\n",
    "        {\n",
    "          \"clustered\": {\n",
    "              \"topology_data\": dict,\n",
    "              \"meta_data\": dict,\n",
    "              \"connection_matrix\": DataFrame\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    environment :\n",
    "        Aligned environment dictionary as returned by align_all_units(...)\n",
    "        under key \"environment_aligned\".\n",
    "\n",
    "    strict_delta_match :\n",
    "        If True, raise an error when dataset Delta != topology time_step.\n",
    "\n",
    "    strict_env_match :\n",
    "        If True, raise an error when topology.number_of_servers != environment.num_servers.\n",
    "    \"\"\"\n",
    "    if environment is None:\n",
    "        raise ValueError(\"environment must not be None in build_topology_episode_pairs.\")\n",
    "\n",
    "    env_num_servers = int(environment[\"num_servers\"])\n",
    "\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    # Iterate over topologies first (topology-centric view)\n",
    "    for topo_name, topo_bundle in topologies.items():\n",
    "        topo_data = topo_bundle.get(\"topology_data\", None)\n",
    "        meta_data = topo_bundle.get(\"meta_data\", None)\n",
    "        cm_df     = topo_bundle.get(\"connection_matrix\", None)\n",
    "\n",
    "        if not isinstance(topo_data, dict):\n",
    "            raise ValueError(f\"[{topo_name}] topology_data missing or not a dict.\")\n",
    "        if cm_df is None:\n",
    "            raise ValueError(f\"[{topo_name}] connection_matrix DataFrame is missing.\")\n",
    "\n",
    "        # Validate K and connection matrix shape\n",
    "        K = int(topo_data.get(\"number_of_servers\", -1))\n",
    "        if K <= 0:\n",
    "            raise ValueError(f\"[{topo_name}] invalid 'number_of_servers' in topology.json\")\n",
    "        if not (cm_df.shape[0] == K and cm_df.shape[1] == K + 1):\n",
    "            raise ValueError(\n",
    "                f\"[{topo_name}] connection_matrix shape must be (K, K+1); got {cm_df.shape}\"\n",
    "            )\n",
    "\n",
    "        # Check consistency between topology and environment (number of MEC servers)\n",
    "        env_match_ok = (K == env_num_servers)\n",
    "        if (not env_match_ok) and strict_env_match:\n",
    "            raise ValueError(\n",
    "                f\"[{topo_name}] number_of_servers ({K}) != environment.num_servers ({env_num_servers})\"\n",
    "            )\n",
    "\n",
    "        topo_ts = _topology_time_step(topo_data)\n",
    "\n",
    "        # Initialize container for this topology\n",
    "        pairs_by_topology[topo_name] = {}\n",
    "\n",
    "        # Compare against every (episode, scenario)\n",
    "        for ep_name, scenarios in datasets_ep_first.items():\n",
    "            pairs_by_topology[topo_name][ep_name] = {}\n",
    "\n",
    "            for scen_name, ds in scenarios.items():\n",
    "                # Skip metadata entries such as \"_meta\"\n",
    "                if not (isinstance(ds, dict) and \"episodes\" in ds):\n",
    "                    continue\n",
    "\n",
    "                ds_Delta = _delta_from_episodes(ds[\"episodes\"])\n",
    "                delta_ok = bool(np.isclose(ds_Delta, topo_ts, atol=1e-12))\n",
    "                msg_delta = \"OK\" if delta_ok else (\n",
    "                    f\"time_step mismatch (dataset Delta={ds_Delta}, topology time_step={topo_ts})\"\n",
    "                )\n",
    "\n",
    "                msg_env = \"OK\" if env_match_ok else (\n",
    "                    f\"env.num_servers ({env_num_servers}) != topology.K ({K})\"\n",
    "                )\n",
    "\n",
    "                # If Delta mismatch is not tolerated, raise immediately\n",
    "                if (not delta_ok) and strict_delta_match:\n",
    "                    raise ValueError(f\"[{topo_name} × {ep_name}/{scen_name}] {msg_delta}\")\n",
    "\n",
    "                # Build final message from delta + environment checks\n",
    "                if delta_ok and env_match_ok:\n",
    "                    final_msg = \"OK\"\n",
    "                else:\n",
    "                    final_msg = f\"{msg_delta}; {msg_env}\"\n",
    "\n",
    "                # Store bundle for this (topology, episode, scenario)\n",
    "                pairs_by_topology[topo_name][ep_name][scen_name] = {\n",
    "                    \"scenario\": scen_name,\n",
    "                    \"episode\": ep_name,\n",
    "                    \"topology\": topo_name,\n",
    "                    \"Delta\": ds_Delta,\n",
    "                    \"K\": K,\n",
    "                    \"dataset\": ds,                         # aligned dataset (episodes/arrivals/tasks)\n",
    "                    \"topology_data\": topo_data,\n",
    "                    \"topology_meta_data\": meta_data,\n",
    "                    \"connection_matrix_df\": cm_df,\n",
    "                    \"environment\": environment,            # same aligned environment for all pairs\n",
    "                    \"checks\": {\n",
    "                        \"delta_match\": delta_ok,\n",
    "                        \"env_servers_match\": env_match_ok,\n",
    "                        \"message\": final_msg\n",
    "                    }\n",
    "                }\n",
    "\n",
    "    return pairs_by_topology\n",
    "\n",
    "def print_pairs_summary_topology_first_ep(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print summary of pairs in the form:\n",
    "\n",
    "        TOPOLOGY -> EPISODE -> SCENARIO\n",
    "    \"\"\"\n",
    "    print(\"=== TOPOLOGY × EPISODE × SCENARIO ===\")\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        print(f\"[TOPOLOGY] {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            scen_map = by_ep[ep_name]\n",
    "            if not scen_map:\n",
    "                print(f\"  ├─ Episode: {ep_name}  (no paired scenarios)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  ├─ Episode: {ep_name}\")\n",
    "            for scen_name in sorted(scen_map.keys()):\n",
    "                bundle = scen_map[scen_name]\n",
    "                checks = bundle[\"checks\"]\n",
    "                flag   = \"OK\" if (checks[\"delta_match\"] and checks[\"env_servers_match\"]) else \"FAIL\"\n",
    "                K      = bundle[\"K\"]\n",
    "                Delta  = bundle[\"Delta\"]\n",
    "                msg    = checks[\"message\"]\n",
    "                print(f\"  │    - [{flag}] {scen_name:9s} | K={K:2d}  Δ={Delta:g}  -> {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPOLOGY × EPISODE × SCENARIO ===\n",
      "[TOPOLOGY] clustered\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=20  Δ=1  -> OK\n",
      "[TOPOLOGY] fully_connected\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=20  Δ=1  -> OK\n",
      "[TOPOLOGY] skip_connections\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=20  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=20  Δ=1  -> OK\n",
      "\n",
      " ===EXAMPLE===\n",
      "     scenario  episode_id  task_id  mec_id  t_arrival_slot  t_arrival_time  \\\n",
      "0       light           0        0       0               0             0.0   \n",
      "1       light           0        1       0               0             0.0   \n",
      "2       light           0        2       1               0             0.0   \n",
      "3       light           0        3       3               0             0.0   \n",
      "4       light           0        4       3               0             0.0   \n",
      "...       ...         ...      ...     ...             ...             ...   \n",
      "2003    light           0     2003      15              99            99.0   \n",
      "2004    light           0     2004      15              99            99.0   \n",
      "2005    light           0     2005      15              99            99.0   \n",
      "2006    light           0     2006      17              99            99.0   \n",
      "2007    light           0     2007      17              99            99.0   \n",
      "\n",
      "      b_mb  rho_cyc_per_mb      c_cycles      mem_mb modality  has_deadline  \\\n",
      "0      4.0    5.525261e+08  2.210105e+09   62.412148    video             1   \n",
      "1      4.0    9.394101e+08  3.757640e+09   49.193645     text             0   \n",
      "2      3.0    7.919350e+08  2.375805e+09   39.529118    video             0   \n",
      "3      3.0    1.212347e+09  3.637042e+09   68.742000    video             0   \n",
      "4      4.0    1.291461e+09  5.165844e+09   66.445540     text             0   \n",
      "...    ...             ...           ...         ...      ...           ...   \n",
      "2003   3.0    1.132626e+09  3.397878e+09   87.784890     text             0   \n",
      "2004   3.0    9.731004e+08  2.919301e+09   48.441980   sensor             0   \n",
      "2005   3.0    1.588940e+09  4.766821e+09   56.099064    video             0   \n",
      "2006   5.0    1.246002e+09  6.230012e+09  103.991240    image             0   \n",
      "2007   4.0    8.927368e+08  3.570947e+09   62.005756    image             0   \n",
      "\n",
      "      deadline_s  deadline_time  non_atomic  split_ratio action_space_hint  \\\n",
      "0       1.339893       1.339893           1     0.424900        continuous   \n",
      "1      -1.000000      -1.000000           0     0.000000          discrete   \n",
      "2      -1.000000      -1.000000           0     0.000000          discrete   \n",
      "3      -1.000000      -1.000000           0     0.000000          discrete   \n",
      "4      -1.000000      -1.000000           0     0.000000          discrete   \n",
      "...          ...            ...         ...          ...               ...   \n",
      "2003   -1.000000      -1.000000           1     0.624287        continuous   \n",
      "2004   -1.000000      -1.000000           0     0.000000          discrete   \n",
      "2005   -1.000000      -1.000000           0     0.000000          discrete   \n",
      "2006   -1.000000      -1.000000           1     0.550480        continuous   \n",
      "2007   -1.000000      -1.000000           1     0.361228        continuous   \n",
      "\n",
      "      deadline_slots  \n",
      "0                  2  \n",
      "1                 -1  \n",
      "2                 -1  \n",
      "3                 -1  \n",
      "4                 -1  \n",
      "...              ...  \n",
      "2003              -1  \n",
      "2004              -1  \n",
      "2005              -1  \n",
      "2006              -1  \n",
      "2007              -1  \n",
      "\n",
      "[2008 rows x 18 columns]\n",
      "        mec_0  mec_1  mec_2  mec_3  mec_4  mec_5  mec_6  mec_7  mec_8  mec_9  \\\n",
      "mec_0     0.0    3.0    3.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
      "mec_1     3.0    0.0    3.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
      "mec_2     3.0    3.0    0.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
      "mec_3     3.0    3.0    3.0    0.0    3.0    3.0    3.0    0.0    0.0    0.0   \n",
      "mec_4     3.0    3.0    3.0    3.0    0.0    3.0    3.0    0.0    0.0    0.0   \n",
      "mec_5     3.0    3.0    3.0    3.0    3.0    0.0    3.0    0.0    0.0    0.0   \n",
      "mec_6     3.0    3.0    3.0    3.0    3.0    3.0    0.0    0.0    0.0    0.0   \n",
      "mec_7     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0   \n",
      "mec_8     0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    0.0    3.0   \n",
      "mec_9     0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0    0.0   \n",
      "mec_10    0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0    3.0   \n",
      "mec_11    0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0    3.0   \n",
      "mec_12    0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0    3.0   \n",
      "mec_13    0.0    0.0    0.0    0.0    0.0    0.0    0.0    3.0    3.0    3.0   \n",
      "mec_14    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "mec_15    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "mec_16    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "mec_17    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "mec_18    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "mec_19    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "        ...  mec_11  mec_12  mec_13  mec_14  mec_15  mec_16  mec_17  mec_18  \\\n",
      "mec_0   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_1   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_2   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_3   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_4   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_5   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_6   ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_7   ...     3.0     3.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_8   ...     3.0     3.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_9   ...     3.0     3.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_10  ...     3.0     3.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_11  ...     0.0     3.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_12  ...     3.0     0.0     3.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_13  ...     3.0     3.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mec_14  ...     0.0     0.0     0.0     0.0     3.0     3.0     3.0     3.0   \n",
      "mec_15  ...     0.0     0.0     0.0     3.0     0.0     3.0     3.0     3.0   \n",
      "mec_16  ...     0.0     0.0     0.0     3.0     3.0     0.0     3.0     3.0   \n",
      "mec_17  ...     0.0     0.0     0.0     3.0     3.0     3.0     0.0     3.0   \n",
      "mec_18  ...     0.0     0.0     0.0     3.0     3.0     3.0     3.0     0.0   \n",
      "mec_19  ...     0.0     0.0     0.0     3.0     3.0     3.0     3.0     3.0   \n",
      "\n",
      "        mec_19  cloud  \n",
      "mec_0      0.0    5.0  \n",
      "mec_1      0.0    5.0  \n",
      "mec_2      0.0    5.0  \n",
      "mec_3      0.0    5.0  \n",
      "mec_4      0.0    5.0  \n",
      "mec_5      0.0    5.0  \n",
      "mec_6      0.0    5.0  \n",
      "mec_7      0.0    5.0  \n",
      "mec_8      0.0    5.0  \n",
      "mec_9      0.0    5.0  \n",
      "mec_10     0.0    5.0  \n",
      "mec_11     0.0    5.0  \n",
      "mec_12     0.0    5.0  \n",
      "mec_13     0.0    5.0  \n",
      "mec_14     3.0    5.0  \n",
      "mec_15     3.0    5.0  \n",
      "mec_16     3.0    5.0  \n",
      "mec_17     3.0    5.0  \n",
      "mec_18     3.0    5.0  \n",
      "mec_19     0.0    5.0  \n",
      "\n",
      "[20 rows x 21 columns]\n",
      "{'servers_df':     Server ID  Private CPU Capacity  Public CPU Capacity  \\\n",
      "0           0                   5.0                  5.0   \n",
      "1           1                   5.0                  5.0   \n",
      "2           2                   5.0                  5.0   \n",
      "3           3                   5.0                  5.0   \n",
      "4           4                   5.0                  5.0   \n",
      "5           5                   5.0                  5.0   \n",
      "6           6                   5.0                  5.0   \n",
      "7           7                   5.0                  5.0   \n",
      "8           8                   5.0                  5.0   \n",
      "9           9                   5.0                  5.0   \n",
      "10         10                   5.0                  5.0   \n",
      "11         11                   5.0                  5.0   \n",
      "12         12                   5.0                  5.0   \n",
      "13         13                   5.0                  5.0   \n",
      "14         14                   5.0                  5.0   \n",
      "15         15                   5.0                  5.0   \n",
      "16         16                   5.0                  5.0   \n",
      "17         17                   5.0                  5.0   \n",
      "18         18                   5.0                  5.0   \n",
      "19         19                   5.0                  5.0   \n",
      "\n",
      "    Private CPU per_slot  Public CPU per_slot  \n",
      "0                    5.0                  5.0  \n",
      "1                    5.0                  5.0  \n",
      "2                    5.0                  5.0  \n",
      "3                    5.0                  5.0  \n",
      "4                    5.0                  5.0  \n",
      "5                    5.0                  5.0  \n",
      "6                    5.0                  5.0  \n",
      "7                    5.0                  5.0  \n",
      "8                    5.0                  5.0  \n",
      "9                    5.0                  5.0  \n",
      "10                   5.0                  5.0  \n",
      "11                   5.0                  5.0  \n",
      "12                   5.0                  5.0  \n",
      "13                   5.0                  5.0  \n",
      "14                   5.0                  5.0  \n",
      "15                   5.0                  5.0  \n",
      "16                   5.0                  5.0  \n",
      "17                   5.0                  5.0  \n",
      "18                   5.0                  5.0  \n",
      "19                   5.0                  5.0  , 'cloud_df':    id  computational_capacity  capacity_per_slot\n",
      "0   0                    30.0               30.0, 'num_servers': 20, 'num_clouds': 1, 'private_cpu': array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "       5., 5., 5.]), 'public_cpu': array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "       5., 5., 5.]), 'cloud_capacity': array([30.]), 'private_cpu_slot': array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "       5., 5., 5.]), 'public_cpu_slot': array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "       5., 5., 5.]), 'cloud_capacity_slot': array([30.])}\n"
     ]
    }
   ],
   "source": [
    "# --- Example driver (using current variables) ---\n",
    "result_align = align_all_units(\n",
    "    datasets_ep_first=datasets,\n",
    "    topologies_by_name=topologies,\n",
    "    environment=environment\n",
    ")\n",
    "\n",
    "datasets_aligned    = result_align[\"datasets_aligned\"]\n",
    "environment_aligned = result_align[\"environment_aligned\"]\n",
    "\n",
    "pairs_by_topology = build_topology_episode_pairs(\n",
    "    datasets_ep_first=datasets_aligned,\n",
    "    topologies=topologies,\n",
    "    environment=environment_aligned,\n",
    "    strict_delta_match=True,\n",
    "    strict_env_match=True\n",
    ")\n",
    "\n",
    "print_pairs_summary_topology_first_ep(pairs_by_topology)\n",
    "\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "# Example access:\n",
    "#   - tasks for light scenario under fully_connected topology and ep_000\n",
    "tasks_light = pairs_by_topology[\"fully_connected\"][\"ep_000\"][\"light\"][\"dataset\"][\"tasks\"]\n",
    "print(tasks_light)\n",
    "\n",
    "#   - connection matrix for clustered topology and heavy scenario, ep_000\n",
    "cm_clustered = pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"connection_matrix_df\"]\n",
    "print(cm_clustered)\n",
    "\n",
    "#   - aligned environment for the same pair\n",
    "env_for_pair = pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"environment\"]\n",
    "print(env_for_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.5. Environment Configuration </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we build a unified env_config for each scenario–topology pair.\n",
    "It bundles all required information for the MDP/RL environment—such as compute capacities, the Agent→MEC mapping, connection matrix, initial queue states, and action/state specifications—into a single consistent configuration used by the RL training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_core_from_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract core fields from a (topology × episode × scenario) bundle.\n",
    "    Ensures required fields exist and converts structures to numpy/DF formats.\n",
    "    This version is MEC-based (no agents / agent_to_mec).\n",
    "    \"\"\"\n",
    "    required = [\"dataset\", \"topology_data\", \"connection_matrix_df\", \"Delta\", \"K\"]\n",
    "    for k in required:\n",
    "        if k not in bundle:\n",
    "            raise ValueError(f\"Bundle missing required key: '{k}'\")\n",
    "\n",
    "    ds   = bundle[\"dataset\"]\n",
    "    topo = bundle[\"topology_data\"]\n",
    "    Mdf  = bundle[\"connection_matrix_df\"]\n",
    "\n",
    "    # Dataset tables (already aligned in previous stage)\n",
    "    if not {\"episodes\", \"arrivals\", \"tasks\"}.issubset(ds.keys()):\n",
    "        raise ValueError(\"dataset in bundle must contain 'episodes', 'arrivals', 'tasks'.\")\n",
    "\n",
    "    episodes = ds[\"episodes\"]\n",
    "    arrivals = ds[\"arrivals\"]\n",
    "    tasks    = ds[\"tasks\"]\n",
    "\n",
    "    # Capacities from topology (validated earlier against environment)\n",
    "    private_cpu = np.asarray(topo[\"private_cpu_capacities\"], dtype=float)  # shape (K,)\n",
    "    public_cpu  = np.asarray(topo[\"public_cpu_capacities\"],  dtype=float)  # shape (K,)\n",
    "    cloud_cpu   = float(topo[\"cloud_computational_capacity\"])             # scalar\n",
    "\n",
    "    # Connection matrix: shape (K, K+1), last column = MEC→Cloud\n",
    "    M = Mdf.to_numpy(dtype=float)\n",
    "\n",
    "    return dict(\n",
    "        Delta=float(bundle[\"Delta\"]),\n",
    "        K=int(bundle[\"K\"]),\n",
    "        episodes=episodes,\n",
    "        arrivals=arrivals,\n",
    "        tasks=tasks,\n",
    "        private_cpu=private_cpu,\n",
    "        public_cpu=public_cpu,\n",
    "        cloud_cpu=cloud_cpu,\n",
    "        connection_matrix=M,\n",
    "        topology_type=topo.get(\"topology_type\", \"unknown\"),\n",
    "    )\n",
    "\n",
    "def _build_default_queues(K: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initial queue states for MEC and Cloud tiers, in per-slot units:\n",
    "      - *_cycles store queued CPU cycles.\n",
    "      - mec_bytes_in_transit stores bytes currently being transmitted through MEC links.\n",
    "      - cloud_cycles stores queued cycles at the cloud.\n",
    "    All queues start empty (=0).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mec_local_cycles\":      np.zeros(K, dtype=float),\n",
    "        \"mec_public_cycles\":     np.zeros(K, dtype=float),\n",
    "        \"mec_bytes_in_transit\":  np.zeros(K, dtype=float),\n",
    "        \"cloud_cycles\":          np.array([0.0], dtype=float),\n",
    "    }\n",
    "\n",
    "def _derive_action_space() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Basic discrete offloading action space (HOODIE-style):\n",
    "        0 = Execute locally on the MEC where the task arrived\n",
    "        1 = Offload to another MEC server\n",
    "        2 = Offload to Cloud\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"type\": \"discrete\",\n",
    "        \"n\": 3,\n",
    "        \"labels\": {\n",
    "            0: \"LOCAL\",\n",
    "            1: \"MEC\",\n",
    "            2: \"CLOUD\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def _derive_state_spec(K: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Declarative specification of the RL state structure.\n",
    "    The environment uses this to assemble numerical tensors each step.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"components\": {\n",
    "            \"queues\": {\n",
    "                \"mec_local_cycles\":     {\"shape\": (K,),      \"dtype\": \"float\"},\n",
    "                \"mec_public_cycles\":    {\"shape\": (K,),      \"dtype\": \"float\"},\n",
    "                \"mec_bytes_in_transit\": {\"shape\": (K,),      \"dtype\": \"float\"},\n",
    "                \"cloud_cycles\":         {\"shape\": (1,),      \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"links\": {\n",
    "                \"connection_matrix\":    {\"shape\": (K, K + 1), \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"capacities\": {\n",
    "                \"private_cpu\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"public_cpu\":  {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"cloud_cpu\":   {\"shape\": (1,), \"dtype\": \"float\"},\n",
    "            },\n",
    "        },\n",
    "        \"note\": \"Declarative state description; environment assembles numerical tensors at runtime.\",\n",
    "    }\n",
    "\n",
    "def build_env_config_for_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build the complete environment configuration structure for a single\n",
    "    (topology × episode × scenario) bundle.\n",
    "\n",
    "    MEC-based version (no agents / no agent_to_mec):\n",
    "\n",
    "    env_config includes:\n",
    "        - Time parameters: Delta, T_slots\n",
    "        - Topology specification: K, connection_matrix, topology_type\n",
    "        - Resource capacities: private/public/cloud CPU (per MEC / cloud)\n",
    "        - Aligned dataset tables (episodes, arrivals, tasks)\n",
    "        - Initial queue states\n",
    "        - Action space and state specification\n",
    "        - Basic consistency info from previous checks (bundle[\"checks\"])\n",
    "    \"\"\"\n",
    "    core = _extract_core_from_bundle(bundle)\n",
    "\n",
    "    episodes = core[\"episodes\"]\n",
    "\n",
    "    # Simulation horizon\n",
    "    if \"T_slots\" not in episodes.columns:\n",
    "        raise ValueError(\"episodes.csv must contain 'T_slots'.\")\n",
    "    T_slots = int(episodes[\"T_slots\"].iloc[0])\n",
    "\n",
    "    # Number of MECs (from episodes.csv) should match topology K\n",
    "    N_mecs = None\n",
    "    if \"N_mecs\" in episodes.columns:\n",
    "        N_mecs = int(episodes[\"N_mecs\"].iloc[0])\n",
    "        if N_mecs != core[\"K\"]:\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between episodes.N_mecs ({N_mecs}) and topology K ({core['K']}).\"\n",
    "            )\n",
    "    else:\n",
    "        # If N_mecs column is missing, fall back to K\n",
    "        N_mecs = core[\"K\"]\n",
    "\n",
    "    # Build initial queues and specs\n",
    "    queues_initial = _build_default_queues(core[\"K\"])\n",
    "    action_space   = _derive_action_space()\n",
    "    state_spec     = _derive_state_spec(core[\"K\"])\n",
    "\n",
    "    # Final environment configuration object\n",
    "    env_config = {\n",
    "        # Time / horizon\n",
    "        \"Delta\":   core[\"Delta\"],\n",
    "        \"T_slots\": T_slots,\n",
    "\n",
    "        # Topology\n",
    "        \"K\":              core[\"K\"],\n",
    "        \"N_mecs\":         N_mecs,\n",
    "        \"topology_type\":  core[\"topology_type\"],\n",
    "        \"connection_matrix\": core[\"connection_matrix\"],\n",
    "\n",
    "        # Capacities (per-slot units, as defined in topology/environment)\n",
    "        \"private_cpu\": core[\"private_cpu\"],   # shape (K,)\n",
    "        \"public_cpu\":  core[\"public_cpu\"],    # shape (K,)\n",
    "        \"cloud_cpu\":   core[\"cloud_cpu\"],     # scalar\n",
    "\n",
    "        # Datasets (aligned, MEC-based)\n",
    "        \"episodes\": core[\"episodes\"],\n",
    "        \"arrivals\": core[\"arrivals\"],\n",
    "        \"tasks\":    core[\"tasks\"],\n",
    "\n",
    "        # Initial queue states and specifications\n",
    "        \"queues_initial\": queues_initial,\n",
    "        \"action_space\":   action_space,\n",
    "        \"state_spec\":     state_spec,\n",
    "\n",
    "        # Validation / consistency info propagated from previous stage\n",
    "        \"checks\": bundle.get(\"checks\", {\"delta_match\": True, \"env_servers_match\": True, \"message\": \"n/a\"}),\n",
    "    }\n",
    "\n",
    "    return env_config\n",
    "\n",
    "def build_all_env_configs(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Build environment configurations for all (topology × episode × scenario) bundles.\n",
    "\n",
    "    Result shape (episode-first):\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]] = {}\n",
    "\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            if ep_name not in out:\n",
    "                out[ep_name] = {}\n",
    "            if topo_name not in out[ep_name]:\n",
    "                out[ep_name][topo_name] = {}\n",
    "\n",
    "            for scen_name, bundle in by_scen.items():\n",
    "                # No more 'agent_to_mec' required here (MEC-based).\n",
    "                env_cfg = build_env_config_for_bundle(bundle)\n",
    "                out[ep_name][topo_name][scen_name] = env_cfg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE ENV CONFIG ===\n",
      "Delta: 1.0\n",
      "T_slots: 110\n",
      "K (number of MECs): 20\n",
      "queues_initial keys: ['mec_local_cycles', 'mec_public_cycles', 'mec_bytes_in_transit', 'cloud_cycles']\n",
      "tasks shape: (7929, 18)\n"
     ]
    }
   ],
   "source": [
    "env_configs = build_all_env_configs(pairs_by_topology)\n",
    "\n",
    "print(\"\\n=== EXAMPLE ENV CONFIG ===\")\n",
    "example_cfg = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"]\n",
    "print(\"Delta:\", example_cfg[\"Delta\"])\n",
    "print(\"T_slots:\", example_cfg[\"T_slots\"])\n",
    "print(\"K (number of MECs):\", example_cfg[\"K\"])\n",
    "print(\"queues_initial keys:\", list(example_cfg[\"queues_initial\"].keys()))\n",
    "print(\"tasks shape:\", example_cfg[\"tasks\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.6. Sanity Checks </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we verify that each env_config is internally consistent (queue shapes, capacities, agent→MEC mapping, and connection matrix are valid and ready for simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_env_config(env_config: Dict[str, Any]) -> list:\n",
    "    \"\"\"\n",
    "    Run basic sanity checks on a single env_config dictionary (MEC-based).\n",
    "    Returns a list of error strings; empty list means 'no issues found'.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    # ---- 0) Basic required keys ----\n",
    "    required_keys = [\n",
    "        \"K\",\n",
    "        \"Delta\",\n",
    "        \"T_slots\",\n",
    "        \"connection_matrix\",\n",
    "        \"private_cpu\",\n",
    "        \"public_cpu\",\n",
    "        \"cloud_cpu\",\n",
    "        \"queues_initial\",\n",
    "        \"action_space\",\n",
    "        \"episodes\",\n",
    "        \"arrivals\",\n",
    "        \"tasks\",\n",
    "    ]\n",
    "    for k in required_keys:\n",
    "        if k not in env_config:\n",
    "            errors.append(f\"Missing required key in env_config: '{k}'\")\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K       = int(env_config[\"K\"])\n",
    "    Delta   = float(env_config[\"Delta\"])\n",
    "    T_slots = int(env_config[\"T_slots\"])\n",
    "\n",
    "    episodes = env_config[\"episodes\"]\n",
    "    arrivals = env_config[\"arrivals\"]\n",
    "    tasks    = env_config[\"tasks\"]\n",
    "\n",
    "    # ---- 1) MEC count alignment (K vs N_mecs / episodes) ----\n",
    "    N_mecs_cfg = env_config.get(\"N_mecs\", None)\n",
    "    if N_mecs_cfg is not None:\n",
    "        if int(N_mecs_cfg) != K:\n",
    "            errors.append(f\"N_mecs ({N_mecs_cfg}) != K ({K}) in env_config.\")\n",
    "\n",
    "    if isinstance(episodes, pd.DataFrame) and \"N_mecs\" in episodes.columns and len(episodes):\n",
    "        N_mecs_ep = int(episodes[\"N_mecs\"].iloc[0])\n",
    "        if N_mecs_ep != K:\n",
    "            errors.append(f\"episodes.N_mecs ({N_mecs_ep}) != K ({K}).\")\n",
    "\n",
    "    # ---- 2) MEC IDs in tasks / arrivals ----\n",
    "    if isinstance(tasks, pd.DataFrame):\n",
    "        if \"mec_id\" not in tasks.columns:\n",
    "            errors.append(\"tasks table is missing 'mec_id' column.\")\n",
    "        else:\n",
    "            mec_ids_tasks = tasks[\"mec_id\"].to_numpy()\n",
    "            if len(mec_ids_tasks):\n",
    "                if (mec_ids_tasks < 0).any() or (mec_ids_tasks >= K).any():\n",
    "                    errors.append(\"tasks.mec_id contains values outside [0, K-1].\")\n",
    "    else:\n",
    "        errors.append(\"tasks is not a DataFrame.\")\n",
    "\n",
    "    if isinstance(arrivals, pd.DataFrame):\n",
    "        if \"mec_id\" not in arrivals.columns:\n",
    "            errors.append(\"arrivals table is missing 'mec_id' column.\")\n",
    "        else:\n",
    "            mec_ids_arr = arrivals[\"mec_id\"].to_numpy()\n",
    "            if len(mec_ids_arr):\n",
    "                if (mec_ids_arr < 0).any() or (mec_ids_arr >= K).any():\n",
    "                    errors.append(\"arrivals.mec_id contains values outside [0, K-1].\")\n",
    "    else:\n",
    "        errors.append(\"arrivals is not a DataFrame.\")\n",
    "\n",
    "    # ---- 3) Queue initial state shapes ----\n",
    "    q = env_config[\"queues_initial\"]\n",
    "    if q[\"mec_local_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_local_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_public_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_public_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_bytes_in_transit\"].shape != (K,):\n",
    "        errors.append(\"mec_bytes_in_transit queue shape mismatch.\")\n",
    "    if q[\"cloud_cycles\"].shape != (1,):\n",
    "        errors.append(\"cloud_cycles shape mismatch (should be (1,)).\")\n",
    "\n",
    "    # ---- 4) Non-negative compute capacities ----\n",
    "    private_cpu = np.asarray(env_config[\"private_cpu\"], dtype=float)\n",
    "    public_cpu  = np.asarray(env_config[\"public_cpu\"],  dtype=float)\n",
    "    cloud_cpu   = float(env_config[\"cloud_cpu\"])\n",
    "\n",
    "    if (private_cpu < 0).any():\n",
    "        errors.append(\"private_cpu has negative values.\")\n",
    "    if (public_cpu < 0).any():\n",
    "        errors.append(\"public_cpu has negative values.\")\n",
    "    if cloud_cpu < 0:\n",
    "        errors.append(\"cloud_cpu is negative.\")\n",
    "\n",
    "    if private_cpu.shape != (K,):\n",
    "        errors.append(f\"private_cpu shape mismatch, expected ({K},).\")\n",
    "    if public_cpu.shape != (K,):\n",
    "        errors.append(f\"public_cpu shape mismatch, expected ({K},).\")\n",
    "\n",
    "    # ---- 5) Connection matrix dimension (K x K+1) ----\n",
    "    M = np.asarray(env_config[\"connection_matrix\"], dtype=float)\n",
    "    if M.shape != (K, K + 1):\n",
    "        errors.append(\"connection_matrix shape mismatch (expected K x (K+1)).\")\n",
    "    if (M < 0).any():\n",
    "        errors.append(\"connection_matrix contains negative values.\")\n",
    "\n",
    "    # ---- 6) Action space correctness ----\n",
    "    action_space = env_config.get(\"action_space\", {})\n",
    "    if action_space.get(\"type\", None) != \"discrete\":\n",
    "        errors.append(\"Action space must be discrete (LOCAL/MEC/CLOUD).\")\n",
    "    if action_space.get(\"n\", None) != 3:\n",
    "        errors.append(\"Action space 'n' must be 3 (LOCAL/MEC/CLOUD).\")\n",
    "\n",
    "    # ---- 7) Basic time parameters + consistency with episodes ----\n",
    "    if not np.isfinite(Delta) or Delta <= 0:\n",
    "        errors.append(f\"Invalid Delta in env_config (got {Delta}).\")\n",
    "    if T_slots <= 0:\n",
    "        errors.append(f\"Invalid T_slots in env_config (got {T_slots}).\")\n",
    "\n",
    "    if isinstance(episodes, pd.DataFrame) and len(episodes):\n",
    "        if \"Delta\" in episodes.columns:\n",
    "            Delta_ep = float(episodes[\"Delta\"].iloc[0])\n",
    "            if not np.isclose(Delta_ep, Delta, atol=1e-9):\n",
    "                errors.append(f\"episodes.Delta ({Delta_ep}) != env_config.Delta ({Delta}).\")\n",
    "        if \"T_slots\" in episodes.columns:\n",
    "            T_slots_ep = int(episodes[\"T_slots\"].iloc[0])\n",
    "            if T_slots_ep != T_slots:\n",
    "                errors.append(f\"episodes.T_slots ({T_slots_ep}) != env_config.T_slots ({T_slots}).\")\n",
    "\n",
    "    # ---- 8) Arrival slot range sanity ----\n",
    "    if isinstance(tasks, pd.DataFrame) and \"t_arrival_slot\" in tasks.columns and len(tasks):\n",
    "        max_slot = int(tasks[\"t_arrival_slot\"].max())\n",
    "        if max_slot >= T_slots:\n",
    "            errors.append(\n",
    "                f\"tasks.t_arrival_slot has values >= T_slots (max={max_slot}, T_slots={T_slots}).\"\n",
    "            )\n",
    "\n",
    "    return errors\n",
    "\n",
    "def sanity_check_all(env_configs: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Run sanity_check_env_config over all env_config instances.\n",
    "\n",
    "    env_configs shape (episode-first):\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "    \"\"\"\n",
    "    print(\"=== SANITY CHECK OVER ALL ENV CONFIGS ===\")\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                errs = sanity_check_env_config(env_cfg)\n",
    "                if errs:\n",
    "                    print(f\"[FAIL] {ep_name}/{topo_name}/{scen_name}:\")\n",
    "                    for e in errs:\n",
    "                        print(\"   -\", e)\n",
    "                else:\n",
    "                    print(f\"[OK]   {ep_name}/{topo_name}/{scen_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANITY CHECK OVER ALL ENV CONFIGS ===\n",
      "[OK]   ep_000/clustered/heavy\n",
      "[OK]   ep_000/clustered/light\n",
      "[OK]   ep_000/clustered/moderate\n",
      "[OK]   ep_000/fully_connected/heavy\n",
      "[OK]   ep_000/fully_connected/light\n",
      "[OK]   ep_000/fully_connected/moderate\n",
      "[OK]   ep_000/skip_connections/heavy\n",
      "[OK]   ep_000/skip_connections/light\n",
      "[OK]   ep_000/skip_connections/moderate\n",
      "\n",
      "=== EXAMPLE TASK TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "      <th>deadline_slots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.588426e+09</td>\n",
       "      <td>7.942128e+09</td>\n",
       "      <td>76.217380</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.578404</td>\n",
       "      <td>0.578404</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408449</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.486160e+09</td>\n",
       "      <td>5.944639e+09</td>\n",
       "      <td>96.590225</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.441022e+09</td>\n",
       "      <td>4.882044e+09</td>\n",
       "      <td>95.325920</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.564350e+09</td>\n",
       "      <td>1.282175e+10</td>\n",
       "      <td>75.990030</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495647</td>\n",
       "      <td>0.495647</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800898</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.821414e+09</td>\n",
       "      <td>7.285658e+09</td>\n",
       "      <td>74.964060</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516735</td>\n",
       "      <td>continuous</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>7924</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.506356e+09</td>\n",
       "      <td>5.012711e+09</td>\n",
       "      <td>56.085820</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818721</td>\n",
       "      <td>99.818718</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>7925</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.338801e+09</td>\n",
       "      <td>6.694007e+09</td>\n",
       "      <td>100.389120</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475141</td>\n",
       "      <td>99.475143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574161</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>7926</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.519705e+09</td>\n",
       "      <td>5.039410e+09</td>\n",
       "      <td>44.373272</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.756305</td>\n",
       "      <td>99.756302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>7927</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.048822e+09</td>\n",
       "      <td>2.097645e+09</td>\n",
       "      <td>96.209526</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>7928</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.354514e+09</td>\n",
       "      <td>6.772569e+09</td>\n",
       "      <td>78.189970</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774583</td>\n",
       "      <td>99.774582</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807006</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7929 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     scenario  episode_id  task_id  mec_id  t_arrival_slot  t_arrival_time  \\\n",
       "0       heavy           0        0       0               0             0.0   \n",
       "1       heavy           0        1       0               0             0.0   \n",
       "2       heavy           0        2       0               0             0.0   \n",
       "3       heavy           0        3       0               0             0.0   \n",
       "4       heavy           0        4       0               0             0.0   \n",
       "...       ...         ...      ...     ...             ...             ...   \n",
       "7924    heavy           0     7924      19              99            99.0   \n",
       "7925    heavy           0     7925      19              99            99.0   \n",
       "7926    heavy           0     7926      19              99            99.0   \n",
       "7927    heavy           0     7927      19              99            99.0   \n",
       "7928    heavy           0     7928      19              99            99.0   \n",
       "\n",
       "      b_mb  rho_cyc_per_mb      c_cycles      mem_mb modality  has_deadline  \\\n",
       "0      5.0    1.588426e+09  7.942128e+09   76.217380     text             1   \n",
       "1      4.0    1.486160e+09  5.944639e+09   96.590225     text             0   \n",
       "2      2.0    2.441022e+09  4.882044e+09   95.325920   sensor             0   \n",
       "3      5.0    2.564350e+09  1.282175e+10   75.990030    image             1   \n",
       "4      4.0    1.821414e+09  7.285658e+09   74.964060     text             0   \n",
       "...    ...             ...           ...         ...      ...           ...   \n",
       "7924   2.0    2.506356e+09  5.012711e+09   56.085820   sensor             1   \n",
       "7925   5.0    1.338801e+09  6.694007e+09  100.389120     text             1   \n",
       "7926   2.0    2.519705e+09  5.039410e+09   44.373272     text             1   \n",
       "7927   2.0    1.048822e+09  2.097645e+09   96.209526   sensor             0   \n",
       "7928   5.0    1.354514e+09  6.772569e+09   78.189970   sensor             1   \n",
       "\n",
       "      deadline_s  deadline_time  non_atomic  split_ratio action_space_hint  \\\n",
       "0       0.578404       0.578404           1     0.408449        continuous   \n",
       "1      -1.000000      -1.000000           0     0.000000          discrete   \n",
       "2      -1.000000      -1.000000           0     0.000000          discrete   \n",
       "3       0.495647       0.495647           1     0.800898        continuous   \n",
       "4      -1.000000      -1.000000           1     0.516735        continuous   \n",
       "...          ...            ...         ...          ...               ...   \n",
       "7924    0.818721      99.818718           0     0.000000          discrete   \n",
       "7925    0.475141      99.475143           1     0.574161        continuous   \n",
       "7926    0.756305      99.756302           0     0.000000          discrete   \n",
       "7927   -1.000000      -1.000000           0     0.000000          discrete   \n",
       "7928    0.774583      99.774582           1     0.807006        continuous   \n",
       "\n",
       "      deadline_slots  \n",
       "0                  1  \n",
       "1                 -1  \n",
       "2                 -1  \n",
       "3                  1  \n",
       "4                 -1  \n",
       "...              ...  \n",
       "7924               1  \n",
       "7925               1  \n",
       "7926               1  \n",
       "7927              -1  \n",
       "7928               1  \n",
       "\n",
       "[7929 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run all sanity checks\n",
    "sanity_check_all(env_configs)\n",
    "\n",
    "print(\"\\n=== EXAMPLE TASK TABLE ===\")\n",
    "display(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] env_configs summary → ./artifacts/env_configs_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def _summarize_array(arr, max_items=6):\n",
    "    \"\"\"Return a short, readable summary string for numpy arrays.\"\"\"\n",
    "    try:\n",
    "        arr = np.asarray(arr)\n",
    "        base = f\"ndarray shape={arr.shape}, dtype={arr.dtype}\"\n",
    "        if arr.size == 0:\n",
    "            return base + \" | empty\"\n",
    "\n",
    "        # If small 1D vector, show full values\n",
    "        if arr.ndim == 1 and arr.size <= max_items:\n",
    "            return base + f\" | values={arr.tolist()}\"\n",
    "\n",
    "        # If numeric, show basic stats\n",
    "        if np.issubdtype(arr.dtype, np.number):\n",
    "            return (\n",
    "                base +\n",
    "                f\" | min={np.nanmin(arr):.4g}, max={np.nanmax(arr):.4g}, mean={np.nanmean(arr):.4g}\"\n",
    "            )\n",
    "\n",
    "        return base\n",
    "    except Exception as e:\n",
    "        return f\"(array summary failed: {e})\"\n",
    "\n",
    "def _summarize_df(df: pd.DataFrame, max_cols=10):\n",
    "    \"\"\"Return a short summary string for DataFrames.\"\"\"\n",
    "    try:\n",
    "        cols = df.columns.tolist()\n",
    "        cols_show = cols[:max_cols] + ([\"...\"] if len(cols) > max_cols else [])\n",
    "        return f\"DataFrame shape={df.shape}, columns={cols_show}\"\n",
    "    except Exception as e:\n",
    "        return f\"(dataframe summary failed: {e})\"\n",
    "\n",
    "def _summarize_any(name, obj, indent=\"    \"):\n",
    "    \"\"\"\n",
    "    Produce a few readable summary lines depending on the object type.\n",
    "    Used recursively for nested dicts (e.g., queues_initial).\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        lines.append(f\"{indent}{name}: {_summarize_df(obj)}\")\n",
    "\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        lines.append(f\"{indent}{name}: {_summarize_array(obj)}\")\n",
    "\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        preview = obj[:6] if len(obj) > 6 else obj\n",
    "        lines.append(f\"{indent}{name}: list len={len(obj)}, preview={preview}\")\n",
    "\n",
    "    elif isinstance(obj, dict):\n",
    "        lines.append(f\"{indent}{name}: dict keys={list(obj.keys())}\")\n",
    "\n",
    "        # Dive deeper for small dicts or queue dictionaries\n",
    "        if name == \"queues_initial\" or len(obj) <= 6:\n",
    "            for k, v in obj.items():\n",
    "                sub = _summarize_any(k, v, indent=indent + \"  \")\n",
    "                if isinstance(sub, list):\n",
    "                    lines.extend(sub)\n",
    "                else:\n",
    "                    lines.append(sub)\n",
    "\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        lines.append(f\"{indent}{name}: {repr(obj)}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback: try converting to array\n",
    "        try:\n",
    "            arr = np.asarray(obj)\n",
    "            lines.append(f\"{indent}{name}: {_summarize_array(arr)}\")\n",
    "        except Exception:\n",
    "            lines.append(f\"{indent}{name}: ({type(obj).__name__})\")\n",
    "\n",
    "    return lines\n",
    "\n",
    "def save_env_configs_text(env_configs: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]],\n",
    "                          out_path: str = \"./artifacts/env_configs_summary.txt\"):\n",
    "    \"\"\"\n",
    "    Save a human-readable summary of all env_configs:\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "\n",
    "    The summary includes:\n",
    "    - key scalar parameters (Delta, K, N_mecs, topology_type)\n",
    "    - shapes and stats of numeric arrays\n",
    "    - summary of DataFrames (episodes, arrivals, tasks)\n",
    "    - queue initialization\n",
    "    - RL descriptors (action_space, state_spec, checks)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    lines = []\n",
    "    lines.append(\"=== ENV CONFIGS SUMMARY (episode → topology → scenario) ===\\n\")\n",
    "\n",
    "    # deterministic ordering for reproducible summaries\n",
    "    for ep_name in sorted(env_configs.keys()):\n",
    "        lines.append(f\"[EPISODE] {ep_name}\")\n",
    "        by_topo = env_configs[ep_name]\n",
    "\n",
    "        for topo_name in sorted(by_topo.keys()):\n",
    "            lines.append(f\"  [TOPOLOGY] {topo_name}\")\n",
    "            by_scen = by_topo[topo_name]\n",
    "\n",
    "            for scen_name in sorted(by_scen.keys()):\n",
    "                env_cfg = by_scen[scen_name]\n",
    "                lines.append(f\"    [SCENARIO] {scen_name}\")\n",
    "\n",
    "                # -- important scalars --\n",
    "                for key in [\"Delta\", \"K\", \"N_mecs\", \"topology_type\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- main tensors/arrays --\n",
    "                for key in [\n",
    "                    \"connection_matrix\",\n",
    "                    \"private_cpu\",\n",
    "                    \"public_cpu\",\n",
    "                    \"cloud_cpu\",\n",
    "                ]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- dataframes (no agents in MEC-based design) --\n",
    "                for key in [\"episodes\", \"arrivals\", \"tasks\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- RL descriptors and queues --\n",
    "                for key in [\"queues_initial\", \"action_space\", \"state_spec\", \"checks\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                lines.append(\"\")  # blank line after scenario\n",
    "\n",
    "            lines.append(\"\")  # blank line after topology\n",
    "\n",
    "        lines.append(\"\")  # blank line after episode\n",
    "\n",
    "    # Write file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(f\"[saved] env_configs summary → {out_path}\")\n",
    "\n",
    "\n",
    "# --------- Usage ---------\n",
    "save_env_configs_text(env_configs, out_path=\"./artifacts/env_configs_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Step 1, we have loaded the data, aligned the units, assigned agents to MECs, and prepared the environment configuration. Finally, we have performed consistency checks to ensure the data is correct. Next, we can move on to task labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 2: Task Labeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Basic Task Labeling (buckets, urgency, atomicity, ...) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers: quantile-based cut points ----------\n",
    "def _quantile_cutpoints(s: pd.Series, q_low=0.33, q_high=0.66) -> Tuple[float, float]:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(s.quantile(q_low)), float(s.quantile(q_high)))\n",
    "\n",
    "def _bucketize(value: float, q1: float, q2: float) -> str:\n",
    "    \"\"\"\n",
    "    Map a scalar value into a bucket:\n",
    "      - 'S' = small\n",
    "      - 'M' = medium\n",
    "      - 'L' = large\n",
    "      - 'U' = unknown (if any input is non-finite)\n",
    "    \"\"\"\n",
    "    if not np.isfinite(value) or not np.isfinite(q1) or not np.isfinite(q2):\n",
    "        return \"U\"  # Unknown\n",
    "    if value <= q1:\n",
    "        return \"S\"\n",
    "    if value <= q2:\n",
    "        return \"M\"\n",
    "    return \"L\"\n",
    "\n",
    "# ---------- threshold builder (adaptive to each tasks DF) ----------\n",
    "def build_task_label_thresholds(\n",
    "    tasks_df: pd.DataFrame,\n",
    "    q_low: float = 0.33,\n",
    "    q_high: float = 0.66,\n",
    "    urgent_slots_cap: int = 2,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build adaptive thresholds from the data itself (per-episode/scenario),\n",
    "    so 'light/moderate/heavy' are handled robustly.\n",
    "    \"\"\"\n",
    "    q_b_mb  = _quantile_cutpoints(tasks_df[\"b_mb\"], q_low, q_high) if \"b_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_rho   = _quantile_cutpoints(tasks_df[\"rho_cyc_per_mb\"], q_low, q_high) if \"rho_cyc_per_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_mem   = _quantile_cutpoints(tasks_df[\"mem_mb\"], q_low, q_high) if \"mem_mb\" in tasks_df else (np.nan, np.nan)\n",
    "\n",
    "    if \"split_ratio\" in tasks_df.columns:\n",
    "        mask_split = tasks_df.get(\"non_atomic\", 0) == 1\n",
    "        q_split = _quantile_cutpoints(tasks_df.loc[mask_split, \"split_ratio\"], q_low, q_high)\n",
    "    else:\n",
    "        q_split = (np.nan, np.nan)\n",
    "\n",
    "    return {\n",
    "        \"b_mb\":   {\"q1\": q_b_mb[0],  \"q2\": q_b_mb[1]},\n",
    "        \"rho\":    {\"q1\": q_rho[0],   \"q2\": q_rho[1]},\n",
    "        \"mem\":    {\"q1\": q_mem[0],   \"q2\": q_mem[1]},\n",
    "        \"split\":  {\"q1\": q_split[0], \"q2\": q_split[1]},\n",
    "        # If deadline_slots ≤ urgent_slots_cap → 'hard' (latency sensitive)\n",
    "        \"urgent_slots_cap\": int(urgent_slots_cap),\n",
    "    }\n",
    "\n",
    "# ---------- main labeling for a single tasks DF ----------\n",
    "def label_tasks_df(\n",
    "    tasks_df: pd.DataFrame,\n",
    "    Delta: float,\n",
    "    thresholds: Dict[str, Any]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add label columns to tasks_df (returns a COPY).\n",
    "\n",
    "    Columns added:\n",
    "      - size_bucket, compute_bucket, mem_bucket\n",
    "      - (if missing) deadline_slots, and then urgency: none/soft/hard\n",
    "      - atomicity, split_bucket\n",
    "      - latency_sensitive, compute_heavy, io_heavy, memory_heavy (bools)\n",
    "      - routing_hint (LOCAL/MEC/CLOUD) – only for EDA / debugging\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    # --- ensure numeric types for main features\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\", \"deadline_s\", \"split_ratio\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # --- deadline_slots (only if not precomputed earlier)\n",
    "    # In our current pipeline, Units Alignment already created `deadline_slots`\n",
    "    # and used -1 as sentinel for \"no deadline\".\n",
    "    if \"deadline_slots\" not in df.columns:\n",
    "        if \"has_deadline\" in df.columns and \"deadline_s\" in df.columns:\n",
    "            df[\"deadline_slots\"] = np.where(\n",
    "                (df[\"has_deadline\"] == 1) & np.isfinite(df[\"deadline_s\"]),\n",
    "                np.ceil(df[\"deadline_s\"] / float(Delta)).astype(\"float\"),\n",
    "                np.nan\n",
    "            )\n",
    "        else:\n",
    "            df[\"deadline_slots\"] = np.nan\n",
    "\n",
    "    # --- bucketize size/compute/memory\n",
    "    b_q1, b_q2       = thresholds[\"b_mb\"][\"q1\"],  thresholds[\"b_mb\"][\"q2\"]\n",
    "    rho_q1, rho_q2   = thresholds[\"rho\"][\"q1\"],   thresholds[\"rho\"][\"q2\"]\n",
    "    mem_q1, mem_q2   = thresholds[\"mem\"][\"q1\"],   thresholds[\"mem\"][\"q2\"]\n",
    "\n",
    "    df[\"size_bucket\"] = (\n",
    "        df[\"b_mb\"].apply(lambda x: _bucketize(x, b_q1, b_q2))\n",
    "        if \"b_mb\" in df else \"U\"\n",
    "    )\n",
    "    df[\"compute_bucket\"] = (\n",
    "        df[\"rho_cyc_per_mb\"].apply(lambda x: _bucketize(x, rho_q1, rho_q2))\n",
    "        if \"rho_cyc_per_mb\" in df else \"U\"\n",
    "    )\n",
    "    df[\"mem_bucket\"] = (\n",
    "        df[\"mem_mb\"].apply(lambda x: _bucketize(x, mem_q1, mem_q2))\n",
    "        if \"mem_mb\" in df else \"U\"\n",
    "    )\n",
    "\n",
    "    # --- atomicity & split buckets\n",
    "    if \"non_atomic\" in df.columns:\n",
    "        df[\"atomicity\"] = np.where(df[\"non_atomic\"] == 1, \"splittable\", \"atomic\")\n",
    "    else:\n",
    "        df[\"atomicity\"] = \"atomic\"\n",
    "\n",
    "    if \"split_ratio\" in df.columns:\n",
    "        sp_q1, sp_q2 = thresholds[\"split\"][\"q1\"], thresholds[\"split\"][\"q2\"]\n",
    "        df[\"split_bucket\"] = np.where(\n",
    "            df[\"atomicity\"] == \"splittable\",\n",
    "            df[\"split_ratio\"].apply(lambda v: _bucketize(v, sp_q1, sp_q2)),\n",
    "            \"NA\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"split_bucket\"] = \"NA\"\n",
    "\n",
    "    # --- urgency levels\n",
    "    urgent_cap = int(thresholds.get(\"urgent_slots_cap\", 2))\n",
    "\n",
    "    def _urg(row):\n",
    "        # `deadline_slots` in our pipeline:\n",
    "        #   - positive integer => has a valid deadline\n",
    "        #   - 0 or negative    => sentinel (no deadline)\n",
    "        has_deadline_flag = int(row.get(\"has_deadline\", 0)) == 1\n",
    "        slots_val = row.get(\"deadline_slots\", -1)\n",
    "\n",
    "        # Try to cast to int; if it fails, treat as \"no deadline\"\n",
    "        try:\n",
    "            slots = int(slots_val)\n",
    "        except Exception:\n",
    "            return \"none\"\n",
    "\n",
    "        if (not has_deadline_flag) or (slots <= 0):\n",
    "            return \"none\"\n",
    "\n",
    "        if slots <= urgent_cap:  # very tight deadline → hard\n",
    "            return \"hard\"\n",
    "        return \"soft\"\n",
    "\n",
    "    df[\"urgency\"] = df.apply(_urg, axis=1)\n",
    "\n",
    "    # --- boolean convenience labels\n",
    "    df[\"latency_sensitive\"] = (df[\"urgency\"] == \"hard\")\n",
    "    df[\"compute_heavy\"]     = (df[\"compute_bucket\"] == \"L\")\n",
    "    df[\"io_heavy\"]          = (df[\"size_bucket\"] == \"L\")\n",
    "    df[\"memory_heavy\"]      = (df[\"mem_bucket\"] == \"L\")\n",
    "\n",
    "    # --- a very simple routing hint (for EDA only; not used by RL)\n",
    "    def _hint(row):\n",
    "        if row[\"compute_heavy\"] or row[\"memory_heavy\"]:\n",
    "            return \"CLOUD\"\n",
    "        if row[\"latency_sensitive\"]:\n",
    "            return \"MEC\"\n",
    "        return \"LOCAL\"\n",
    "\n",
    "    df[\"routing_hint\"] = df.apply(_hint, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- batch apply to env_configs (episode → topology → scenario) ----------\n",
    "def label_all_tasks_in_env_configs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    q_low: float = 0.33,\n",
    "    q_high: float = 0.66,\n",
    "    urgent_slots_cap: int = 2,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[Dict[str, Dict[str, Dict[str, Any]]],\n",
    "           Dict[str, Dict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    For each env_config (episode → topology → scenario):\n",
    "      - build thresholds from its own tasks DF\n",
    "      - label tasks\n",
    "      - put labeled DF back into env_config[\"tasks\"]\n",
    "      - return a concise summary per bundle\n",
    "\n",
    "    env_configs structure (as built earlier):\n",
    "        env_configs[ep_name][topology_name][scenario_name] = env_config\n",
    "    \"\"\"\n",
    "    summary: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        summary[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            summary[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                Delta = float(env_cfg[\"Delta\"])\n",
    "\n",
    "                # Build thresholds for this tasks DF\n",
    "                th = build_task_label_thresholds(\n",
    "                    tasks, q_low=q_low, q_high=q_high, urgent_slots_cap=urgent_slots_cap\n",
    "                )\n",
    "\n",
    "                labeled = label_tasks_df(tasks, Delta=Delta, thresholds=th)\n",
    "                env_cfg[\"tasks\"] = labeled  # write back into env_config\n",
    "\n",
    "                # Small summary\n",
    "                cnt = {\n",
    "                    \"n\": len(labeled),\n",
    "                    \"urg_hard\": int((labeled[\"urgency\"] == \"hard\").sum()),\n",
    "                    \"splittable\": int((labeled[\"atomicity\"] == \"splittable\").sum()),\n",
    "                    \"size_L\": int((labeled[\"size_bucket\"] == \"L\").sum()),\n",
    "                    \"compute_L\": int((labeled[\"compute_bucket\"] == \"L\").sum()),\n",
    "                    \"mem_L\": int((labeled[\"mem_bucket\"] == \"L\").sum()),\n",
    "                }\n",
    "                summary[ep_name][topo_name][scen_name] = cnt\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"[label] {ep_name}/{topo_name}/{scen_name} -> \"\n",
    "                        f\"n={cnt['n']}, hard={cnt['urg_hard']}, split={cnt['splittable']}, \"\n",
    "                        f\"sizeL={cnt['size_L']}, compL={cnt['compute_L']}, memL={cnt['mem_L']}\"\n",
    "                    )\n",
    "\n",
    "    return env_configs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[label] ep_000/clustered/heavy -> n=7929, hard=2762, split=3633, sizeL=1909, compL=2696, memL=2696\n",
      "[label] ep_000/clustered/light -> n=2008, hard=300, split=525, sizeL=508, compL=683, memL=683\n",
      "[label] ep_000/clustered/moderate -> n=4969, hard=1231, split=1748, sizeL=1214, compL=1690, memL=1690\n",
      "[label] ep_000/fully_connected/heavy -> n=7929, hard=2762, split=3633, sizeL=1909, compL=2696, memL=2696\n",
      "[label] ep_000/fully_connected/light -> n=2008, hard=300, split=525, sizeL=508, compL=683, memL=683\n",
      "[label] ep_000/fully_connected/moderate -> n=4969, hard=1231, split=1748, sizeL=1214, compL=1690, memL=1690\n",
      "[label] ep_000/skip_connections/heavy -> n=7929, hard=2762, split=3633, sizeL=1909, compL=2696, memL=2696\n",
      "[label] ep_000/skip_connections/light -> n=2008, hard=300, split=525, sizeL=508, compL=683, memL=683\n",
      "[label] ep_000/skip_connections/moderate -> n=4969, hard=1231, split=1748, sizeL=1214, compL=1690, memL=1690\n",
      "\n",
      " ===EXAMPLE (labeled tasks) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>...</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "      <th>final_flag</th>\n",
       "      <th>is_general</th>\n",
       "      <th>is_deadline_hard</th>\n",
       "      <th>is_latency_sensitive</th>\n",
       "      <th>is_compute_intensive</th>\n",
       "      <th>is_data_intensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.588426e+09</td>\n",
       "      <td>7.942128e+09</td>\n",
       "      <td>76.217380</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, io_heavy, splitt...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.486160e+09</td>\n",
       "      <td>5.944639e+09</td>\n",
       "      <td>96.590225</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.441022e+09</td>\n",
       "      <td>4.882044e+09</td>\n",
       "      <td>95.325920</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.564350e+09</td>\n",
       "      <td>1.282175e+10</td>\n",
       "      <td>75.990030</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.821414e+09</td>\n",
       "      <td>7.285658e+09</td>\n",
       "      <td>74.964060</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  mec_id  t_arrival_slot  t_arrival_time  b_mb  \\\n",
       "0    heavy           0        0       0               0             0.0   5.0   \n",
       "1    heavy           0        1       0               0             0.0   4.0   \n",
       "2    heavy           0        2       0               0             0.0   2.0   \n",
       "3    heavy           0        3       0               0             0.0   5.0   \n",
       "4    heavy           0        4       0               0             0.0   4.0   \n",
       "\n",
       "   rho_cyc_per_mb      c_cycles     mem_mb  ...          task_type  \\\n",
       "0    1.588426e+09  7.942128e+09  76.217380  ...      deadline_hard   \n",
       "1    1.486160e+09  5.944639e+09  96.590225  ...  compute_intensive   \n",
       "2    2.441022e+09  4.882044e+09  95.325920  ...  compute_intensive   \n",
       "3    2.564350e+09  1.282175e+10  75.990030  ...      deadline_hard   \n",
       "4    1.821414e+09  7.285658e+09  74.964060  ...  compute_intensive   \n",
       "\n",
       "              task_subtype                  type_reason  \\\n",
       "0            deadline_hard  hard deadline (tight slots)   \n",
       "1  compute_or_memory_heavy   high compute/memory demand   \n",
       "2  compute_or_memory_heavy   high compute/memory demand   \n",
       "3            deadline_hard  hard deadline (tight slots)   \n",
       "4  compute_or_memory_heavy   high compute/memory demand   \n",
       "\n",
       "                                         multi_flags         final_flag  \\\n",
       "0  [deadline_hard, memory_heavy, io_heavy, splitt...      deadline_hard   \n",
       "1                                     [memory_heavy]  compute_intensive   \n",
       "2                      [compute_heavy, memory_heavy]  compute_intensive   \n",
       "3  [deadline_hard, compute_heavy, io_heavy, split...      deadline_hard   \n",
       "4                        [compute_heavy, splittable]  compute_intensive   \n",
       "\n",
       "   is_general is_deadline_hard  is_latency_sensitive is_compute_intensive  \\\n",
       "0       False             True                 False                False   \n",
       "1       False            False                 False                 True   \n",
       "2       False            False                 False                 True   \n",
       "3       False             True                 False                False   \n",
       "4       False            False                 False                 True   \n",
       "\n",
       "  is_data_intensive  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   scenario              7929 non-null   object \n",
      " 1   episode_id            7929 non-null   int64  \n",
      " 2   task_id               7929 non-null   int64  \n",
      " 3   mec_id                7929 non-null   int64  \n",
      " 4   t_arrival_slot        7929 non-null   int32  \n",
      " 5   t_arrival_time        7929 non-null   float64\n",
      " 6   b_mb                  7929 non-null   float64\n",
      " 7   rho_cyc_per_mb        7929 non-null   float64\n",
      " 8   c_cycles              7929 non-null   float64\n",
      " 9   mem_mb                7929 non-null   float64\n",
      " 10  modality              7929 non-null   object \n",
      " 11  has_deadline          7929 non-null   int32  \n",
      " 12  deadline_s            7929 non-null   float32\n",
      " 13  deadline_time         7929 non-null   float32\n",
      " 14  non_atomic            7929 non-null   int64  \n",
      " 15  split_ratio           7929 non-null   float64\n",
      " 16  action_space_hint     7929 non-null   object \n",
      " 17  deadline_slots        7929 non-null   int32  \n",
      " 18  size_bucket           7929 non-null   object \n",
      " 19  compute_bucket        7929 non-null   object \n",
      " 20  mem_bucket            7929 non-null   object \n",
      " 21  atomicity             7929 non-null   object \n",
      " 22  split_bucket          7929 non-null   object \n",
      " 23  urgency               7929 non-null   object \n",
      " 24  latency_sensitive     7929 non-null   bool   \n",
      " 25  compute_heavy         7929 non-null   bool   \n",
      " 26  io_heavy              7929 non-null   bool   \n",
      " 27  memory_heavy          7929 non-null   bool   \n",
      " 28  routing_hint          7929 non-null   object \n",
      " 29  task_type             7929 non-null   object \n",
      " 30  task_subtype          7929 non-null   object \n",
      " 31  type_reason           7929 non-null   object \n",
      " 32  multi_flags           7929 non-null   object \n",
      " 33  final_flag            7929 non-null   object \n",
      " 34  is_general            7929 non-null   bool   \n",
      " 35  is_deadline_hard      7929 non-null   bool   \n",
      " 36  is_latency_sensitive  7929 non-null   bool   \n",
      " 37  is_compute_intensive  7929 non-null   bool   \n",
      " 38  is_data_intensive     7929 non-null   bool   \n",
      "dtypes: bool(9), float32(2), float64(6), int32(3), int64(4), object(15)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ---- Run labeling on current env_configs (episode → topology → scenario) ----\n",
    "env_configs, label_summary = label_all_tasks_in_env_configs(\n",
    "    env_configs,\n",
    "    q_low=0.33,\n",
    "    q_high=0.66,\n",
    "    urgent_slots_cap=2,  # tunable\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n ===EXAMPLE (labeled tasks) ===\")\n",
    "labeled_tasks = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"]\n",
    "display(labeled_tasks.head())\n",
    "print(labeled_tasks.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Task Type Classification </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derive_task_type_row(row: pd.Series) -> tuple[str, str, str, list, str]:\n",
    "    \"\"\"\n",
    "    Returns (task_type, task_subtype, type_reason, multi_flags, final_flag),\n",
    "    where task_type is one of:\n",
    "        - 'deadline_hard'\n",
    "        - 'latency_sensitive'\n",
    "        - 'compute_intensive'\n",
    "        - 'data_intensive'\n",
    "        - 'general'\n",
    "    \"\"\"\n",
    "    # Flags based on previous labeling\n",
    "    urgency        = str(row.get(\"urgency\", \"none\"))     # \"hard\" | \"soft\" | \"none\"\n",
    "    latency_flag   = (urgency == \"hard\") or (urgency == \"soft\")\n",
    "    hard_deadline  = (urgency == \"hard\")\n",
    "\n",
    "    compute_heavy  = bool(row.get(\"compute_heavy\", False))\n",
    "    memory_heavy   = bool(row.get(\"memory_heavy\", False))\n",
    "    io_heavy       = bool(row.get(\"io_heavy\", False))\n",
    "    non_atomic     = bool(row.get(\"atomicity\", \"atomic\") == \"splittable\")\n",
    "\n",
    "    # Collect all active traits for audit\n",
    "    multi_flags = []\n",
    "    if hard_deadline:\n",
    "        multi_flags.append(\"deadline_hard\")\n",
    "    elif latency_flag:\n",
    "        multi_flags.append(\"deadline_soft\")\n",
    "    if compute_heavy:\n",
    "        multi_flags.append(\"compute_heavy\")\n",
    "    if memory_heavy:\n",
    "        multi_flags.append(\"memory_heavy\")\n",
    "    if io_heavy:\n",
    "        multi_flags.append(\"io_heavy\")\n",
    "    if non_atomic:\n",
    "        multi_flags.append(\"splittable\")\n",
    "\n",
    "    # --- Priority resolution (simple, Chapter-4 style) ---\n",
    "\n",
    "    # 1) Hard deadline dominates everything\n",
    "    if hard_deadline:\n",
    "        final_flag = \"deadline_hard\"\n",
    "        return (\"deadline_hard\", \"deadline_hard\", \"hard deadline (tight slots)\", multi_flags, final_flag)\n",
    "\n",
    "    # 2) Latency-sensitive (soft deadlines)\n",
    "    if latency_flag:\n",
    "        final_flag = \"latency_sensitive\"\n",
    "        return (\"latency_sensitive\", \"deadline_soft\", \"delay-sensitive (soft deadline)\", multi_flags, final_flag)\n",
    "\n",
    "    # 3) Compute-intensive (compute or memory heavy)\n",
    "    if compute_heavy or memory_heavy:\n",
    "        final_flag = \"compute_intensive\"\n",
    "        return (\"compute_intensive\", \"compute_or_memory_heavy\", \"high compute/memory demand\", multi_flags, final_flag)\n",
    "\n",
    "    # 4) Data-intensive (large input size / IO heavy)\n",
    "    if io_heavy:\n",
    "        final_flag = \"data_intensive\"\n",
    "        return (\"data_intensive\", \"large_input_bandwidth\", \"large data volume / IO heavy\", multi_flags, final_flag)\n",
    "\n",
    "    # 5) Otherwise general\n",
    "    final_flag = \"general\"\n",
    "    return (\"general\", \"general\", \"no dominant constraint\", multi_flags, final_flag)\n",
    "\n",
    "def apply_ch4_task_typing(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds Chapter-4 level task classes with priority rules into tasks_df (returns a COPY).\n",
    "\n",
    "    Requires that tasks_df already has:\n",
    "        - urgency\n",
    "        - compute_heavy\n",
    "        - memory_heavy\n",
    "        - io_heavy\n",
    "        - atomicity\n",
    "\n",
    "    Columns added:\n",
    "      - task_type       (5-way class)\n",
    "      - task_subtype    (finer descriptor)\n",
    "      - type_reason     (short textual rationale)\n",
    "      - multi_flags     (list of all active boolean traits)\n",
    "      - final_flag      (single primary flag)\n",
    "      - is_* one-hot convenience columns\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    required_cols = [\"urgency\", \"compute_heavy\", \"memory_heavy\", \"io_heavy\", \"atomicity\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"apply_ch4_task_typing: missing label columns: {missing}\")\n",
    "\n",
    "    out_type, out_sub, out_reason, out_flags, out_final_flag = [], [], [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        t, s, msg, flags, final_flag = _derive_task_type_row(r)\n",
    "        out_type.append(t)\n",
    "        out_sub.append(s)\n",
    "        out_reason.append(msg)\n",
    "        out_flags.append(flags)\n",
    "        out_final_flag.append(final_flag)\n",
    "\n",
    "    df[\"task_type\"]    = out_type\n",
    "    df[\"task_subtype\"] = out_sub\n",
    "    df[\"type_reason\"]  = out_reason\n",
    "    df[\"multi_flags\"]  = out_flags\n",
    "    df[\"final_flag\"]   = out_final_flag\n",
    "\n",
    "    # One-hot convenience view\n",
    "    df[\"is_general\"]           = (df[\"task_type\"] == \"general\")\n",
    "    df[\"is_deadline_hard\"]     = (df[\"task_type\"] == \"deadline_hard\")\n",
    "    df[\"is_latency_sensitive\"] = (df[\"task_type\"] == \"latency_sensitive\")\n",
    "    df[\"is_compute_intensive\"] = (df[\"task_type\"] == \"compute_intensive\")\n",
    "    df[\"is_data_intensive\"]    = (df[\"task_type\"] == \"data_intensive\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_task_typing_in_env_configs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    env_configs structure (episode-first):\n",
    "        env_configs[ep_name][topology_name][scenario_name][\"tasks\"] -> DataFrame\n",
    "\n",
    "    This function:\n",
    "      - applies Chapter-4 task typing to every tasks DF\n",
    "      - writes back the enriched DataFrame\n",
    "      - prints a short summary if verbose=True\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                enriched = apply_ch4_task_typing(tasks)\n",
    "                env_cfg[\"tasks\"] = enriched\n",
    "\n",
    "                if verbose:\n",
    "                    n = len(enriched)\n",
    "                    counts = enriched[\"task_type\"].value_counts().to_dict()\n",
    "                    print(f\"[typing] {ep_name}/{topo_name}/{scen_name}  n={n}  → {counts}\")\n",
    "\n",
    "    return env_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[typing] ep_000/clustered/heavy  n=7929  → {'compute_intensive': 2894, 'deadline_hard': 2762, 'general': 1741, 'data_intensive': 532}\n",
      "[typing] ep_000/clustered/light  n=2008  → {'compute_intensive': 942, 'general': 577, 'deadline_hard': 300, 'data_intensive': 189}\n",
      "[typing] ep_000/clustered/moderate  n=4969  → {'compute_intensive': 2091, 'general': 1235, 'deadline_hard': 1231, 'data_intensive': 412}\n",
      "[typing] ep_000/fully_connected/heavy  n=7929  → {'compute_intensive': 2894, 'deadline_hard': 2762, 'general': 1741, 'data_intensive': 532}\n",
      "[typing] ep_000/fully_connected/light  n=2008  → {'compute_intensive': 942, 'general': 577, 'deadline_hard': 300, 'data_intensive': 189}\n",
      "[typing] ep_000/fully_connected/moderate  n=4969  → {'compute_intensive': 2091, 'general': 1235, 'deadline_hard': 1231, 'data_intensive': 412}\n",
      "[typing] ep_000/skip_connections/heavy  n=7929  → {'compute_intensive': 2894, 'deadline_hard': 2762, 'general': 1741, 'data_intensive': 532}\n",
      "[typing] ep_000/skip_connections/light  n=2008  → {'compute_intensive': 942, 'general': 577, 'deadline_hard': 300, 'data_intensive': 189}\n",
      "[typing] ep_000/skip_connections/moderate  n=4969  → {'compute_intensive': 2091, 'general': 1235, 'deadline_hard': 1231, 'data_intensive': 412}\n",
      "\n",
      " ===EXAMPLE (task typing) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "      <th>final_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, io_heavy, splitt...</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, splittable]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[splittable]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[splittable]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[splittable]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, splittable]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, memory_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[splittable]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, splittable]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_id          task_type             task_subtype  \\\n",
       "0         0      deadline_hard            deadline_hard   \n",
       "1         1  compute_intensive  compute_or_memory_heavy   \n",
       "2         2  compute_intensive  compute_or_memory_heavy   \n",
       "3         3      deadline_hard            deadline_hard   \n",
       "4         4  compute_intensive  compute_or_memory_heavy   \n",
       "5         5  compute_intensive  compute_or_memory_heavy   \n",
       "6         6            general                  general   \n",
       "7         7      deadline_hard            deadline_hard   \n",
       "8         8      deadline_hard            deadline_hard   \n",
       "9         9  compute_intensive  compute_or_memory_heavy   \n",
       "10       10            general                  general   \n",
       "11       11            general                  general   \n",
       "12       12            general                  general   \n",
       "13       13      deadline_hard            deadline_hard   \n",
       "14       14            general                  general   \n",
       "15       15  compute_intensive  compute_or_memory_heavy   \n",
       "16       16      deadline_hard            deadline_hard   \n",
       "17       17      deadline_hard            deadline_hard   \n",
       "18       18  compute_intensive  compute_or_memory_heavy   \n",
       "19       19  compute_intensive  compute_or_memory_heavy   \n",
       "20       20            general                  general   \n",
       "21       21      deadline_hard            deadline_hard   \n",
       "22       22      deadline_hard            deadline_hard   \n",
       "23       23      deadline_hard            deadline_hard   \n",
       "24       24      deadline_hard            deadline_hard   \n",
       "\n",
       "                    type_reason  \\\n",
       "0   hard deadline (tight slots)   \n",
       "1    high compute/memory demand   \n",
       "2    high compute/memory demand   \n",
       "3   hard deadline (tight slots)   \n",
       "4    high compute/memory demand   \n",
       "5    high compute/memory demand   \n",
       "6        no dominant constraint   \n",
       "7   hard deadline (tight slots)   \n",
       "8   hard deadline (tight slots)   \n",
       "9    high compute/memory demand   \n",
       "10       no dominant constraint   \n",
       "11       no dominant constraint   \n",
       "12       no dominant constraint   \n",
       "13  hard deadline (tight slots)   \n",
       "14       no dominant constraint   \n",
       "15   high compute/memory demand   \n",
       "16  hard deadline (tight slots)   \n",
       "17  hard deadline (tight slots)   \n",
       "18   high compute/memory demand   \n",
       "19   high compute/memory demand   \n",
       "20       no dominant constraint   \n",
       "21  hard deadline (tight slots)   \n",
       "22  hard deadline (tight slots)   \n",
       "23  hard deadline (tight slots)   \n",
       "24  hard deadline (tight slots)   \n",
       "\n",
       "                                          multi_flags         final_flag  \n",
       "0   [deadline_hard, memory_heavy, io_heavy, splitt...      deadline_hard  \n",
       "1                                      [memory_heavy]  compute_intensive  \n",
       "2                       [compute_heavy, memory_heavy]  compute_intensive  \n",
       "3   [deadline_hard, compute_heavy, io_heavy, split...      deadline_hard  \n",
       "4                         [compute_heavy, splittable]  compute_intensive  \n",
       "5                         [compute_heavy, splittable]  compute_intensive  \n",
       "6                                                  []            general  \n",
       "7           [deadline_hard, memory_heavy, splittable]      deadline_hard  \n",
       "8                           [deadline_hard, io_heavy]      deadline_hard  \n",
       "9                                      [memory_heavy]  compute_intensive  \n",
       "10                                       [splittable]            general  \n",
       "11                                                 []            general  \n",
       "12                                       [splittable]            general  \n",
       "13                     [deadline_hard, compute_heavy]      deadline_hard  \n",
       "14                                       [splittable]            general  \n",
       "15                                    [compute_heavy]  compute_intensive  \n",
       "16                        [deadline_hard, splittable]      deadline_hard  \n",
       "17                                    [deadline_hard]      deadline_hard  \n",
       "18                         [memory_heavy, splittable]  compute_intensive  \n",
       "19          [compute_heavy, memory_heavy, splittable]  compute_intensive  \n",
       "20                                       [splittable]            general  \n",
       "21                      [deadline_hard, memory_heavy]      deadline_hard  \n",
       "22                      [deadline_hard, memory_heavy]      deadline_hard  \n",
       "23                        [deadline_hard, splittable]      deadline_hard  \n",
       "24                      [deadline_hard, memory_heavy]      deadline_hard  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Run typing on current env_configs (episode → topology → scenario) ----\n",
    "env_configs = apply_task_typing_in_env_configs(env_configs, verbose=True)\n",
    "\n",
    "print(\"\\n ===EXAMPLE (task typing) ===\")\n",
    "display(\n",
    "    env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"][\n",
    "        [\"task_id\", \"task_type\", \"task_subtype\", \"type_reason\", \"multi_flags\", \"final_flag\"]\n",
    "    ].head(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "none → Tasks that do not have a specific deadline or time sensitivity </br>\n",
    "hard → Tasks that have a very limited deadline and delay is very important to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none    5167\n",
      "hard    2762\n",
      "Name: urgency, dtype: int64\n",
      "\n",
      " compute_intensive    2894\n",
      "deadline_hard        2762\n",
      "general              1741\n",
      "data_intensive        532\n",
      "Name: task_type, dtype: int64\n",
      "\n",
      "                    b_mb  rho_cyc_per_mb     mem_mb\n",
      "task_type                                         \n",
      "compute_intensive   3.0    1.931592e+09  81.547025\n",
      "data_intensive      5.0    1.221956e+09  53.114409\n",
      "deadline_hard       4.0    1.503182e+09  64.455010\n",
      "general             3.0    1.236334e+09  53.812195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mec_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>...</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "      <th>final_flag</th>\n",
       "      <th>is_general</th>\n",
       "      <th>is_deadline_hard</th>\n",
       "      <th>is_latency_sensitive</th>\n",
       "      <th>is_compute_intensive</th>\n",
       "      <th>is_data_intensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.588426e+09</td>\n",
       "      <td>7.942128e+09</td>\n",
       "      <td>76.217380</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, io_heavy, splitt...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.486160e+09</td>\n",
       "      <td>5.944639e+09</td>\n",
       "      <td>96.590225</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.441022e+09</td>\n",
       "      <td>4.882044e+09</td>\n",
       "      <td>95.325920</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.564350e+09</td>\n",
       "      <td>1.282175e+10</td>\n",
       "      <td>75.990030</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.821414e+09</td>\n",
       "      <td>7.285658e+09</td>\n",
       "      <td>74.964060</td>\n",
       "      <td>...</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  mec_id  t_arrival_slot  t_arrival_time  b_mb  \\\n",
       "0    heavy           0        0       0               0             0.0   5.0   \n",
       "1    heavy           0        1       0               0             0.0   4.0   \n",
       "2    heavy           0        2       0               0             0.0   2.0   \n",
       "3    heavy           0        3       0               0             0.0   5.0   \n",
       "4    heavy           0        4       0               0             0.0   4.0   \n",
       "\n",
       "   rho_cyc_per_mb      c_cycles     mem_mb  ...          task_type  \\\n",
       "0    1.588426e+09  7.942128e+09  76.217380  ...      deadline_hard   \n",
       "1    1.486160e+09  5.944639e+09  96.590225  ...  compute_intensive   \n",
       "2    2.441022e+09  4.882044e+09  95.325920  ...  compute_intensive   \n",
       "3    2.564350e+09  1.282175e+10  75.990030  ...      deadline_hard   \n",
       "4    1.821414e+09  7.285658e+09  74.964060  ...  compute_intensive   \n",
       "\n",
       "              task_subtype                  type_reason  \\\n",
       "0            deadline_hard  hard deadline (tight slots)   \n",
       "1  compute_or_memory_heavy   high compute/memory demand   \n",
       "2  compute_or_memory_heavy   high compute/memory demand   \n",
       "3            deadline_hard  hard deadline (tight slots)   \n",
       "4  compute_or_memory_heavy   high compute/memory demand   \n",
       "\n",
       "                                         multi_flags         final_flag  \\\n",
       "0  [deadline_hard, memory_heavy, io_heavy, splitt...      deadline_hard   \n",
       "1                                     [memory_heavy]  compute_intensive   \n",
       "2                      [compute_heavy, memory_heavy]  compute_intensive   \n",
       "3  [deadline_hard, compute_heavy, io_heavy, split...      deadline_hard   \n",
       "4                        [compute_heavy, splittable]  compute_intensive   \n",
       "\n",
       "   is_general is_deadline_hard  is_latency_sensitive is_compute_intensive  \\\n",
       "0       False             True                 False                False   \n",
       "1       False            False                 False                 True   \n",
       "2       False            False                 False                 True   \n",
       "3       False             True                 False                False   \n",
       "4       False            False                 False                 True   \n",
       "\n",
       "  is_data_intensive  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   scenario              7929 non-null   object \n",
      " 1   episode_id            7929 non-null   int64  \n",
      " 2   task_id               7929 non-null   int64  \n",
      " 3   mec_id                7929 non-null   int64  \n",
      " 4   t_arrival_slot        7929 non-null   int32  \n",
      " 5   t_arrival_time        7929 non-null   float64\n",
      " 6   b_mb                  7929 non-null   float64\n",
      " 7   rho_cyc_per_mb        7929 non-null   float64\n",
      " 8   c_cycles              7929 non-null   float64\n",
      " 9   mem_mb                7929 non-null   float64\n",
      " 10  modality              7929 non-null   object \n",
      " 11  has_deadline          7929 non-null   int32  \n",
      " 12  deadline_s            7929 non-null   float32\n",
      " 13  deadline_time         7929 non-null   float32\n",
      " 14  non_atomic            7929 non-null   int64  \n",
      " 15  split_ratio           7929 non-null   float64\n",
      " 16  action_space_hint     7929 non-null   object \n",
      " 17  deadline_slots        7929 non-null   int32  \n",
      " 18  size_bucket           7929 non-null   object \n",
      " 19  compute_bucket        7929 non-null   object \n",
      " 20  mem_bucket            7929 non-null   object \n",
      " 21  atomicity             7929 non-null   object \n",
      " 22  split_bucket          7929 non-null   object \n",
      " 23  urgency               7929 non-null   object \n",
      " 24  latency_sensitive     7929 non-null   bool   \n",
      " 25  compute_heavy         7929 non-null   bool   \n",
      " 26  io_heavy              7929 non-null   bool   \n",
      " 27  memory_heavy          7929 non-null   bool   \n",
      " 28  routing_hint          7929 non-null   object \n",
      " 29  task_type             7929 non-null   object \n",
      " 30  task_subtype          7929 non-null   object \n",
      " 31  type_reason           7929 non-null   object \n",
      " 32  multi_flags           7929 non-null   object \n",
      " 33  final_flag            7929 non-null   object \n",
      " 34  is_general            7929 non-null   bool   \n",
      " 35  is_deadline_hard      7929 non-null   bool   \n",
      " 36  is_latency_sensitive  7929 non-null   bool   \n",
      " 37  is_compute_intensive  7929 non-null   bool   \n",
      " 38  is_data_intensive     7929 non-null   bool   \n",
      "dtypes: bool(9), float32(2), float64(6), int32(3), int64(4), object(15)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "labeled_tasks_completed = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"]\n",
    "print(labeled_tasks_completed[\"urgency\"].value_counts())\n",
    "print(\"\\n\", labeled_tasks_completed[\"task_type\"].value_counts())\n",
    "print(\"\\n\", labeled_tasks_completed.groupby(\"task_type\")[[\"b_mb\",\"rho_cyc_per_mb\",\"mem_mb\"]].median())\n",
    "\n",
    "display(labeled_tasks_completed.head())\n",
    "print(labeled_tasks_completed.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 3: Agent (MEC) Profiling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we construct a behavioral profile for each agent, capturing its local compute resources, task arrival rate, and the distribution of task types it generates. These profiles are later used for clustering agents and assigning suitable reinforcement learning strategies to each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper 1: per-MEC per-slot arrival counts ----\n",
    "def _per_mec_slot_counts(arrivals_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count how many tasks each MEC receives in each time slot.\n",
    "    This is used to estimate lambda (arrival rate) statistics per MEC.\n",
    "    \"\"\"\n",
    "    if not {\"mec_id\", \"t_slot\"}.issubset(arrivals_df.columns):\n",
    "        raise ValueError(\"arrivals must contain 'mec_id' and 't_slot'.\")\n",
    "    grp = arrivals_df.groupby([\"mec_id\", \"t_slot\"], as_index=False).size()\n",
    "    grp.rename(columns={\"size\": \"count\"}, inplace=True)\n",
    "    return grp\n",
    "\n",
    "# ---- Helper 2: estimate λ-mean and λ-variance per MEC ----\n",
    "def _lambda_stats_from_counts_mec(counts_df: pd.DataFrame, Delta: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert per-slot counts to rate statistics per MEC:\n",
    "        lambda_mean = mean(count_per_slot) / Delta\n",
    "        lambda_var  = var(count_per_slot)  / Delta^2\n",
    "    \"\"\"\n",
    "    if counts_df.empty:\n",
    "        return pd.DataFrame(columns=[\"mec_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"])\n",
    "\n",
    "    agg = counts_df.groupby(\"mec_id\")[\"count\"].agg(\n",
    "        lambda_mean_slot=\"mean\",\n",
    "        lambda_var_slot=\"var\",\n",
    "        slots_observed=\"count\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # If only one observation exists, variance becomes NaN → treat as zero.\n",
    "    agg[\"lambda_var_slot\"] = agg[\"lambda_var_slot\"].fillna(0.0).astype(float)\n",
    "\n",
    "    # Convert to per-second rates\n",
    "    agg[\"lambda_mean\"] = (agg[\"lambda_mean_slot\"] / float(Delta)).astype(float)\n",
    "    agg[\"lambda_var\"]  = (agg[\"lambda_var_slot\"]  / float(Delta**2)).astype(float)\n",
    "\n",
    "    return agg[[\"mec_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"]]\n",
    "\n",
    "# ---- Helper 3: task-type distribution per MEC ----\n",
    "_TASK_TYPES = [\"general\", \"latency_sensitive\", \"deadline_hard\", \"data_intensive\", \"compute_intensive\"]\n",
    "\n",
    "def _task_distribution_per_mec(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute distribution of task types per MEC (probabilities sum to 1\n",
    "    for MECs that actually have tasks).\n",
    "\n",
    "    Also adds light median features useful for clustering:\n",
    "      - b_mb_med, rho_med, mem_med, hard_share\n",
    "    \"\"\"\n",
    "    if not {\"mec_id\", \"task_type\"}.issubset(tasks_df.columns):\n",
    "        raise ValueError(\"tasks must contain 'mec_id' and 'task_type'.\")\n",
    "\n",
    "    if tasks_df.empty:\n",
    "        # Empty DF: return an empty frame with all expected columns\n",
    "        piv = pd.DataFrame(index=pd.Index([], name=\"mec_id\"))\n",
    "        for t in _TASK_TYPES:\n",
    "            piv[t] = 0.0\n",
    "        piv[\"n_tasks_mec\"] = 0.0\n",
    "    else:\n",
    "        # Raw counts per (mec_id, task_type)\n",
    "        cnt = tasks_df.groupby([\"mec_id\", \"task_type\"], as_index=False).size()\n",
    "        piv = cnt.pivot(index=\"mec_id\", columns=\"task_type\", values=\"size\").fillna(0.0)\n",
    "\n",
    "        # Ensure all expected classes exist\n",
    "        for t in _TASK_TYPES:\n",
    "            if t not in piv.columns:\n",
    "                piv[t] = 0.0\n",
    "\n",
    "        # Total tasks per MEC\n",
    "        piv[\"n_tasks_mec\"] = piv[_TASK_TYPES].sum(axis=1).astype(float)\n",
    "\n",
    "    # Probabilities\n",
    "    for t in _TASK_TYPES:\n",
    "        piv[f\"P_{t}\"] = np.where(\n",
    "            piv[\"n_tasks_mec\"] > 0,\n",
    "            piv[t] / piv[\"n_tasks_mec\"],\n",
    "            0.0\n",
    "        ).astype(float)\n",
    "\n",
    "    # Optional extra features (medians, hard deadline share)\n",
    "    feats = {}\n",
    "    needed = {\"b_mb\", \"rho_cyc_per_mb\", \"mem_mb\", \"urgency\"}\n",
    "    if needed.issubset(tasks_df.columns) and not tasks_df.empty:\n",
    "        agg = tasks_df.groupby(\"mec_id\").agg(\n",
    "            b_mb_med=(\"b_mb\", \"median\"),\n",
    "            rho_med=(\"rho_cyc_per_mb\", \"median\"),\n",
    "            mem_med=(\"mem_mb\", \"median\"),\n",
    "            hard_share=(\"urgency\", lambda s: float((s == \"hard\").mean()))\n",
    "        ).reset_index()\n",
    "        feats = agg.set_index(\"mec_id\")\n",
    "\n",
    "    # Join extra features (if any)\n",
    "    piv = piv.join(feats, how=\"left\")\n",
    "\n",
    "    for c in [\"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"]:\n",
    "        if c in piv.columns:\n",
    "            piv[c] = piv[c].fillna(0.0).astype(float)\n",
    "        else:\n",
    "            piv[c] = 0.0\n",
    "\n",
    "    # Probability mass sum (diagnostic)\n",
    "    prob_cols = [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    piv[\"TaskDist_sum\"] = piv[prob_cols].sum(axis=1).astype(float)\n",
    "\n",
    "    keep = [\"n_tasks_mec\", \"TaskDist_sum\", \"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"] + prob_cols\n",
    "    return piv[keep].reset_index()  # reset_index → get mec_id as a column\n",
    "\n",
    "# ---- Helper 4: fraction of non-atomic (splittable) tasks per MEC ----\n",
    "def _non_atomic_share_per_mec(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the share of splittable (non-atomic) tasks per MEC.\n",
    "    \"\"\"\n",
    "    if \"mec_id\" not in tasks_df.columns:\n",
    "        raise ValueError(\"tasks must contain 'mec_id'.\")\n",
    "\n",
    "    if \"non_atomic\" not in tasks_df.columns or tasks_df.empty:\n",
    "        # If missing or no tasks, assume zero share for each MEC that appears\n",
    "        mec_ids = tasks_df.get(\"mec_id\")\n",
    "        if mec_ids is None or len(mec_ids) == 0:\n",
    "            return pd.DataFrame(columns=[\"mec_id\", \"non_atomic_share\"])\n",
    "        return pd.DataFrame({\"mec_id\": mec_ids.unique(), \"non_atomic_share\": 0.0})\n",
    "\n",
    "    grp = tasks_df.groupby(\"mec_id\")[\"non_atomic\"].agg(\n",
    "        non_atomic_share=lambda s: float((s == 1).mean())\n",
    "    ).reset_index()\n",
    "    return grp\n",
    "\n",
    "# ---- Build MEC profiles for ONE env_config ----\n",
    "def build_mec_profiles_for_env(env_cfg: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct per-MEC profiles combining:\n",
    "      - Resource capacities:\n",
    "            private_cpu, public_cpu\n",
    "            private_cpu_slot, public_cpu_slot (approx: capacity * Delta)\n",
    "      - Arrival rate statistics:\n",
    "            lambda_mean, lambda_var, slots_observed\n",
    "      - Task type distribution:\n",
    "            P_general, P_latency_sensitive, P_deadline_hard,\n",
    "            P_data_intensive, P_compute_intensive\n",
    "            + n_tasks_mec, TaskDist_sum, b_mb_med, rho_med, mem_med, hard_share\n",
    "      - Splittability share:\n",
    "            non_atomic_share\n",
    "    \"\"\"\n",
    "    tasks    = env_cfg[\"tasks\"]\n",
    "    arrivals = env_cfg[\"arrivals\"]\n",
    "    Delta    = float(env_cfg[\"Delta\"])\n",
    "    K        = int(env_cfg[\"K\"])\n",
    "\n",
    "    # Capacities from topology/env_config\n",
    "    private_cpu = np.asarray(env_cfg[\"private_cpu\"], dtype=float)\n",
    "    public_cpu  = np.asarray(env_cfg[\"public_cpu\"], dtype=float)\n",
    "\n",
    "    if private_cpu.shape[0] != K or public_cpu.shape[0] != K:\n",
    "        raise ValueError(\"Length of private_cpu/public_cpu must equal K in env_config.\")\n",
    "\n",
    "    # Per-slot capacities (assuming private_cpu/public_cpu are per-second-like units)\n",
    "    private_cpu_slot = private_cpu * Delta\n",
    "    public_cpu_slot  = public_cpu  * Delta\n",
    "\n",
    "    # 1) Arrival statistics per MEC\n",
    "    counts_df = _per_mec_slot_counts(arrivals)\n",
    "    lam_df    = _lambda_stats_from_counts_mec(counts_df, Delta=Delta)\n",
    "\n",
    "    # 2) Task-type distribution (+ medians & hard_share)\n",
    "    dist_df   = _task_distribution_per_mec(tasks)\n",
    "\n",
    "    # 3) Splittable-task share per MEC\n",
    "    na_df     = _non_atomic_share_per_mec(tasks)\n",
    "\n",
    "    # Base table: one row per MEC\n",
    "    base = pd.DataFrame({\n",
    "        \"mec_id\": np.arange(K, dtype=int),\n",
    "        \"private_cpu\": private_cpu.astype(float),\n",
    "        \"public_cpu\": public_cpu.astype(float),\n",
    "        \"private_cpu_slot\": private_cpu_slot.astype(float),\n",
    "        \"public_cpu_slot\": public_cpu_slot.astype(float),\n",
    "    })\n",
    "\n",
    "    # Merge all components\n",
    "    prof = (base\n",
    "            .merge(lam_df,  on=\"mec_id\", how=\"left\")\n",
    "            .merge(dist_df, on=\"mec_id\", how=\"left\")\n",
    "            .merge(na_df,   on=\"mec_id\", how=\"left\"))\n",
    "\n",
    "    # Fill missing values for MECs with no arrivals/tasks\n",
    "    fill_zero = [\n",
    "        \"lambda_mean\", \"lambda_var\", \"slots_observed\",\n",
    "        \"n_tasks_mec\", \"non_atomic_share\",\n",
    "        \"TaskDist_sum\", \"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"\n",
    "    ] + [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "\n",
    "    for c in fill_zero:\n",
    "        if c in prof.columns:\n",
    "            prof[c] = prof[c].fillna(0.0).astype(float)\n",
    "\n",
    "    # Soft warning if probabilities don't sum to ~1 for MECs that do have tasks\n",
    "    if \"n_tasks_mec\" in prof.columns and \"TaskDist_sum\" in prof.columns:\n",
    "        mask = (prof[\"n_tasks_mec\"] > 0) & (~np.isclose(prof[\"TaskDist_sum\"], 1.0, atol=1e-6))\n",
    "        if mask.any():\n",
    "            n_bad = int(mask.sum())\n",
    "            print(f\"[warn] TaskDist_sum != 1.0 for {n_bad} MEC(s). (tolerance 1e-6)\")\n",
    "\n",
    "    return prof\n",
    "\n",
    "# ---- Batch profiling for ALL env_configs ----\n",
    "def build_all_mec_profiles(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    Compute MEC profiles for every (episode → topology → scenario) environment.\n",
    "\n",
    "    Input:\n",
    "        env_configs[ep_name][topology_name][scenario_name][\"tasks\"] / [\"arrivals\"] / ...\n",
    "\n",
    "    Output:\n",
    "        mec_profiles[ep_name][topology_name][scen_name] = DataFrame\n",
    "\n",
    "    Also writes back to:\n",
    "        env_configs[ep_name][topology_name][scen_name][\"mec_profiles\"]\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Dict[str, pd.DataFrame]]] = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        out[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            out[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                prof = build_mec_profiles_for_env(env_cfg)\n",
    "                out[ep_name][topo_name][scen_name] = prof\n",
    "                env_cfg[\"mec_profiles\"] = prof  # attach for direct access\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===EXAMPLE MEC PROFILES===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_id</th>\n",
       "      <th>private_cpu</th>\n",
       "      <th>public_cpu</th>\n",
       "      <th>private_cpu_slot</th>\n",
       "      <th>public_cpu_slot</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_mec</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>5.245556</td>\n",
       "      <td>100.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.515893e+09</td>\n",
       "      <td>65.225980</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.210428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.074488</td>\n",
       "      <td>0.394786</td>\n",
       "      <td>0.432030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>4.211503</td>\n",
       "      <td>99.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.440694e+09</td>\n",
       "      <td>64.346577</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.236715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>0.357488</td>\n",
       "      <td>0.396135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>3.712637</td>\n",
       "      <td>99.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.487741e+09</td>\n",
       "      <td>63.485611</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.171717</td>\n",
       "      <td>2.470212</td>\n",
       "      <td>99.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.470488e+09</td>\n",
       "      <td>64.613190</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.334395</td>\n",
       "      <td>0.509554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.755102</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>98.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.461619e+09</td>\n",
       "      <td>63.199425</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.230978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.538043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mec_id  private_cpu  public_cpu  private_cpu_slot  public_cpu_slot  \\\n",
       "0       0          5.0         5.0               5.0              5.0   \n",
       "1       1          5.0         5.0               5.0              5.0   \n",
       "2       2          5.0         5.0               5.0              5.0   \n",
       "3       3          5.0         5.0               5.0              5.0   \n",
       "4       4          5.0         5.0               5.0              5.0   \n",
       "\n",
       "   lambda_mean  lambda_var  slots_observed  n_tasks_mec  TaskDist_sum  \\\n",
       "0     5.370000    5.245556           100.0        537.0           1.0   \n",
       "1     4.181818    4.211503            99.0        414.0           1.0   \n",
       "2     4.040404    3.712637            99.0        400.0           1.0   \n",
       "3     3.171717    2.470212            99.0        314.0           1.0   \n",
       "4     3.755102    3.877551            98.0        368.0           1.0   \n",
       "\n",
       "   b_mb_med       rho_med    mem_med  hard_share  P_general  \\\n",
       "0       3.0  1.515893e+09  65.225980    0.320298   0.210428   \n",
       "1       4.0  1.440694e+09  64.346577    0.338164   0.236715   \n",
       "2       3.0  1.487741e+09  63.485611    0.332500   0.240000   \n",
       "3       4.0  1.470488e+09  64.613190    0.343949   0.248408   \n",
       "4       3.0  1.461619e+09  63.199425    0.320652   0.230978   \n",
       "\n",
       "   P_latency_sensitive  P_deadline_hard  P_data_intensive  \\\n",
       "0                  0.0         0.320298          0.074488   \n",
       "1                  0.0         0.338164          0.067633   \n",
       "2                  0.0         0.332500          0.070000   \n",
       "3                  0.0         0.343949          0.073248   \n",
       "4                  0.0         0.320652          0.078804   \n",
       "\n",
       "   P_compute_intensive  non_atomic_share  \n",
       "0             0.394786          0.432030  \n",
       "1             0.357488          0.396135  \n",
       "2             0.357500          0.460000  \n",
       "3             0.334395          0.509554  \n",
       "4             0.369565          0.538043  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_id</th>\n",
       "      <th>private_cpu</th>\n",
       "      <th>public_cpu</th>\n",
       "      <th>private_cpu_slot</th>\n",
       "      <th>public_cpu_slot</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_mec</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>5.245556</td>\n",
       "      <td>100.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.515893e+09</td>\n",
       "      <td>65.225980</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.210428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.074488</td>\n",
       "      <td>0.394786</td>\n",
       "      <td>0.432030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>4.211503</td>\n",
       "      <td>99.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.440694e+09</td>\n",
       "      <td>64.346577</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.236715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>0.357488</td>\n",
       "      <td>0.396135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>3.712637</td>\n",
       "      <td>99.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.487741e+09</td>\n",
       "      <td>63.485611</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.171717</td>\n",
       "      <td>2.470212</td>\n",
       "      <td>99.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.470488e+09</td>\n",
       "      <td>64.613190</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.334395</td>\n",
       "      <td>0.509554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.755102</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>98.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.461619e+09</td>\n",
       "      <td>63.199425</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.230978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.538043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.653061</td>\n",
       "      <td>3.445403</td>\n",
       "      <td>98.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.451501e+09</td>\n",
       "      <td>61.931205</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.243017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.349162</td>\n",
       "      <td>0.494413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>2.191095</td>\n",
       "      <td>99.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.493120e+09</td>\n",
       "      <td>65.202740</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.479365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.131313</td>\n",
       "      <td>3.768295</td>\n",
       "      <td>99.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.621984e+09</td>\n",
       "      <td>65.171020</td>\n",
       "      <td>0.356968</td>\n",
       "      <td>0.178484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356968</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.408313</td>\n",
       "      <td>0.481663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.323232</td>\n",
       "      <td>4.159761</td>\n",
       "      <td>99.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.441129e+09</td>\n",
       "      <td>63.089418</td>\n",
       "      <td>0.369159</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369159</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0.336449</td>\n",
       "      <td>0.450935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.552083</td>\n",
       "      <td>3.744627</td>\n",
       "      <td>96.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.546929e+09</td>\n",
       "      <td>67.668550</td>\n",
       "      <td>0.340176</td>\n",
       "      <td>0.199413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340176</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.395894</td>\n",
       "      <td>0.463343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.969697</td>\n",
       "      <td>4.131725</td>\n",
       "      <td>99.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.548238e+09</td>\n",
       "      <td>63.100880</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.244275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.371501</td>\n",
       "      <td>0.445293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.468085</td>\n",
       "      <td>3.197895</td>\n",
       "      <td>94.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.496051e+09</td>\n",
       "      <td>66.019930</td>\n",
       "      <td>0.337423</td>\n",
       "      <td>0.193252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337423</td>\n",
       "      <td>0.064417</td>\n",
       "      <td>0.404908</td>\n",
       "      <td>0.453988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.346939</td>\n",
       "      <td>3.837155</td>\n",
       "      <td>98.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.546243e+09</td>\n",
       "      <td>64.210399</td>\n",
       "      <td>0.361502</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361502</td>\n",
       "      <td>0.077465</td>\n",
       "      <td>0.363850</td>\n",
       "      <td>0.420188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.616162</td>\n",
       "      <td>3.381777</td>\n",
       "      <td>99.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.597695e+09</td>\n",
       "      <td>67.242315</td>\n",
       "      <td>0.343575</td>\n",
       "      <td>0.209497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343575</td>\n",
       "      <td>0.053073</td>\n",
       "      <td>0.393855</td>\n",
       "      <td>0.452514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>2.478664</td>\n",
       "      <td>99.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.494153e+09</td>\n",
       "      <td>65.454258</td>\n",
       "      <td>0.380556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380556</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.292929</td>\n",
       "      <td>4.229643</td>\n",
       "      <td>99.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.467142e+09</td>\n",
       "      <td>62.826683</td>\n",
       "      <td>0.322353</td>\n",
       "      <td>0.244706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322353</td>\n",
       "      <td>0.087059</td>\n",
       "      <td>0.345882</td>\n",
       "      <td>0.472941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.525253</td>\n",
       "      <td>3.435580</td>\n",
       "      <td>99.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.473031e+09</td>\n",
       "      <td>66.614723</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.185268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.444196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>4.628182</td>\n",
       "      <td>100.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.458212e+09</td>\n",
       "      <td>65.109270</td>\n",
       "      <td>0.351670</td>\n",
       "      <td>0.233792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351670</td>\n",
       "      <td>0.056974</td>\n",
       "      <td>0.357564</td>\n",
       "      <td>0.444008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.855670</td>\n",
       "      <td>3.770619</td>\n",
       "      <td>97.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.569169e+09</td>\n",
       "      <td>63.602707</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.222930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.072187</td>\n",
       "      <td>0.360934</td>\n",
       "      <td>0.452229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.391753</td>\n",
       "      <td>3.032431</td>\n",
       "      <td>97.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.491320e+09</td>\n",
       "      <td>63.580395</td>\n",
       "      <td>0.407295</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407295</td>\n",
       "      <td>0.051672</td>\n",
       "      <td>0.328267</td>\n",
       "      <td>0.449848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mec_id  private_cpu  public_cpu  private_cpu_slot  public_cpu_slot  \\\n",
       "0        0          5.0         5.0               5.0              5.0   \n",
       "1        1          5.0         5.0               5.0              5.0   \n",
       "2        2          5.0         5.0               5.0              5.0   \n",
       "3        3          5.0         5.0               5.0              5.0   \n",
       "4        4          5.0         5.0               5.0              5.0   \n",
       "5        5          5.0         5.0               5.0              5.0   \n",
       "6        6          5.0         5.0               5.0              5.0   \n",
       "7        7          5.0         5.0               5.0              5.0   \n",
       "8        8          5.0         5.0               5.0              5.0   \n",
       "9        9          5.0         5.0               5.0              5.0   \n",
       "10      10          5.0         5.0               5.0              5.0   \n",
       "11      11          5.0         5.0               5.0              5.0   \n",
       "12      12          5.0         5.0               5.0              5.0   \n",
       "13      13          5.0         5.0               5.0              5.0   \n",
       "14      14          5.0         5.0               5.0              5.0   \n",
       "15      15          5.0         5.0               5.0              5.0   \n",
       "16      16          5.0         5.0               5.0              5.0   \n",
       "17      17          5.0         5.0               5.0              5.0   \n",
       "18      18          5.0         5.0               5.0              5.0   \n",
       "19      19          5.0         5.0               5.0              5.0   \n",
       "\n",
       "    lambda_mean  lambda_var  slots_observed  n_tasks_mec  TaskDist_sum  \\\n",
       "0      5.370000    5.245556           100.0        537.0           1.0   \n",
       "1      4.181818    4.211503            99.0        414.0           1.0   \n",
       "2      4.040404    3.712637            99.0        400.0           1.0   \n",
       "3      3.171717    2.470212            99.0        314.0           1.0   \n",
       "4      3.755102    3.877551            98.0        368.0           1.0   \n",
       "5      3.653061    3.445403            98.0        358.0           1.0   \n",
       "6      3.181818    2.191095            99.0        315.0           1.0   \n",
       "7      4.131313    3.768295            99.0        409.0           1.0   \n",
       "8      4.323232    4.159761            99.0        428.0           1.0   \n",
       "9      3.552083    3.744627            96.0        341.0           1.0   \n",
       "10     3.969697    4.131725            99.0        393.0           1.0   \n",
       "11     3.468085    3.197895            94.0        326.0           1.0   \n",
       "12     4.346939    3.837155            98.0        426.0           1.0   \n",
       "13     3.616162    3.381777            99.0        358.0           1.0   \n",
       "14     3.636364    2.478664            99.0        360.0           1.0   \n",
       "15     4.292929    4.229643            99.0        425.0           1.0   \n",
       "16     4.525253    3.435580            99.0        448.0           1.0   \n",
       "17     5.090000    4.628182           100.0        509.0           1.0   \n",
       "18     4.855670    3.770619            97.0        471.0           1.0   \n",
       "19     3.391753    3.032431            97.0        329.0           1.0   \n",
       "\n",
       "    b_mb_med       rho_med    mem_med  hard_share  P_general  \\\n",
       "0        3.0  1.515893e+09  65.225980    0.320298   0.210428   \n",
       "1        4.0  1.440694e+09  64.346577    0.338164   0.236715   \n",
       "2        3.0  1.487741e+09  63.485611    0.332500   0.240000   \n",
       "3        4.0  1.470488e+09  64.613190    0.343949   0.248408   \n",
       "4        3.0  1.461619e+09  63.199425    0.320652   0.230978   \n",
       "5        3.0  1.451501e+09  61.931205    0.357542   0.243017   \n",
       "6        4.0  1.493120e+09  65.202740    0.377778   0.219048   \n",
       "7        3.0  1.621984e+09  65.171020    0.356968   0.178484   \n",
       "8        3.0  1.441129e+09  63.089418    0.369159   0.221963   \n",
       "9        4.0  1.546929e+09  67.668550    0.340176   0.199413   \n",
       "10       3.0  1.548238e+09  63.100880    0.325700   0.244275   \n",
       "11       3.0  1.496051e+09  66.019930    0.337423   0.193252   \n",
       "12       4.0  1.546243e+09  64.210399    0.361502   0.197183   \n",
       "13       4.0  1.597695e+09  67.242315    0.343575   0.209497   \n",
       "14       3.0  1.494153e+09  65.454258    0.380556   0.222222   \n",
       "15       4.0  1.467142e+09  62.826683    0.322353   0.244706   \n",
       "16       4.0  1.473031e+09  66.614723    0.357143   0.185268   \n",
       "17       3.0  1.458212e+09  65.109270    0.351670   0.233792   \n",
       "18       3.0  1.569169e+09  63.602707    0.343949   0.222930   \n",
       "19       3.0  1.491320e+09  63.580395    0.407295   0.212766   \n",
       "\n",
       "    P_latency_sensitive  P_deadline_hard  P_data_intensive  \\\n",
       "0                   0.0         0.320298          0.074488   \n",
       "1                   0.0         0.338164          0.067633   \n",
       "2                   0.0         0.332500          0.070000   \n",
       "3                   0.0         0.343949          0.073248   \n",
       "4                   0.0         0.320652          0.078804   \n",
       "5                   0.0         0.357542          0.050279   \n",
       "6                   0.0         0.377778          0.060317   \n",
       "7                   0.0         0.356968          0.056235   \n",
       "8                   0.0         0.369159          0.072430   \n",
       "9                   0.0         0.340176          0.064516   \n",
       "10                  0.0         0.325700          0.058524   \n",
       "11                  0.0         0.337423          0.064417   \n",
       "12                  0.0         0.361502          0.077465   \n",
       "13                  0.0         0.343575          0.053073   \n",
       "14                  0.0         0.380556          0.077778   \n",
       "15                  0.0         0.322353          0.087059   \n",
       "16                  0.0         0.357143          0.066964   \n",
       "17                  0.0         0.351670          0.056974   \n",
       "18                  0.0         0.343949          0.072187   \n",
       "19                  0.0         0.407295          0.051672   \n",
       "\n",
       "    P_compute_intensive  non_atomic_share  \n",
       "0              0.394786          0.432030  \n",
       "1              0.357488          0.396135  \n",
       "2              0.357500          0.460000  \n",
       "3              0.334395          0.509554  \n",
       "4              0.369565          0.538043  \n",
       "5              0.349162          0.494413  \n",
       "6              0.342857          0.479365  \n",
       "7              0.408313          0.481663  \n",
       "8              0.336449          0.450935  \n",
       "9              0.395894          0.463343  \n",
       "10             0.371501          0.445293  \n",
       "11             0.404908          0.453988  \n",
       "12             0.363850          0.420188  \n",
       "13             0.393855          0.452514  \n",
       "14             0.319444          0.466667  \n",
       "15             0.345882          0.472941  \n",
       "16             0.390625          0.444196  \n",
       "17             0.357564          0.444008  \n",
       "18             0.360934          0.452229  \n",
       "19             0.328267          0.449848  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Build + quick peek (optional) ----\n",
    "mec_profiles = build_all_mec_profiles(env_configs)\n",
    "\n",
    "print(\"\\n ===EXAMPLE MEC PROFILES===\")\n",
    "display(mec_profiles[\"ep_000\"][\"clustered\"][\"heavy\"].head())\n",
    "\n",
    "# Or directly from env_configs:\n",
    "display(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"mec_profiles\"].head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Delta', 'T_slots', 'K', 'N_mecs', 'topology_type', 'connection_matrix', 'private_cpu', 'public_cpu', 'cloud_cpu', 'episodes', 'arrivals', 'tasks', 'queues_initial', 'action_space', 'state_spec', 'checks', 'mec_profiles'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_id</th>\n",
       "      <th>private_cpu</th>\n",
       "      <th>public_cpu</th>\n",
       "      <th>private_cpu_slot</th>\n",
       "      <th>public_cpu_slot</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_mec</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>5.245556</td>\n",
       "      <td>100.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.515893e+09</td>\n",
       "      <td>65.225980</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.210428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320298</td>\n",
       "      <td>0.074488</td>\n",
       "      <td>0.394786</td>\n",
       "      <td>0.432030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>4.211503</td>\n",
       "      <td>99.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.440694e+09</td>\n",
       "      <td>64.346577</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.236715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>0.357488</td>\n",
       "      <td>0.396135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>3.712637</td>\n",
       "      <td>99.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.487741e+09</td>\n",
       "      <td>63.485611</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.171717</td>\n",
       "      <td>2.470212</td>\n",
       "      <td>99.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.470488e+09</td>\n",
       "      <td>64.613190</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.334395</td>\n",
       "      <td>0.509554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.755102</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>98.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.461619e+09</td>\n",
       "      <td>63.199425</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.230978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.538043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mec_id  private_cpu  public_cpu  private_cpu_slot  public_cpu_slot  \\\n",
       "0       0          5.0         5.0               5.0              5.0   \n",
       "1       1          5.0         5.0               5.0              5.0   \n",
       "2       2          5.0         5.0               5.0              5.0   \n",
       "3       3          5.0         5.0               5.0              5.0   \n",
       "4       4          5.0         5.0               5.0              5.0   \n",
       "\n",
       "   lambda_mean  lambda_var  slots_observed  n_tasks_mec  TaskDist_sum  \\\n",
       "0     5.370000    5.245556           100.0        537.0           1.0   \n",
       "1     4.181818    4.211503            99.0        414.0           1.0   \n",
       "2     4.040404    3.712637            99.0        400.0           1.0   \n",
       "3     3.171717    2.470212            99.0        314.0           1.0   \n",
       "4     3.755102    3.877551            98.0        368.0           1.0   \n",
       "\n",
       "   b_mb_med       rho_med    mem_med  hard_share  P_general  \\\n",
       "0       3.0  1.515893e+09  65.225980    0.320298   0.210428   \n",
       "1       4.0  1.440694e+09  64.346577    0.338164   0.236715   \n",
       "2       3.0  1.487741e+09  63.485611    0.332500   0.240000   \n",
       "3       4.0  1.470488e+09  64.613190    0.343949   0.248408   \n",
       "4       3.0  1.461619e+09  63.199425    0.320652   0.230978   \n",
       "\n",
       "   P_latency_sensitive  P_deadline_hard  P_data_intensive  \\\n",
       "0                  0.0         0.320298          0.074488   \n",
       "1                  0.0         0.338164          0.067633   \n",
       "2                  0.0         0.332500          0.070000   \n",
       "3                  0.0         0.343949          0.073248   \n",
       "4                  0.0         0.320652          0.078804   \n",
       "\n",
       "   P_compute_intensive  non_atomic_share  \n",
       "0             0.394786          0.432030  \n",
       "1             0.357488          0.396135  \n",
       "2             0.357500          0.460000  \n",
       "3             0.334395          0.509554  \n",
       "4             0.369565          0.538043  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mec_id', 'private_cpu', 'public_cpu', 'private_cpu_slot',\n",
      "       'public_cpu_slot', 'lambda_mean', 'lambda_var', 'slots_observed',\n",
      "       'n_tasks_mec', 'TaskDist_sum', 'b_mb_med', 'rho_med', 'mem_med',\n",
      "       'hard_share', 'P_general', 'P_latency_sensitive', 'P_deadline_hard',\n",
      "       'P_data_intensive', 'P_compute_intensive', 'non_atomic_share'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ep   = \"ep_000\"\n",
    "topo = \"clustered\"\n",
    "scen = \"heavy\"\n",
    "\n",
    "env_exp = env_configs[ep][topo][scen]\n",
    "\n",
    "print(env_exp.keys())\n",
    "\n",
    "prof = env_exp[\"mec_profiles\"]\n",
    "display(prof.head())\n",
    "print(prof.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _ensure_dir(path: str):\n",
    "#     \"\"\"Create a folder if it does not already exist.\"\"\"\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def _serialize_non_df_components(env_cfg: dict) -> dict:\n",
    "#     \"\"\"\n",
    "#     Prepare a JSON-serializable dictionary for all non-DataFrame parts\n",
    "#     of env_config. Arrays are converted to lists.\n",
    "#     \"\"\"\n",
    "#     out = {}\n",
    "#     for key, value in env_cfg.items():\n",
    "#         if isinstance(value, pd.DataFrame):\n",
    "#             continue  # handled separately\n",
    "\n",
    "#         # numpy arrays → lists\n",
    "#         if isinstance(value, np.ndarray):\n",
    "#             out[key] = value.tolist()\n",
    "#             continue\n",
    "\n",
    "#         # dicts (queues, action_space, state_spec, checks)\n",
    "#         if isinstance(value, dict):\n",
    "#             try:\n",
    "#                 # recursively convert numpy arrays inside dicts\n",
    "#                 def _convert(obj):\n",
    "#                     if isinstance(obj, np.ndarray):\n",
    "#                         return obj.tolist()\n",
    "#                     if isinstance(obj, dict):\n",
    "#                         return {k: _convert(v) for k, v in obj.items()}\n",
    "#                     return obj\n",
    "#                 out[key] = _convert(value)\n",
    "#             except Exception as e:\n",
    "#                 out[key] = f\"(serialization error: {e})\"\n",
    "#             continue\n",
    "\n",
    "#         # scalars (int, float, str, None)\n",
    "#         if isinstance(value, (int, float, str, bool, type(None))):\n",
    "#             out[key] = value\n",
    "#             continue\n",
    "\n",
    "#         # fallback\n",
    "#         try:\n",
    "#             out[key] = json.loads(json.dumps(value))\n",
    "#         except Exception:\n",
    "#             out[key] = f\"(unserializable type: {type(value).__name__})\"\n",
    "\n",
    "#     return out\n",
    "\n",
    "# def save_all_env_configs(env_configs, out_root: str = \"./artifacts/env_configs\"):\n",
    "#     \"\"\"\n",
    "#     Save all env_configs to disk in a structured layout:\n",
    "#         artifacts/env_configs/ep_xxx/topology/scenario/\n",
    "#             tasks_env_config.csv\n",
    "#             agents_env_config.csv\n",
    "#             arrivals_env_config.csv\n",
    "#             episodes_env_config.csv\n",
    "#             env_meta.json   <-- (non-DF components)\n",
    "#     \"\"\"\n",
    "#     n_saved = 0\n",
    "\n",
    "#     for ep_name, by_topo in env_configs.items():\n",
    "#         for topo_name, by_scen in by_topo.items():\n",
    "#             for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "#                 out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "#                 _ensure_dir(out_dir)\n",
    "\n",
    "#                 # ---- Save DataFrame components ----\n",
    "#                 for df_name, df in env_cfg.items():\n",
    "#                     if isinstance(df, pd.DataFrame):\n",
    "#                         file_path_csv = os.path.join(out_dir, f\"{df_name}_env_config.csv\")\n",
    "#                         df.to_csv(file_path_csv, index=False)\n",
    "\n",
    "#                         print(f\"[saved] {file_path_csv}  (rows={len(df)})\")\n",
    "#                         n_saved += 1\n",
    "\n",
    "#                 # ---- Save non-DataFrame metadata ----\n",
    "#                 meta = _serialize_non_df_components(env_cfg)\n",
    "\n",
    "#                 meta_path = os.path.join(out_dir, \"env_meta.json\")\n",
    "#                 with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#                     json.dump(meta, f, indent=2)\n",
    "\n",
    "#                 print(f\"[saved] {meta_path}\")\n",
    "\n",
    "#     print(f\"\\nDone. Saved {n_saved} DataFrames + meta files for all env_configs.\")\n",
    "\n",
    "# save_all_env_configs(env_configs, out_root=\"./artifacts/env_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 4: Clustering Agents (MECs) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.1. Feature Matrix </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform clustering, the characteristics of each agent must first be stored in a feature matrix. These characteristics include:\n",
    "\n",
    "1) Local resources\n",
    "2) task generation pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEC_FEATURES_V1 = [\n",
    "    # ---- (1) Local resources / capacities ----\n",
    "    \"private_cpu_slot\",     # Private CPU cycles per slot\n",
    "    \"public_cpu_slot\",      # Public CPU cycles per slot\n",
    "\n",
    "    # ---- (2) Arrival statistics ----\n",
    "    \"lambda_mean\",          # Mean task arrival rate (tasks / second)\n",
    "    \"lambda_var\",           # Variance of task arrival rate\n",
    "\n",
    "    # ---- (3) Task generation pattern (probabilities over types) ----\n",
    "    \"P_deadline_hard\",\n",
    "    \"P_latency_sensitive\",\n",
    "    \"P_compute_intensive\",\n",
    "    \"P_data_intensive\",\n",
    "    \"P_general\",\n",
    "\n",
    "    # ---- (4) Statistical descriptors of tasks handled by this MEC ----\n",
    "    \"b_mb_med\",             # Median input size\n",
    "    \"rho_med\",              # Median compute demand (cycles / MB)\n",
    "    \"mem_med\",              # Median memory demand (MB)\n",
    "    \"non_atomic_share\",     # Share of splittable tasks\n",
    "    \"hard_share\",           # Share of hard-deadline tasks\n",
    "]\n",
    "\n",
    "# Features to standardize (others like probabilities are already 0–1)\n",
    "FEATURES_TO_STANDARDIZE = [\n",
    "    \"private_cpu_slot\",\n",
    "    \"public_cpu_slot\",\n",
    "    \"lambda_mean\",\n",
    "    \"lambda_var\",\n",
    "    \"b_mb_med\",\n",
    "    \"rho_med\",\n",
    "    \"mem_med\",\n",
    "]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Utility: keep only existing columns\n",
    "# --------------------------------------------\n",
    "def _safe_cols(df: pd.DataFrame, cols: List[str]) -> List[str]:\n",
    "    \"\"\"Return only those columns from 'cols' that actually exist in df.\"\"\"\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Normalize selected feature columns\n",
    "# --------------------------------------------\n",
    "def normalize_features(\n",
    "    X: np.ndarray,\n",
    "    cols: List[str],\n",
    "    feature_list: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply standardization (zero mean, unit variance) only to selected columns.\n",
    "    \"\"\"\n",
    "    standardize_cols = [col for col in feature_list if col in FEATURES_TO_STANDARDIZE]\n",
    "    if len(standardize_cols) > 0:\n",
    "        scaler = StandardScaler()\n",
    "        col_indices = [cols.index(col) for col in standardize_cols if col in cols]\n",
    "        if col_indices:\n",
    "            X[:, col_indices] = scaler.fit_transform(X[:, col_indices])\n",
    "    return X\n",
    "\n",
    "# --------------------------------------------\n",
    "# Build MEC feature matrix for one env_config\n",
    "# --------------------------------------------\n",
    "def make_mec_feature_matrix_for_env(\n",
    "    env_cfg: Dict[str, Any],\n",
    "    feature_list: Optional[List[str]] = None,\n",
    "    standardize: bool = True,\n",
    ") -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build the feature matrix (X) for all MECs in one environment configuration.\n",
    "    Each row represents a MEC; each column a numerical feature.\n",
    "\n",
    "    Returns:\n",
    "        X_scaled   : np.ndarray (n_mecs × n_features)\n",
    "        used_cols  : list of feature names in order\n",
    "        mec_ids    : np.ndarray of MEC identifiers\n",
    "    \"\"\"\n",
    "    if \"mec_profiles\" not in env_cfg or not isinstance(env_cfg[\"mec_profiles\"], pd.DataFrame):\n",
    "        raise ValueError(\"env_cfg['mec_profiles'] must contain a valid DataFrame.\")\n",
    "\n",
    "    prof = env_cfg[\"mec_profiles\"].copy()\n",
    "    if \"mec_id\" not in prof.columns:\n",
    "        raise ValueError(\"mec_profiles must include column 'mec_id'.\")\n",
    "\n",
    "    if feature_list is None:\n",
    "        feature_list = MEC_FEATURES_V1\n",
    "\n",
    "    # Keep valid features and fill missing values with zeros\n",
    "    cols = _safe_cols(prof, feature_list)\n",
    "    X = prof.reindex(columns=cols).fillna(0.0).astype(float).to_numpy()\n",
    "    mec_ids = prof[\"mec_id\"].to_numpy(dtype=int)\n",
    "\n",
    "    # Standardize selected features\n",
    "    if standardize:\n",
    "        X = normalize_features(X, cols, feature_list)\n",
    "\n",
    "    return X, cols, mec_ids\n",
    "\n",
    "# --------------------------------------------\n",
    "# Attach MEC features to a single env_config\n",
    "# --------------------------------------------\n",
    "def attach_mec_features_to_env(\n",
    "    env_cfg: Dict[str, Any],\n",
    "    feature_list: Optional[List[str]] = None,\n",
    "    standardize: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Attach the constructed MEC feature matrix and metadata to:\n",
    "        env_cfg[\"clustering\"][\"features\"]\n",
    "    \"\"\"\n",
    "    X, cols, mec_ids = make_mec_feature_matrix_for_env(env_cfg, feature_list, standardize)\n",
    "\n",
    "    env_cfg.setdefault(\"clustering\", {})\n",
    "    env_cfg[\"clustering\"][\"features\"] = {\n",
    "        \"X\": X,                      # Feature matrix (possibly standardized)\n",
    "        \"feature_cols\": cols,        # List of feature names\n",
    "        \"mec_ids\": mec_ids,          # MEC identifiers\n",
    "        \"n_mecs\": int(X.shape[0]),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "    return env_cfg\n",
    "\n",
    "# --------------------------------------------\n",
    "# Apply feature construction to all envs\n",
    "# --------------------------------------------\n",
    "def attach_mec_features_to_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    feature_list: Optional[List[str]] = None,\n",
    "    standardize: bool = True,\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Iterate through all (episode → topology → scenario) combinations\n",
    "    and build the MEC feature matrix for each one.\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                env_configs[ep_name][topo_name][scen_name] = attach_mec_features_to_env(\n",
    "                    env_cfg, feature_list, standardize\n",
    "                )\n",
    "                fz = env_configs[ep_name][topo_name][scen_name][\"clustering\"][\"features\"]\n",
    "                print(\n",
    "                    f\"[features] {ep_name}/{topo_name}/{scen_name} \"\n",
    "                    f\"-> X.shape={fz['X'].shape}  (mecs={fz['n_mecs']}, feats={fz['n_features']})\"\n",
    "                )\n",
    "    return env_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Sanity checks\n",
    "# --------------------------------------------\n",
    "def _assert_no_nan_inf(X: np.ndarray, where: str):\n",
    "    if not np.isfinite(X).all():\n",
    "        n_nan = int(np.isnan(X).sum())\n",
    "        n_inf = int(np.isinf(X).sum())\n",
    "        raise AssertionError(f\"{where}: Feature matrix contains NaN or Inf. counts=(NaN={n_nan}, Inf={n_inf})\")\n",
    "\n",
    "def _assert_mec_count_match(env_cfg: Dict[str, Any], where: str):\n",
    "    \"\"\"\n",
    "    Check that:\n",
    "      - episodes.N_mecs\n",
    "      - mec_profiles rows\n",
    "      - feature matrix rows\n",
    "    all match.\n",
    "    \"\"\"\n",
    "    if \"episodes\" not in env_cfg or \"mec_profiles\" not in env_cfg:\n",
    "        raise AssertionError(f\"{where}: Missing episodes or mec_profiles in env_cfg.\")\n",
    "\n",
    "    n_mecs_ep = int(env_cfg[\"episodes\"][\"N_mecs\"].iloc[0])\n",
    "    n_mecs_prof = len(env_cfg[\"mec_profiles\"])\n",
    "    fz = env_cfg[\"clustering\"][\"features\"]\n",
    "    n_mecs_X = fz[\"n_mecs\"]\n",
    "\n",
    "    if not (n_mecs_X == n_mecs_prof == n_mecs_ep):\n",
    "        raise AssertionError(\n",
    "            f\"{where}: MEC count mismatch. episodes={n_mecs_ep}, \"\n",
    "            f\"profiles={n_mecs_prof}, X={n_mecs_X}\"\n",
    "        )\n",
    "\n",
    "def _assert_feature_prob_sum_hint_mec(env_cfg: Dict[str, Any], tol: float = 1e-3):\n",
    "    \"\"\"\n",
    "    Hint check: for MECs that have tasks, TaskDist_sum ≈ 1 on average.\n",
    "    \"\"\"\n",
    "    prof = env_cfg[\"mec_profiles\"]\n",
    "    if \"TaskDist_sum\" in prof.columns and \"n_tasks_mec\" in prof.columns:\n",
    "        mask = prof[\"n_tasks_mec\"] > 0\n",
    "        if mask.any():\n",
    "            mean_sum = float(prof.loc[mask, \"TaskDist_sum\"].mean())\n",
    "            if abs(mean_sum - 1.0) > tol:\n",
    "                print(f\"[warn] Mean(TaskDist_sum)={mean_sum:.4f} ≠ 1 (tol={tol})\")\n",
    "\n",
    "def run_mec_feature_matrix_sanity_checks(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Run sanity checks over all MEC feature matrices in env_configs.\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                where = f\"{ep_name}/{topo_name}/{scen_name}\"\n",
    "                if \"clustering\" not in env_cfg or \"features\" not in env_cfg[\"clustering\"]:\n",
    "                    raise AssertionError(f\"{where}: Missing clustering.features.\")\n",
    "                X = env_cfg[\"clustering\"][\"features\"][\"X\"]\n",
    "                _assert_no_nan_inf(X, where)\n",
    "                _assert_mec_count_match(env_cfg, where)\n",
    "                _assert_feature_prob_sum_hint_mec(env_cfg)\n",
    "                if X.shape[1] == 0:\n",
    "                    raise AssertionError(f\"{where}: Empty feature matrix.\")\n",
    "    print(\"[checks] All MEC feature matrices passed sanity checks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features] ep_000/clustered/heavy -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/clustered/light -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/clustered/moderate -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/fully_connected/heavy -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/fully_connected/light -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/fully_connected/moderate -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/skip_connections/heavy -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/skip_connections/light -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[features] ep_000/skip_connections/moderate -> X.shape=(20, 14)  (mecs=20, feats=14)\n",
      "[checks] All MEC feature matrices passed sanity checks.\n",
      "\n",
      "=== EXAMPLE: MEC features of ep_000 / clustered / heavy ===\n",
      "X.shape: (20, 14)\n",
      "feature_cols: ['private_cpu_slot', 'public_cpu_slot', 'lambda_mean', 'lambda_var', 'P_deadline_hard', 'P_latency_sensitive', 'P_compute_intensive', 'P_data_intensive', 'P_general', 'b_mb_med', 'rho_med', 'mem_med', 'non_atomic_share', 'hard_share']\n",
      "mec_ids (first 10): [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Build feature matrices for all envs\n",
    "env_configs = attach_mec_features_to_all_envs(\n",
    "    env_configs,\n",
    "    feature_list=MEC_FEATURES_V1,\n",
    "    standardize=True\n",
    ")\n",
    "\n",
    "# Run sanity checks\n",
    "run_mec_feature_matrix_sanity_checks(env_configs)\n",
    "\n",
    "# Example inspection\n",
    "print(\"\\n=== EXAMPLE: MEC features of ep_000 / clustered / heavy ===\")\n",
    "fz = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"features\"]\n",
    "print(\"X.shape:\", fz[\"X\"].shape)\n",
    "print(\"feature_cols:\", fz[\"feature_cols\"])\n",
    "print(\"mec_ids (first 10):\", fz[\"mec_ids\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing features: []\n",
      "   private_cpu_slot  public_cpu_slot  lambda_mean  lambda_var  \\\n",
      "0               5.0              5.0     5.370000    5.245556   \n",
      "1               5.0              5.0     4.181818    4.211503   \n",
      "2               5.0              5.0     4.040404    3.712637   \n",
      "3               5.0              5.0     3.171717    2.470212   \n",
      "4               5.0              5.0     3.755102    3.877551   \n",
      "\n",
      "   P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "0         0.320298                  0.0             0.394786   \n",
      "1         0.338164                  0.0             0.357488   \n",
      "2         0.332500                  0.0             0.357500   \n",
      "3         0.343949                  0.0             0.334395   \n",
      "4         0.320652                  0.0             0.369565   \n",
      "\n",
      "   P_data_intensive  P_general  b_mb_med       rho_med    mem_med  \\\n",
      "0          0.074488   0.210428       3.0  1.515893e+09  65.225980   \n",
      "1          0.067633   0.236715       4.0  1.440694e+09  64.346577   \n",
      "2          0.070000   0.240000       3.0  1.487741e+09  63.485611   \n",
      "3          0.073248   0.248408       4.0  1.470488e+09  64.613190   \n",
      "4          0.078804   0.230978       3.0  1.461619e+09  63.199425   \n",
      "\n",
      "   non_atomic_share  hard_share  \n",
      "0          0.432030    0.320298  \n",
      "1          0.396135    0.338164  \n",
      "2          0.460000    0.332500  \n",
      "3          0.509554    0.343949  \n",
      "4          0.538043    0.320652  \n"
     ]
    }
   ],
   "source": [
    "print(\"missing features:\",\n",
    "      [c for c in MEC_FEATURES_V1 if c not in prof.columns])\n",
    "\n",
    "print(prof[ [c for c in MEC_FEATURES_V1 if c in prof.columns] ].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (20, 14)\n",
      "feature_cols actually used: ['private_cpu_slot', 'public_cpu_slot', 'lambda_mean', 'lambda_var', 'P_deadline_hard', 'P_latency_sensitive', 'P_compute_intensive', 'P_data_intensive', 'P_general', 'b_mb_med', 'rho_med', 'mem_med', 'non_atomic_share', 'hard_share']\n",
      "mec_ids[:10]: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "fz = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"features\"]\n",
    "print(\"X.shape:\", fz[\"X\"].shape)\n",
    "print(\"feature_cols actually used:\", fz[\"feature_cols\"])\n",
    "print(\"mec_ids[:10]:\", fz[\"mec_ids\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.2. Optimal Number of Clusters </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the optimal number of clusters (K), we use a hybrid method that combines evaluation indices such as WCSS, Silhouette, DBI, and CH Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4.2.1 Candidate K values ----------\n",
    "def _candidate_K_values(\n",
    "    n_mecs: int,\n",
    "    k_min: int = 2,\n",
    "    max_K_fraction: float = 0.25,\n",
    "    max_K_abs: int = 10\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a reasonable candidate set for K given n_mecs.\n",
    "\n",
    "    - Lower bound is k_min (default 2).\n",
    "    - Upper bound is min(max_K_abs, floor(max_K_fraction * n_mecs), n_mecs - 1).\n",
    "    - If n_mecs is too small, returns an empty list.\n",
    "    \"\"\"\n",
    "    if n_mecs <= k_min:\n",
    "        return []\n",
    "\n",
    "    k_max_by_fraction = int(np.floor(max_K_fraction * n_mecs))\n",
    "    k_max = min(max_K_abs, n_mecs - 1, max(k_min, k_max_by_fraction))\n",
    "\n",
    "    if k_max < k_min:\n",
    "        return []\n",
    "\n",
    "    return list(range(k_min, k_max + 1))\n",
    "\n",
    "# ---------- 4.2.2 Evaluate KMeans for a single K ----------\n",
    "def _evaluate_kmeans_for_K(\n",
    "    X: np.ndarray,\n",
    "    K: int,\n",
    "    random_state: int = 42\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run KMeans for a given K and compute clustering metrics.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"K\": int,\n",
    "          \"inertia\": float,\n",
    "          \"silhouette\": float or np.nan,\n",
    "          \"davies_bouldin\": float or np.nan,\n",
    "          \"calinski_harabasz\": float or np.nan,\n",
    "        }\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    result = {\n",
    "        \"K\": int(K),\n",
    "        \"inertia\": np.nan,\n",
    "        \"silhouette\": np.nan,\n",
    "        \"davies_bouldin\": np.nan,\n",
    "        \"calinski_harabasz\": np.nan,\n",
    "    }\n",
    "\n",
    "    if K <= 1 or K > n_samples:\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        km = KMeans(\n",
    "            n_clusters=K,\n",
    "            random_state=random_state,\n",
    "            n_init=\"auto\"\n",
    "        )\n",
    "        labels = km.fit_predict(X)\n",
    "        result[\"inertia\"] = float(km.inertia_)\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "        if unique_labels.shape[0] > 1:\n",
    "            # Silhouette\n",
    "            try:\n",
    "                result[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"silhouette\"] = np.nan\n",
    "\n",
    "            # Davies–Bouldin\n",
    "            try:\n",
    "                result[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"davies_bouldin\"] = np.nan\n",
    "\n",
    "            # Calinski–Harabasz\n",
    "            try:\n",
    "                result[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"calinski_harabasz\"] = np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] KMeans failed for K={K}: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- 4.2.3 Min-max normalization ----------\n",
    "def _min_max_normalize(\n",
    "    arr: np.ndarray,\n",
    "    invert: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Min-max normalize a 1D array to [0, 1].\n",
    "\n",
    "    - If all values are NaN or the range is zero, returns NaN array.\n",
    "    - If invert=True, larger original values map to lower normalized ones.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    if np.all(np.isnan(arr)):\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    valid = ~np.isnan(arr)\n",
    "    if valid.sum() <= 1:\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    vmin = np.nanmin(arr[valid])\n",
    "    vmax = np.nanmax(arr[valid])\n",
    "    if vmax - vmin == 0:\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    norm = (arr - vmin) / (vmax - vmin)\n",
    "    if invert:\n",
    "        norm = 1.0 - norm\n",
    "    return norm\n",
    "\n",
    "# ---------- 4.2.4 Add composite score ----------\n",
    "def _add_composite_score(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.4,\n",
    "    gamma: float = 0.2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given metrics_df with:\n",
    "        K, inertia, silhouette, calinski_harabasz, davies_bouldin\n",
    "\n",
    "    Add:\n",
    "        sil_norm, ch_norm, db_norm, score\n",
    "    where:\n",
    "        score = alpha * sil_norm + beta * ch_norm - gamma * db_norm\n",
    "    \"\"\"\n",
    "    df = metrics_df.copy().reset_index(drop=True)\n",
    "\n",
    "    sil = df[\"silhouette\"].to_numpy(dtype=float)\n",
    "    ch  = df[\"calinski_harabasz\"].to_numpy(dtype=float)\n",
    "    db  = df[\"davies_bouldin\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Silhouette: higher is better\n",
    "    df[\"sil_norm\"] = _min_max_normalize(sil, invert=False)\n",
    "\n",
    "    # Calinski–Harabasz: higher is better\n",
    "    df[\"ch_norm\"] = _min_max_normalize(ch, invert=False)\n",
    "\n",
    "    # Davies–Bouldin: lower is better → we subtract db_norm in the score\n",
    "    df[\"db_norm\"] = _min_max_normalize(db, invert=False)\n",
    "\n",
    "    df[\"score\"] = (\n",
    "        alpha * df[\"sil_norm\"].fillna(0.0)\n",
    "        + beta * df[\"ch_norm\"].fillna(0.0)\n",
    "        - gamma * df[\"db_norm\"].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- 4.2.5 Elbow score from curvature ----------\n",
    "def _compute_elbow_rank(metrics_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a simple 'elbow_score' based on curvature of inertia vs K.\n",
    "\n",
    "    - Normalize inertia to [0,1].\n",
    "    - Use discrete second derivative:\n",
    "        curvature_i ≈ |y_{i-1} - 2*y_i + y_{i+1}|\n",
    "    - Endpoints get curvature 0.\n",
    "    \"\"\"\n",
    "    df = metrics_df.sort_values(\"K\").reset_index(drop=True)\n",
    "    inertia = df[\"inertia\"].to_numpy(dtype=float)\n",
    "\n",
    "    if inertia.shape[0] < 3:\n",
    "        return np.zeros_like(inertia, dtype=float)\n",
    "\n",
    "    inertia_norm = _min_max_normalize(inertia, invert=False)\n",
    "    if np.all(np.isnan(inertia_norm)):\n",
    "        return np.zeros_like(inertia, dtype=float)\n",
    "\n",
    "    curv = np.zeros_like(inertia_norm, dtype=float)\n",
    "    for i in range(1, len(inertia_norm) - 1):\n",
    "        y_prev = inertia_norm[i - 1]\n",
    "        y_curr = inertia_norm[i]\n",
    "        y_next = inertia_norm[i + 1]\n",
    "        if np.isnan(y_prev) or np.isnan(y_curr) or np.isnan(y_next):\n",
    "            curv[i] = 0.0\n",
    "        else:\n",
    "            curv[i] = abs(y_prev - 2.0 * y_curr + y_next)\n",
    "\n",
    "    curv_norm = _min_max_normalize(curv, invert=False)\n",
    "    curv_norm = np.nan_to_num(curv_norm, nan=0.0)\n",
    "    return curv_norm\n",
    "\n",
    "# ---------- 4.2.6 Select best_K and K_elbow ----------\n",
    "def _select_best_K_from_df(metrics_with_scores: pd.DataFrame) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Given metrics_with_scores with columns:\n",
    "        K, score, elbow_score\n",
    "\n",
    "    Returns:\n",
    "        best_K  : K with max composite score  (tie → smallest K)\n",
    "        K_elbow : K with max elbow_score     (tie → smallest K)\n",
    "    \"\"\"\n",
    "    df = metrics_with_scores.sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "    # best_K from composite score\n",
    "    if df[\"score\"].notna().any():\n",
    "        idx_best = df[\"score\"].idxmax()\n",
    "    else:\n",
    "        idx_best = df[\"K\"].idxmin()\n",
    "    best_K = int(df.loc[idx_best, \"K\"])\n",
    "\n",
    "    # K_elbow from elbow_score\n",
    "    if \"elbow_score\" in df.columns and df[\"elbow_score\"].notna().any():\n",
    "        idx_elb = df[\"elbow_score\"].idxmax()\n",
    "    else:\n",
    "        idx_elb = idx_best\n",
    "    K_elbow = int(df.loc[idx_elb, \"K\"])\n",
    "\n",
    "    return best_K, K_elbow\n",
    "\n",
    "# ---------- 4.2.7 Plot elbow + composite score ----------\n",
    "def _plot_elbow_and_scores(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    ep_name: str,\n",
    "    topo_name: str,\n",
    "    scen_name: str,\n",
    "    out_root: str = \"./artifacts/clustering\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Plot:\n",
    "      - inertia (WCSS) vs K  → classic elbow\n",
    "      - composite score vs K\n",
    "\n",
    "    Save under:\n",
    "      ./artifacts/clustering/<ep>/<topology>/<scenario>/elbow_and_score.png\n",
    "    \"\"\"\n",
    "    df = metrics_df.sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "    Ks      = df[\"K\"].to_numpy(dtype=int)\n",
    "    inertia = df[\"inertia\"].to_numpy(dtype=float)\n",
    "    scores  = df[\"score\"].to_numpy(dtype=float)\n",
    "\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"elbow_and_score.png\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Left axis: inertia (WCSS)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(Ks, inertia, marker=\"o\", linestyle=\"-\", label=\"WCSS (inertia)\")\n",
    "    ax1.set_xlabel(\"Number of clusters K\")\n",
    "    ax1.set_ylabel(\"WCSS (inertia)\")\n",
    "\n",
    "    # Right axis: composite score\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(Ks, scores, marker=\"s\", linestyle=\"--\", label=\"Composite score\")\n",
    "    ax2.set_ylabel(\"Composite score\")\n",
    "\n",
    "    title = f\"Elbow & Score: {ep_name} / {topo_name} / {scen_name}\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# ---------- 4.2.8 Main driver over all env_configs ----------\n",
    "def step4_2_select_K_for_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    random_state: int = 42,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.4,\n",
    "    gamma: float = 0.2,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Step 4.2: K-selection for all environments (MEC clustering).\n",
    "\n",
    "    For each:\n",
    "        env_configs[ep_name][topology_name][scenario_name][\"clustering\"][\"features\"][\"X\"]\n",
    "    we:\n",
    "      - build candidate K list (based on n_mecs = X.shape[0])\n",
    "      - run KMeans for each K\n",
    "      - compute metrics + composite score\n",
    "      - compute elbow_score\n",
    "      - select best_K and K_elbow\n",
    "      - plot & save elbow figure\n",
    "      - store results in:\n",
    "            env_cfg[\"clustering\"][\"k_selection\"]\n",
    "\n",
    "    Returns:\n",
    "      K_selection[ep_name][topology_name][scenario_name] = {\n",
    "        \"best_K\", \"K_elbow\", \"metrics_df\", \"elbow_plot_path\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    K_selection: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        K_selection[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            K_selection[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                # Check clustering features exist\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"no clustering features found.\")\n",
    "                    continue\n",
    "\n",
    "                X = np.asarray(feats[\"X\"], dtype=float)\n",
    "                if X.ndim != 2 or X.shape[0] == 0:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"empty or invalid feature matrix.\")\n",
    "                    continue\n",
    "\n",
    "                n_mecs = X.shape[0]\n",
    "                K_candidates = _candidate_K_values(n_mecs)\n",
    "                if not K_candidates:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"not enough MECs for clustering (n_mecs={n_mecs}).\")\n",
    "                    continue\n",
    "\n",
    "                # 1) Evaluate KMeans for all candidate K\n",
    "                metrics_list = []\n",
    "                for K in K_candidates:\n",
    "                    m = _evaluate_kmeans_for_K(X, K, random_state=random_state)\n",
    "                    metrics_list.append(m)\n",
    "                metrics_df_raw = pd.DataFrame(metrics_list).sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "                # 2) Add composite score\n",
    "                metrics_df_full = _add_composite_score(\n",
    "                    metrics_df_raw,\n",
    "                    alpha=alpha,\n",
    "                    beta=beta,\n",
    "                    gamma=gamma\n",
    "                )\n",
    "\n",
    "                # 3) Add elbow_score\n",
    "                metrics_df_full[\"elbow_score\"] = _compute_elbow_rank(metrics_df_full)\n",
    "\n",
    "                # 4) Select best_K and K_elbow\n",
    "                best_K, K_elbow = _select_best_K_from_df(metrics_df_full)\n",
    "\n",
    "                # 5) Plot elbow + composite score\n",
    "                elbow_plot_path = _plot_elbow_and_scores(\n",
    "                    metrics_df_full,\n",
    "                    ep_name=ep_name,\n",
    "                    topo_name=topo_name,\n",
    "                    scen_name=scen_name,\n",
    "                    out_root=\"./artifacts/clustering\"\n",
    "                )\n",
    "\n",
    "                # 6) Attach to env_config\n",
    "                env_cfg.setdefault(\"clustering\", {})\n",
    "                env_cfg[\"clustering\"][\"k_selection\"] = {\n",
    "                    \"metrics_df\": metrics_df_full,\n",
    "                    \"best_K\": best_K,\n",
    "                    \"K_elbow\": K_elbow,\n",
    "                    \"elbow_plot_path\": elbow_plot_path,\n",
    "                }\n",
    "\n",
    "                # 7) Record summary\n",
    "                K_selection[ep_name][topo_name][scen_name] = {\n",
    "                    \"best_K\": best_K,\n",
    "                    \"K_elbow\": K_elbow,\n",
    "                    \"metrics_df\": metrics_df_full,\n",
    "                    \"elbow_plot_path\": elbow_plot_path,\n",
    "                }\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[4.2] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                          f\"n_mecs={n_mecs}, candidates={K_candidates}\")\n",
    "                    print(f\"      → best_K  (composite score) = {best_K}\")\n",
    "                    print(f\"      → K_elbow (inertia elbow)    = {K_elbow}\")\n",
    "                    print(f\"      → elbow plot saved at: {elbow_plot_path}\")\n",
    "                    cols_show = [\n",
    "                        \"K\", \"inertia\", \"silhouette\",\n",
    "                        \"calinski_harabasz\", \"davies_bouldin\", \"score\"\n",
    "                    ]\n",
    "                    print(metrics_df_full[cols_show].round(4))\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "    return K_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.2] ep_000/clustered/heavy: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/clustered/light: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 4\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/clustered/moderate: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\moderate\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/fully_connected/heavy: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\fully_connected\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/fully_connected/light: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 4\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\fully_connected\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/fully_connected/moderate: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\fully_connected\\moderate\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/skip_connections/heavy: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\skip_connections\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/skip_connections/light: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 4\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\skip_connections\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/skip_connections/moderate: n_mecs=20, candidates=[2, 3, 4, 5]\n",
      "      → best_K  (composite score) = 5\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\skip_connections\\moderate\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== STEP 4.2 EXAMPLE: ep_000 / clustered / heavy ===\n",
      "Chosen best_K  (composite score): 5\n",
      "Elbow-based K_elbow             : 3\n",
      "Elbow plot path                 : ./artifacts/clustering\\ep_000\\clustered\\heavy\\elbow_and_score.png\n",
      "\n",
      "Metrics per K:\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4.2.9 Example driver ----------\n",
    "# After Step 4.1 (attach_mec_features_to_all_envs + sanity checks), run:\n",
    "K_selection = step4_2_select_K_for_all_envs(\n",
    "    env_configs,\n",
    "    random_state=42,\n",
    "    alpha=0.4,\n",
    "    beta=0.4,\n",
    "    gamma=0.2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== STEP 4.2 EXAMPLE: ep_000 / clustered / heavy ===\")\n",
    "ex_ep   = \"ep_000\"\n",
    "ex_topo = \"clustered\"\n",
    "ex_scen = \"heavy\"\n",
    "\n",
    "if (ex_ep in K_selection and\n",
    "    ex_topo in K_selection[ex_ep] and\n",
    "    ex_scen in K_selection[ex_ep][ex_topo]):\n",
    "\n",
    "    ex_sel = K_selection[ex_ep][ex_topo][ex_scen]\n",
    "    print(\"Chosen best_K  (composite score):\", ex_sel[\"best_K\"])\n",
    "    print(\"Elbow-based K_elbow             :\", ex_sel[\"K_elbow\"])\n",
    "    print(\"Elbow plot path                 :\", ex_sel[\"elbow_plot_path\"])\n",
    "    print(\"\\nMetrics per K:\")\n",
    "    print(\n",
    "        ex_sel[\"metrics_df\"][\n",
    "            [\"K\", \"inertia\", \"silhouette\", \"calinski_harabasz\", \"davies_bouldin\", \"score\"]\n",
    "        ].round(4)\n",
    "    )\n",
    "else:\n",
    "    print(\"[warn] Example triple (ep_000/clustered/heavy) not found in K_selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkups !!! (the charts are alike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. profile differences between light/moderate/heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_light vs X_heavy allclose: False\n",
      "X_light vs X_mod   allclose: False\n",
      "shapes: (20, 14) (20, 14) (20, 14)\n"
     ]
    }
   ],
   "source": [
    "ep   = \"ep_000\"\n",
    "topo = \"clustered\"\n",
    "\n",
    "X_light = env_configs[ep][topo][\"light\"][\"clustering\"][\"features\"][\"X\"]\n",
    "X_mod   = env_configs[ep][topo][\"moderate\"][\"clustering\"][\"features\"][\"X\"]\n",
    "X_heavy = env_configs[ep][topo][\"heavy\"][\"clustering\"][\"features\"][\"X\"]\n",
    "\n",
    "print(\"X_light vs X_heavy allclose:\", np.allclose(X_light, X_heavy))\n",
    "print(\"X_light vs X_mod   allclose:\", np.allclose(X_light, X_mod))\n",
    "print(\"shapes:\", X_light.shape, X_mod.shape, X_heavy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEC profiles equal (light vs heavy): False\n",
      "MEC profiles equal (light vs mod)  : False\n"
     ]
    }
   ],
   "source": [
    "prof_light = env_configs[ep][topo][\"light\"][\"mec_profiles\"]\n",
    "prof_mod   = env_configs[ep][topo][\"moderate\"][\"mec_profiles\"]\n",
    "prof_heavy = env_configs[ep][topo][\"heavy\"][\"mec_profiles\"]\n",
    "\n",
    "print(\"MEC profiles equal (light vs heavy):\", prof_light.equals(prof_heavy))\n",
    "print(\"MEC profiles equal (light vs mod)  :\", prof_light.equals(prof_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Distributions differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ep_000 / clustered / light ===\n",
      "\n",
      "λ (arrival rate) stats per MEC:\n",
      "       lambda_mean  lambda_var\n",
      "count    20.000000   20.000000\n",
      "mean      1.568355    0.623132\n",
      "std       0.093343    0.146317\n",
      "min       1.432836    0.421186\n",
      "25%       1.513584    0.538938\n",
      "50%       1.544866    0.604022\n",
      "75%       1.636029    0.717899\n",
      "max       1.770492    0.924038\n",
      "\n",
      "P(task_type) stats per MEC:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        20.000000                 20.0            20.000000   \n",
      "mean          0.149266                  0.0             0.469301   \n",
      "std           0.027712                  0.0             0.053526   \n",
      "min           0.104167                  0.0             0.367521   \n",
      "25%           0.129400                  0.0             0.433256   \n",
      "50%           0.147469                  0.0             0.472747   \n",
      "75%           0.163302                  0.0             0.513641   \n",
      "max           0.216495                  0.0             0.544554   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         20.000000  20.000000  \n",
      "mean           0.094400   0.287033  \n",
      "std            0.037741   0.044469  \n",
      "min            0.039604   0.216495  \n",
      "25%            0.065076   0.258446  \n",
      "50%            0.093387   0.285721  \n",
      "75%            0.116121   0.309508  \n",
      "max            0.172414   0.415094  \n",
      "\n",
      "median task resource stats per MEC:\n",
      "        b_mb_med       rho_med    mem_med\n",
      "count  20.000000  2.000000e+01  20.000000\n",
      "mean    3.500000  9.984182e+08  64.611027\n",
      "std     0.512989  4.967684e+07   3.049476\n",
      "min     3.000000  9.335972e+08  56.502741\n",
      "25%     3.000000  9.630642e+08  62.791372\n",
      "50%     3.500000  9.837006e+08  65.410967\n",
      "75%     4.000000  1.023725e+09  66.684501\n",
      "max     4.000000  1.091533e+09  69.099990\n",
      "\n",
      "=== ep_000 / clustered / moderate ===\n",
      "\n",
      "λ (arrival rate) stats per MEC:\n",
      "       lambda_mean  lambda_var\n",
      "count    20.000000   20.000000\n",
      "mean      2.700963    2.192819\n",
      "std       0.278494    0.448741\n",
      "min       2.258065    1.531943\n",
      "25%       2.479513    1.853210\n",
      "50%       2.668144    2.156473\n",
      "75%       2.960106    2.342338\n",
      "max       3.127660    3.155928\n",
      "\n",
      "P(task_type) stats per MEC:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        20.000000                 20.0            20.000000   \n",
      "mean          0.247695                  0.0             0.419712   \n",
      "std           0.028332                  0.0             0.037398   \n",
      "min           0.193548                  0.0             0.347619   \n",
      "25%           0.234679                  0.0             0.398166   \n",
      "50%           0.247468                  0.0             0.426786   \n",
      "75%           0.270387                  0.0             0.444535   \n",
      "max           0.290476                  0.0             0.483333   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         20.000000  20.000000  \n",
      "mean           0.083724   0.248869  \n",
      "std            0.022678   0.038921  \n",
      "min            0.051793   0.168142  \n",
      "25%            0.062589   0.225605  \n",
      "50%            0.088849   0.245670  \n",
      "75%            0.093306   0.269097  \n",
      "max            0.141667   0.336323  \n",
      "\n",
      "median task resource stats per MEC:\n",
      "        b_mb_med       rho_med    mem_med\n",
      "count  20.000000  2.000000e+01  20.000000\n",
      "mean    3.500000  1.217954e+09  63.981149\n",
      "std     0.512989  3.713667e+07   1.994866\n",
      "min     3.000000  1.164596e+09  59.589726\n",
      "25%     3.000000  1.178843e+09  63.071701\n",
      "50%     3.500000  1.217389e+09  64.100980\n",
      "75%     4.000000  1.244042e+09  64.718448\n",
      "max     4.000000  1.289236e+09  67.552790\n",
      "\n",
      "=== ep_000 / clustered / heavy ===\n",
      "\n",
      "λ (arrival rate) stats per MEC:\n",
      "       lambda_mean  lambda_var\n",
      "count    20.000000   20.000000\n",
      "mean      4.027670    3.647516\n",
      "std       0.610873    0.740264\n",
      "min       3.171717    2.191095\n",
      "25%       3.600142    3.335807\n",
      "50%       4.005051    3.756461\n",
      "75%       4.329159    4.138734\n",
      "max       5.370000    5.245556\n",
      "\n",
      "P(task_type) stats per MEC:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        20.000000                 20.0            20.000000   \n",
      "mean          0.349418                  0.0             0.364162   \n",
      "std           0.022424                  0.0             0.026475   \n",
      "min           0.320298                  0.0             0.319444   \n",
      "25%           0.336192                  0.0             0.345126   \n",
      "50%           0.343949                  0.0             0.359249   \n",
      "75%           0.358532                  0.0             0.391432   \n",
      "max           0.407295                  0.0             0.408313   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         20.000000  20.000000  \n",
      "mean           0.066703   0.219717  \n",
      "std            0.010210   0.021010  \n",
      "min            0.050279   0.178484  \n",
      "25%            0.058137   0.206976  \n",
      "50%            0.067299   0.222092  \n",
      "75%            0.073558   0.237536  \n",
      "max            0.087059   0.248408  \n",
      "\n",
      "median task resource stats per MEC:\n",
      "        b_mb_med       rho_med    mem_med\n",
      "count  20.000000  2.000000e+01  20.000000\n",
      "mean    3.400000  1.503618e+09  64.584764\n",
      "std     0.502625  5.202352e+07   1.535355\n",
      "min     3.000000  1.440694e+09  61.931205\n",
      "25%     3.000000  1.465761e+09  63.414065\n",
      "50%     3.000000  1.492220e+09  64.479883\n",
      "75%     4.000000  1.546415e+09  65.283050\n",
      "max     4.000000  1.621984e+09  67.668550\n"
     ]
    }
   ],
   "source": [
    "targets = [\n",
    "    (\"clustered\", \"light\"),\n",
    "    (\"clustered\", \"moderate\"),\n",
    "    (\"clustered\", \"heavy\"),\n",
    "]\n",
    "\n",
    "for topo, scen in targets:\n",
    "    print(f\"\\n=== {ep} / {topo} / {scen} ===\")\n",
    "    prof = env_configs[ep][topo][scen][\"mec_profiles\"]\n",
    "\n",
    "    print(\"\\nλ (arrival rate) stats per MEC:\")\n",
    "    print(prof[[\"lambda_mean\", \"lambda_var\"]].describe())\n",
    "\n",
    "    print(\"\\nP(task_type) stats per MEC:\")\n",
    "    cols_p = [f\"P_{t}\" for t in [\n",
    "        \"deadline_hard\",\n",
    "        \"latency_sensitive\",\n",
    "        \"compute_intensive\",\n",
    "        \"data_intensive\",\n",
    "        \"general\"\n",
    "    ]]\n",
    "    existing_p = [c for c in cols_p if c in prof.columns]\n",
    "    print(prof[existing_p].describe())\n",
    "\n",
    "    print(\"\\nmedian task resource stats per MEC:\")\n",
    "    cols_med = [c for c in [\"b_mb_med\",\"rho_med\",\"mem_med\"] if c in prof.columns]\n",
    "    print(prof[cols_med].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. metrics similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== clustered / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "\n",
      "=== clustered / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "\n",
      "=== clustered / moderate ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n",
      "\n",
      "=== fully_connected / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "\n",
      "=== fully_connected / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "\n",
      "=== fully_connected / moderate ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n",
      "\n",
      "=== skip_connections / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  77.8353      0.1832             5.1399          1.8137 -0.2000\n",
      "1  3  59.1103      0.2025             5.8887          1.4353  0.0871\n",
      "2  4  49.7196      0.2122             5.4001          1.2794  0.0798\n",
      "3  5  33.1977      0.2890             7.5529          0.9717  0.8000\n",
      "\n",
      "=== skip_connections / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  79.7852      0.1732             4.6049          1.9186 -0.2000\n",
      "1  3  63.5324      0.1862             4.9053          1.4946  0.1197\n",
      "2  4  49.1226      0.2121             5.5452          1.2557  0.6343\n",
      "3  5  38.8595      0.1893             5.9191          1.0236  0.5656\n",
      "\n",
      "=== skip_connections / moderate ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  86.1633      0.0941             2.9131          2.3769 -0.2000\n",
      "1  3  60.3277      0.1488             5.6049          1.6314  0.6016\n",
      "2  4  50.4458      0.1545             5.2505          1.4492  0.6105\n",
      "3  5  41.1749      0.1788             5.3673          1.3345  0.7647\n"
     ]
    }
   ],
   "source": [
    "if ep not in K_selection:\n",
    "    print(f\"[warn] episode {ep} not in K_selection.\")\n",
    "else:\n",
    "    for topo_name, by_scen in K_selection[ep].items():\n",
    "        for scen_name, sel in by_scen.items():\n",
    "            print(f\"\\n=== {topo_name} / {scen_name} ===\")\n",
    "            dfm = sel[\"metrics_df\"]\n",
    "            print(\n",
    "                dfm[[\n",
    "                    \"K\",\n",
    "                    \"inertia\",\n",
    "                    \"silhouette\",\n",
    "                    \"calinski_harabasz\",\n",
    "                    \"davies_bouldin\",\n",
    "                    \"score\"\n",
    "                ]].round(4)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.3. Implementing K-Means Clustering </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the optimal number of clusters (K_opt), we use the K-Means algorithm for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_3_run_final_kmeans_for_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Step 4.3: Run final K-Means clustering for all environments.\n",
    "\n",
    "    Assumes:\n",
    "      - Step 4.1 has already built feature matrices under:\n",
    "            env_cfg[\"clustering\"][\"features\"] = {\n",
    "                \"X\": np.ndarray (n_mecs, n_features),\n",
    "                \"feature_cols\": list[str],\n",
    "                \"mec_ids\": np.ndarray (n_mecs,),\n",
    "                \"n_agents\": int,\n",
    "                \"n_features\": int,\n",
    "            }\n",
    "      - Step 4.2 has already selected best_K under:\n",
    "            env_cfg[\"clustering\"][\"k_selection\"][\"best_K\"]\n",
    "\n",
    "    For each (episode / topology / scenario), this function:\n",
    "      - runs KMeans with K = best_K\n",
    "      - stores the result under env_cfg[\"clustering\"][\"final\"]\n",
    "\n",
    "    Returns:\n",
    "      clustering_results[ep_name][topology_name][scen_name] = {\n",
    "        \"K\": int,\n",
    "        \"labels\": np.ndarray (n_mecs,),\n",
    "        \"centers\": np.ndarray (K, n_features),\n",
    "        \"mec_ids\": np.ndarray (n_mecs,)\n",
    "      }\n",
    "    \"\"\"\n",
    "    clustering_results: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        clustering_results[ep_name] = {}\n",
    "\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            clustering_results[ep_name][topo_name] = {}\n",
    "\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                k_sel = clust.get(\"k_selection\", None)\n",
    "\n",
    "                # --- Sanity: feature matrix must be present ---\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.3/skip] {ep_name}/{topo_name}/{scen_name}: no feature matrix.\")\n",
    "                    continue\n",
    "\n",
    "                # mec_ids must exist (MEC-based clustering only)\n",
    "                if \"mec_ids\" not in feats:\n",
    "                    raise ValueError(\n",
    "                        f\"[4.3] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                        \"'mec_ids' missing in clustering.features (expected MEC-based features).\"\n",
    "                    )\n",
    "\n",
    "                X = np.asarray(feats[\"X\"], dtype=float)\n",
    "                mec_ids = np.asarray(feats[\"mec_ids\"], dtype=int)\n",
    "\n",
    "                # --- Sanity: K selection must be available ---\n",
    "                if k_sel is None or \"best_K\" not in k_sel:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.3/skip] {ep_name}/{topo_name}/{scen_name}: no K chosen.\")\n",
    "                    continue\n",
    "\n",
    "                best_K = int(k_sel[\"best_K\"])\n",
    "\n",
    "                # --- Sanity: K must be valid ---\n",
    "                if best_K <= 1 or best_K > X.shape[0]:\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"[4.3/skip] invalid best_K={best_K} for \"\n",
    "                            f\"{ep_name}/{topo_name}/{scen_name} (n_mecs={X.shape[0]}).\"\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "                # --- Final K-Means fit over MECs ---\n",
    "                km = KMeans(\n",
    "                    n_clusters=best_K,\n",
    "                    random_state=random_state,\n",
    "                    n_init=\"auto\"\n",
    "                )\n",
    "                labels = km.fit_predict(X)\n",
    "                centers = km.cluster_centers_\n",
    "\n",
    "                # --- Store results into env_config ---\n",
    "                env_cfg.setdefault(\"clustering\", {})\n",
    "                env_cfg[\"clustering\"][\"final\"] = {\n",
    "                    \"K\": best_K,\n",
    "                    \"labels\": labels,\n",
    "                    \"centers\": centers,\n",
    "                    \"mec_ids\": mec_ids,\n",
    "                }\n",
    "\n",
    "                # --- Also store in return dictionary ---\n",
    "                clustering_results[ep_name][topo_name][scen_name] = {\n",
    "                    \"K\": best_K,\n",
    "                    \"labels\": labels,\n",
    "                    \"centers\": centers,\n",
    "                    \"mec_ids\": mec_ids,\n",
    "                }\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[4.3] {ep_name}/{topo_name}/{scen_name}:\")\n",
    "                    print(f\"      best_K = {best_K}\")\n",
    "                    print(f\"      labels distribution:\", np.bincount(labels))\n",
    "                    print(f\"      centers shape:\", centers.shape)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "    return clustering_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3] ep_000/clustered/heavy:\n",
      "      best_K = 5\n",
      "      labels distribution: [3 6 4 2 5]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/clustered/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [8 6 4 2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/clustered/moderate:\n",
      "      best_K = 5\n",
      "      labels distribution: [4 4 3 3 6]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/fully_connected/heavy:\n",
      "      best_K = 5\n",
      "      labels distribution: [3 6 4 2 5]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/fully_connected/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [8 6 4 2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/fully_connected/moderate:\n",
      "      best_K = 5\n",
      "      labels distribution: [4 4 3 3 6]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/skip_connections/heavy:\n",
      "      best_K = 5\n",
      "      labels distribution: [3 6 4 2 5]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/skip_connections/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [8 6 4 2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/skip_connections/moderate:\n",
      "      best_K = 5\n",
      "      labels distribution: [4 4 3 3 6]\n",
      "      centers shape: (5, 14)\n",
      "--------------------------------------------------\n",
      "\n",
      "=== STEP 4.3 EXAMPLE: ep_000 / clustered / heavy ===\n",
      "K: 5\n",
      "Label counts: [3 6 4 2 5]\n",
      "Centers shape: (5, 14)\n",
      "MEC IDs (first 10): [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "clustering_final = step4_3_run_final_kmeans_for_all_envs(\n",
    "    env_configs,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== STEP 4.3 EXAMPLE: ep_000 / clustered / heavy ===\")\n",
    "ex = clustering_final[\"ep_000\"][\"clustered\"][\"heavy\"]\n",
    "print(\"K:\", ex[\"K\"])\n",
    "print(\"Label counts:\", np.bincount(ex[\"labels\"]))\n",
    "print(\"Centers shape:\", ex[\"centers\"].shape)\n",
    "print(\"MEC IDs (first 10):\", ex[\"mec_ids\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization — PCA: Display clusters in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\moderate\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\moderate\\cluster_plot_pca.png\n"
     ]
    }
   ],
   "source": [
    "def step4_3_plot_clusters_pca(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    out_root: str = \"./artifacts/clustering\",\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each environment (ep/topology/scenario), take:\n",
    "        - X (scaled MEC feature matrix)\n",
    "        - labels (final K-Means cluster labels)\n",
    "    Project X to 2D via PCA and save a scatter plot.\n",
    "\n",
    "    Output saved as:\n",
    "        <out_root>/<ep>/<topology>/<scenario>/cluster_plot_pca.png\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                final = clust.get(\"final\", None)\n",
    "\n",
    "                # Need feature matrix and final clustering\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    continue\n",
    "                if final is None or \"labels\" not in final:\n",
    "                    if verbose:\n",
    "                        print(f\"[PCA/skip] {ep_name}/{topo_name}/{scen_name}: no final KMeans labels.\")\n",
    "                    continue\n",
    "\n",
    "                X = np.asarray(feats[\"X\"], dtype=float)\n",
    "                labels = np.asarray(final[\"labels\"], dtype=int)\n",
    "                K = int(final[\"K\"])\n",
    "\n",
    "                n_mecs = X.shape[0]\n",
    "\n",
    "                # PCA requires n_samples >= n_components (2 here)\n",
    "                if n_mecs < 2:\n",
    "                    if verbose:\n",
    "                        print(f\"[PCA/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"n_mecs={n_mecs} < 2, cannot run PCA.\")\n",
    "                    continue\n",
    "\n",
    "                # PCA projection to 2D\n",
    "                pca = PCA(n_components=2, random_state=42)\n",
    "                X_2d = pca.fit_transform(X)\n",
    "\n",
    "                # Output path\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_path = os.path.join(out_dir, \"cluster_plot_pca.png\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "\n",
    "                for cl in np.unique(labels):\n",
    "                    mask = (labels == cl)\n",
    "                    plt.scatter(\n",
    "                        X_2d[mask, 0],\n",
    "                        X_2d[mask, 1],\n",
    "                        label=f\"Cluster {cl}\",\n",
    "                        alpha=0.75,\n",
    "                        s=50,\n",
    "                    )\n",
    "\n",
    "                plt.title(f\"PCA Clusters (MEC): {ep_name} / {topo_name} / {scen_name}  (K={K})\")\n",
    "                plt.xlabel(\"PCA Component 1\")\n",
    "                plt.ylabel(\"PCA Component 2\")\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_path, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[PCA] Saved PCA MEC cluster plot → {out_path}\")\n",
    "\n",
    "\n",
    "step4_3_plot_clusters_pca(env_configs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization — spacet-SNE: Display clusters in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\fully_connected\\moderate\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE MEC cluster plot → ./artifacts/clustering\\ep_000\\skip_connections\\moderate\\cluster_plot_tsne.png\n"
     ]
    }
   ],
   "source": [
    "def step4_3_plot_clusters_tsne(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    out_root: str = \"./artifacts/clustering\",\n",
    "    perplexity: int = 5,\n",
    "    early_exaggeration: int = 12,\n",
    "    n_iter: int = 1500,   # kept in signature for compatibility, NOT used (depends on sklearn version)\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw 2D t-SNE visualization for final KMeans clusters over MEC feature space.\n",
    "\n",
    "    NOTE: Some sklearn versions do not support `n_iter` in TSNE.__init__.\n",
    "          To keep compatibility, we rely on the library's default n_iter.\n",
    "\n",
    "    Saves figure as:\n",
    "        <out_root>/<ep>/<topology>/<scenario>/cluster_plot_tsne.png\n",
    "    \"\"\"\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                final = clust.get(\"final\", None)\n",
    "\n",
    "                # Need feature matrix and final clustering\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    continue\n",
    "                if final is None or \"labels\" not in final:\n",
    "                    if verbose:\n",
    "                        print(f\"[t-SNE/skip] {ep_name}/{topo_name}/{scen_name}: no cluster labels.\")\n",
    "                    continue\n",
    "\n",
    "                X = np.asarray(feats[\"X\"], dtype=float)\n",
    "                labels = np.asarray(final[\"labels\"], dtype=int)\n",
    "                K = int(final[\"K\"])\n",
    "\n",
    "                n_mecs = X.shape[0]\n",
    "                if n_mecs <= perplexity:\n",
    "                    if verbose:\n",
    "                        print(f\"[t-SNE/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"n_mecs={n_mecs} <= perplexity={perplexity}\")\n",
    "                    continue\n",
    "\n",
    "                # Run t-SNE (n_iter omitted for sklearn compatibility)\n",
    "                tsne = TSNE(\n",
    "                    n_components=2,\n",
    "                    perplexity=perplexity,\n",
    "                    early_exaggeration=early_exaggeration,\n",
    "                    init='pca',\n",
    "                    learning_rate='auto',\n",
    "                    random_state=42,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "\n",
    "                X_2d = tsne.fit_transform(X)\n",
    "\n",
    "                # Plot\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_path = os.path.join(out_dir, \"cluster_plot_tsne.png\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "\n",
    "                for cl in np.unique(labels):\n",
    "                    mask = (labels == cl)\n",
    "                    plt.scatter(\n",
    "                        X_2d[mask, 0],\n",
    "                        X_2d[mask, 1],\n",
    "                        label=f\"Cluster {cl}\",\n",
    "                        s=50,\n",
    "                        alpha=0.8,\n",
    "                    )\n",
    "\n",
    "                plt.title(f\"t-SNE Clusters (MEC): {ep_name} / {topo_name} / {scen_name}  (K={K})\")\n",
    "                plt.xlabel(\"t-SNE Dim 1\")\n",
    "                plt.ylabel(\"t-SNE Dim 2\")\n",
    "                plt.grid(True)\n",
    "                plt.legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_path, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[t-SNE] Saved t-SNE MEC cluster plot → {out_path}\")\n",
    "\n",
    "\n",
    "step4_3_plot_clusters_tsne(env_configs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.4. Cluster Interpretation & Profiling </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation, Summaries, and Cluster Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_profiles_for_env(\n",
    "    ep_name: str,\n",
    "    topo_name: str,\n",
    "    scen_name: str,\n",
    "    env_cfg: Dict[str, Any],\n",
    "    out_root: str = \"./artifacts/clustering\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build cluster representative profiles using:\n",
    "      - labels from Step 4.3 (env_cfg['clustering']['final'])\n",
    "      - scaled centers from Step 4.3\n",
    "      - inverse-transformed centers using scaler from Step 4.1\n",
    "      - agent_profiles from Step 3\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract dependencies\n",
    "    clust = env_cfg.get(\"clustering\", {})\n",
    "    feats = clust.get(\"features\", None)\n",
    "    final = clust.get(\"final\", None)   # This must exist (Step 4.3)\n",
    "\n",
    "    if feats is None or \"X\" not in feats:\n",
    "        raise ValueError(f\"[4.4] Missing features for {ep_name}/{topo_name}/{scen_name}\")\n",
    "\n",
    "    if final is None or \"labels\" not in final or \"centers\" not in final:\n",
    "        raise ValueError(\n",
    "            f\"[4.4] Missing final KMeans results for {ep_name}/{topo_name}/{scen_name}. \"\n",
    "            f\"Did you forget to run Step 4.3?\"\n",
    "        )\n",
    "\n",
    "    best_K         = int(final[\"K\"])\n",
    "    labels         = np.asarray(final[\"labels\"], dtype=int)\n",
    "    centers_scaled = np.asarray(final[\"centers\"], dtype=float)\n",
    "\n",
    "    agent_ids    = feats[\"agent_ids\"]\n",
    "    feature_cols = feats[\"feature_cols\"]\n",
    "\n",
    "    prof = env_cfg[\"agent_profiles\"].copy()\n",
    "\n",
    "    # 2) Build assignment table (agent_id → cluster_id)\n",
    "    assign_df = pd.DataFrame({\n",
    "        \"agent_id\": agent_ids,\n",
    "        \"cluster_id\": labels\n",
    "    })\n",
    "\n",
    "    prof = prof.merge(assign_df, on=\"agent_id\", how=\"left\")\n",
    "\n",
    "    # === NEW: Inject cluster_id into tasks DataFrame ===\n",
    "    tasks_df = env_cfg.get(\"tasks\", None)\n",
    "    if tasks_df is not None and \"agent_id\" in tasks_df.columns:\n",
    "        tasks_df = tasks_df.merge(assign_df, on=\"agent_id\", how=\"left\")\n",
    "        # Convert to int (and fill agents with no tasks with -1)\n",
    "        tasks_df[\"cluster_id\"] = tasks_df[\"cluster_id\"].fillna(-1).astype(int)\n",
    "        # write back\n",
    "        env_cfg[\"tasks\"] = tasks_df\n",
    "        print(f\"[4.4] Added 'cluster_id' to tasks for {ep_name}/{topo_name}/{scen_name} \"\n",
    "              f\"(rows={len(tasks_df)})\")\n",
    "\n",
    "    # Debug: Check if cluster_id is in tasks_df and cluster_summary\n",
    "    print(f\"Debug: Tasks DataFrame for {ep_name}/{topo_name}/{scen_name} includes 'cluster_id':\")\n",
    "    print(tasks_df.head())\n",
    "\n",
    "    # 3) Cluster-level summary (numeric columns only, excluding cluster_id from aggregation)\n",
    "    numeric_cols = prof.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Separate group key from aggregated columns\n",
    "    agg_cols = [c for c in numeric_cols if c != \"cluster_id\"]\n",
    "\n",
    "    cluster_summary = (\n",
    "        prof[[\"cluster_id\"] + agg_cols]\n",
    "        .groupby(\"cluster_id\", as_index=False)\n",
    "        .mean()\n",
    "        .sort_values(\"cluster_id\")\n",
    "    )\n",
    "\n",
    "    cluster_sizes = (\n",
    "        prof.groupby(\"cluster_id\")[\"agent_id\"]\n",
    "        .count()\n",
    "        .rename(\"n_agents_cluster\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    cluster_summary = cluster_summary.merge(cluster_sizes, on=\"cluster_id\", how=\"left\")\n",
    "\n",
    "    # Debug: Check if cluster_summary has 'cluster_id'\n",
    "    print(f\"Debug: Cluster summary for {ep_name}/{topo_name}/{scen_name} includes 'cluster_id':\")\n",
    "    print(cluster_summary.head())\n",
    "\n",
    "    # 4) Decode centroids back to original scale (no need for scaler now)\n",
    "    centers_original = centers_scaled.copy()\n",
    "\n",
    "    centroids_scaled_df = pd.DataFrame(centers_scaled, columns=feature_cols)\n",
    "    centroids_scaled_df.insert(0, \"cluster_id\", np.arange(best_K))\n",
    "\n",
    "    centroids_original_df = pd.DataFrame(centers_original, columns=feature_cols)\n",
    "    centroids_original_df.insert(0, \"cluster_id\", np.arange(best_K))\n",
    "\n",
    "    # 5) Save to disk\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    assign_path    = os.path.join(out_dir, \"cluster_assignments.csv\")\n",
    "    summary_path   = os.path.join(out_dir, \"cluster_summary.csv\")\n",
    "    cent_sc_path   = os.path.join(out_dir, \"centroids_scaled.csv\")\n",
    "    cent_orig_path = os.path.join(out_dir, \"centroids_original.csv\")\n",
    "\n",
    "    assign_df.to_csv(assign_path, index=False)\n",
    "    cluster_summary.to_csv(summary_path, index=False)\n",
    "    centroids_scaled_df.to_csv(cent_sc_path, index=False)\n",
    "    centroids_original_df.to_csv(cent_orig_path, index=False)\n",
    "\n",
    "    print(f\"[4.4] {ep_name}/{topo_name}/{scen_name} → cluster profiles built.\")\n",
    "    print(cluster_sizes.set_index(\"cluster_id\")[\"n_agents_cluster\"])\n",
    "\n",
    "    # 6) Attach final results to env_cfg\n",
    "    env_cfg[\"agent_profiles\"] = prof\n",
    "    env_cfg[\"clustering\"][\"profiles\"] = {\n",
    "        \"K\": best_K,\n",
    "        \"cluster_assignments\": assign_df,\n",
    "        \"cluster_summary\": cluster_summary,\n",
    "        \"centroids_scaled_df\": centroids_scaled_df,\n",
    "        \"centroids_original_df\": centroids_original_df,\n",
    "        \"centroids_scaled\": centers_scaled,\n",
    "        \"centroids_original\": centers_original,\n",
    "    }\n",
    "\n",
    "    return env_cfg[\"clustering\"][\"profiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_cluster_profiles(env_configs):\n",
    "    out = {}\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        out[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            out[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                try:\n",
    "                    prof = build_cluster_profiles_for_env(\n",
    "                        ep_name, topo_name, scen_name, env_cfg\n",
    "                    )\n",
    "                    out[ep_name][topo_name][scen_name] = prof\n",
    "                except Exception as e:\n",
    "                    print(f\"[4.4/warn] skipping {ep_name}/{topo_name}/{scen_name}: {e}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "\n",
      "=== EXAMPLE: cluster summary for ep_000 / clustered / heavy ===\n",
      "K = 2\n",
      "\n",
      "Cluster summary (first few cols):\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  \n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  \n"
     ]
    }
   ],
   "source": [
    "# ---- Run Step 4.4 on all environments ----\n",
    "cluster_profiles = build_all_cluster_profiles(env_configs)\n",
    "\n",
    "print(\"\\n=== EXAMPLE: cluster summary for ep_000 / clustered / heavy ===\")\n",
    "ex_ep   = \"ep_000\"\n",
    "ex_topo = \"clustered\"\n",
    "ex_scen = \"heavy\"\n",
    "\n",
    "if (ex_ep in cluster_profiles and\n",
    "    ex_topo in cluster_profiles[ex_ep] and\n",
    "    ex_scen in cluster_profiles[ex_ep][ex_topo]):\n",
    "\n",
    "    ex_prof = cluster_profiles[ex_ep][ex_topo][ex_scen]\n",
    "    print(\"K =\", ex_prof[\"K\"])\n",
    "    print(\"\\nCluster summary (first few cols):\")\n",
    "    print(ex_prof[\"cluster_summary\"].iloc[:, :10])\n",
    "else:\n",
    "    print(\"[warn] Example triple not found in cluster_profiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Structure of env_configs:\n",
    "\n",
    "env_configs[ep][topo][scen][\"clustering\"] = {</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"features\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;# from step 4.1</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"k_selection\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.2</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"final\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.3</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"profiles\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.4</br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features', 'k_selection', 'final', 'profiles'])\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "test = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"]\n",
    "print(test.keys())\n",
    "\n",
    "print(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"profiles\"][\"cluster_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_profile_heatmap(env_cfg,\n",
    "                                 ep_name: str,\n",
    "                                 topo_name: str,\n",
    "                                 scen_name: str,\n",
    "                                 out_root=\"./artifacts/clustering\"):\n",
    "    \"\"\"\n",
    "    Draw heatmap of cluster profile means for a given env_cfg.\n",
    "\n",
    "    Uses:\n",
    "        env_cfg[\"clustering\"][\"profiles\"][\"cluster_summary\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract cluster profiles\n",
    "    clust = env_cfg.get(\"clustering\", {})\n",
    "    profs = clust.get(\"profiles\", None)\n",
    "\n",
    "    if profs is None or \"cluster_summary\" not in profs:\n",
    "        print(f\"[heatmap/skip] No cluster profiles for {ep_name}/{topo_name}/{scen_name}\")\n",
    "        return None\n",
    "\n",
    "    cluster_summary = profs[\"cluster_summary\"].copy()\n",
    "    K = profs[\"K\"]\n",
    "\n",
    "    # remove non-feature columns if present\n",
    "    drop_cols = [\"cluster_id\", \"n_agents_cluster\"]\n",
    "    feature_cols = [c for c in cluster_summary.columns if c not in drop_cols]\n",
    "\n",
    "    df = cluster_summary[feature_cols].copy()\n",
    "\n",
    "    # 2) Normalize per-column\n",
    "    df_norm = (df - df.min()) / (df.max() - df.min() + 1e-9)\n",
    "\n",
    "    # 3) Output directory\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"cluster_profile_heatmap.png\")\n",
    "\n",
    "    # 4) Plot heatmap\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(\n",
    "        df_norm,\n",
    "        annot=False,\n",
    "        cmap=\"viridis\",\n",
    "        xticklabels=df_norm.columns,\n",
    "        yticklabels=[f\"Cluster {i}\" for i in range(K)]\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Cluster Profile Heatmap: {ep_name}/{topo_name}/{scen_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[heatmap] Saved → {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\moderate\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\moderate\\cluster_profile_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "for ep in env_configs:\n",
    "    for topo in env_configs[ep]:\n",
    "        for scen in env_configs[ep][topo]:\n",
    "            plot_cluster_profile_heatmap(\n",
    "                env_configs[ep][topo][scen],\n",
    "                ep, topo, scen\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\clustering.csv\n",
      "[save] All data has been saved in the folder ./artifacts/env_configs.\n"
     ]
    }
   ],
   "source": [
    "def save_env_configs_to_csv(env_configs, out_root=\"./artifacts/env_configs\"):\n",
    "    \"\"\"\n",
    "    Save the env_configs data to CSV format in the appropriate folders based on ep, topology, and scenario.\n",
    "    \"\"\"\n",
    "    # Loop through each ep in env_configs\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                # Create the folders for each ep, topology, and scenario\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                \n",
    "                # Save the existing DataFrames in env_cfg to CSV files\n",
    "                if \"agent_profiles\" in env_cfg:\n",
    "                    agent_profiles_path = os.path.join(out_dir, \"agent_profiles.csv\")\n",
    "                    env_cfg[\"agent_profiles\"].to_csv(agent_profiles_path, index=False)\n",
    "                    print(f\"[save] agent_profiles saved to {agent_profiles_path}\")\n",
    "\n",
    "                if \"tasks\" in env_cfg:\n",
    "                    tasks_path = os.path.join(out_dir, \"tasks.csv\")\n",
    "                    env_cfg[\"tasks\"].to_csv(tasks_path, index=False)\n",
    "                    print(f\"[save] tasks saved to {tasks_path}\")\n",
    "\n",
    "                if \"clustering\" in env_cfg:\n",
    "                    clustering_path = os.path.join(out_dir, \"clustering.csv\")\n",
    "                    # If clustering data needs to be saved\n",
    "                    pd.DataFrame(env_cfg[\"clustering\"]).to_csv(clustering_path, index=False)\n",
    "                    print(f\"[save] clustering saved to {clustering_path}\")\n",
    "\n",
    "                # If there are other data to be saved, you can add them here\n",
    "                # For example, if \"tasks_df\" exists, you can save it as well\n",
    "\n",
    "    print(f\"[save] All data has been saved in the folder {out_root}.\")\n",
    "\n",
    "# Using the function\n",
    "save_env_configs_to_csv(env_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>f_local_slot</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_agent</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "      <th>mec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>46.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.053006</td>\n",
       "      <td>1.346282e+09</td>\n",
       "      <td>63.366714</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.674345</td>\n",
       "      <td>1.242386e+09</td>\n",
       "      <td>77.133770</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.414545</td>\n",
       "      <td>1.479269e+09</td>\n",
       "      <td>62.402143</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.166774</td>\n",
       "      <td>1.297716e+09</td>\n",
       "      <td>53.336022</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>1.268293</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>41.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.142497</td>\n",
       "      <td>1.451104e+09</td>\n",
       "      <td>72.397343</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local  f_local_slot      m_local  lambda_mean  lambda_var  \\\n",
       "0         0  1.741183e+09  1.741183e+09  5713.849721     1.500000    0.566667   \n",
       "1         1  1.352326e+09  1.352326e+09  4566.428755     1.062500    0.062500   \n",
       "2         2  1.726668e+09  1.726668e+09  5815.120004     1.181818    0.251082   \n",
       "3         3  1.543616e+09  1.543616e+09  3539.850245     1.200000    0.166667   \n",
       "4         4  1.130883e+09  1.130883e+09  4161.367769     1.268293    0.351220   \n",
       "\n",
       "   slots_observed  n_tasks_agent  TaskDist_sum  b_mb_med       rho_med  \\\n",
       "0            46.0           69.0           1.0  5.053006  1.346282e+09   \n",
       "1            16.0           17.0           1.0  5.674345  1.242386e+09   \n",
       "2            22.0           26.0           1.0  4.414545  1.479269e+09   \n",
       "3            25.0           30.0           1.0  6.166774  1.297716e+09   \n",
       "4            41.0           52.0           1.0  5.142497  1.451104e+09   \n",
       "\n",
       "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
       "0  63.366714    0.304348   0.260870                  0.0         0.304348   \n",
       "1  77.133770    0.294118   0.176471                  0.0         0.294118   \n",
       "2  62.402143    0.153846   0.192308                  0.0         0.153846   \n",
       "3  53.336022    0.300000   0.200000                  0.0         0.300000   \n",
       "4  72.397343    0.250000   0.153846                  0.0         0.250000   \n",
       "\n",
       "   P_data_intensive  P_compute_intensive  non_atomic_share  mec_id  \n",
       "0          0.086957             0.347826          0.449275       0  \n",
       "1          0.058824             0.470588          0.411765       1  \n",
       "2          0.115385             0.538462          0.500000       2  \n",
       "3          0.166667             0.333333          0.433333       3  \n",
       "4          0.076923             0.519231          0.384615       4  "
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 5: MDP Environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.1 — Initialization & State Builder\n",
    "\n",
    "Step 5.2 — Action Handling\n",
    "\n",
    "Step 5.3 — Queue & Execution Update\n",
    "\n",
    "Step 5.4 — Reward Calculation\n",
    "\n",
    "Step 5.5 — Done / Episode Horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, DQN, A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature_array(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features.reshape(-1, 1))  # Assuming features are in a 1D array\n",
    "\n",
    "\n",
    "# making it to work for each agent\n",
    "class OffloadingEnv(gym.Env):\n",
    "    def __init__(self, env_cfg, cluster_id, agent_id, seed=42):\n",
    "        super(OffloadingEnv, self).__init__()\n",
    "        print(\"inside OffloadingEnv\")\n",
    "\n",
    "        self.env_cfg = env_cfg\n",
    "        self.cluster_id = cluster_id\n",
    "        self.agent_id = agent_id\n",
    "        \n",
    "        self.current_time = 0\n",
    "        self.max_time = int(self.env_cfg[\"tasks\"][\"t_arrival_slot\"].max())\n",
    "\n",
    "        self.alpha = 0.5   # penalty delay\n",
    "        self.beta = 2.0    # penalty drop\n",
    "\n",
    "        self.cpu_capacity = float(self.env_cfg[\"private_cpu\"].mean())\n",
    "        \n",
    "        # Queues ONLY for this agent\n",
    "        self.queues = {'local': [], 'mec': [], 'cloud': []}\n",
    "                \n",
    "        # Define action space: LOCAL, MEC, CLOUD\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # obs = [queue_len(3) + features(4) + task_type_one_hot(5) + cluster_id_onehot(K)]\n",
    "        k = env_cfg[\"clustering\"][\"profiles\"][\"K\"]\n",
    "        self.obs_dim = 3 + 4 + 5 + k\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-10, high=10,\n",
    "                                            shape=(self.obs_dim,),\n",
    "                                            dtype=np.float32)\n",
    "        \n",
    "        self.seed_value = seed\n",
    "        \n",
    "        print(f\"Env created → cluster {cluster_id}, agent {agent_id}\")\n",
    "    \n",
    "    \n",
    "    def _inject_new_task(self):\n",
    "        ts = self.env_cfg[\"tasks\"]\n",
    "        new_tasks = ts[(ts[\"agent_id\"] == self.agent_id) &\n",
    "                    (ts[\"t_arrival_slot\"] == self.current_time)]\n",
    "        for _, t in new_tasks.iterrows():\n",
    "            self.queues['local'].append(t.to_dict())\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        \n",
    "        print(\"inside reset method step 5\\n\")\n",
    "        \n",
    "        if seed is not None:\n",
    "            self.seed_value = seed\n",
    "        \n",
    "        print(\"inside reset method step 5, before calling _get_state()\")\n",
    "         \n",
    "        self.queues = {'local': [], 'mec': [], 'cloud': []}\n",
    "        state = self._get_state()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Add new tasks that arrive now\n",
    "        self._inject_new_task()\n",
    "\n",
    "        if action == 0:\n",
    "            pass  # already local\n",
    "        elif action == 1:\n",
    "            if self.queues['local']:\n",
    "                self.queues['mec'].append(self.queues['local'].pop(0))\n",
    "        elif action == 2:\n",
    "            if self.queues['local']:\n",
    "                self.queues['cloud'].append(self.queues['local'].pop(0))\n",
    "\n",
    "        # Calculate reward for each agent based on task completions, delays, etc.\n",
    "        reward = self._process_and_reward()\n",
    "\n",
    "        # Build next state\n",
    "        next_state = self._get_state()\n",
    "\n",
    "        # Progress time\n",
    "        self.current_time += 1\n",
    "        done = self._all_done()\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "    def _process_and_reward(self):\n",
    "        reward = 0\n",
    "        cpu_share = self.cpu_capacity\n",
    "\n",
    "        for qname in ['local', 'mec', 'cloud']:\n",
    "            if self.queues[qname]:\n",
    "                task = self.queues[qname][0]\n",
    "                comp_time = task[\"c_cycles\"] / cpu_share\n",
    "                delay = max(0, comp_time - task[\"deadline_slots\"])\n",
    "                drop = 1 if comp_time > task[\"deadline_slots\"] else 0\n",
    "\n",
    "                reward += -self.alpha * delay - self.beta * drop\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        queues = self.queues\n",
    "\n",
    "        q_local = len(queues['local'])\n",
    "        q_mec = len(queues['mec'])\n",
    "        q_cloud = len(queues['cloud'])\n",
    "\n",
    "        feat = np.zeros(4)\n",
    "        if queues['local']:\n",
    "            t = queues['local'][0]\n",
    "            feat = np.array([t[\"b_mb\"], t[\"c_cycles\"], t[\"mem_mb\"], t[\"rho_cyc_per_mb\"]])\n",
    "\n",
    "        task_type = [0]*5\n",
    "        if queues['local']:\n",
    "            task_type = self._one_hot_task_type(t[\"task_type\"])\n",
    "\n",
    "        cid = self._one_hot_cluster(self.cluster_id)\n",
    "\n",
    "        state = np.concatenate([[q_local,q_mec,q_cloud], feat, task_type, cid])\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "\n",
    "    # One-hot encoding for task type and cluster id\n",
    "    def _one_hot_task_type(self, task_type):\n",
    "        task_types = ['general', 'latency_sensitive', 'compute_intensive', 'data_intensive', 'deadline_hard']\n",
    "        task_type_onehot = [0] * len(task_types)\n",
    "        if task_type in task_types:\n",
    "            task_type_onehot[task_types.index(task_type)] = 1\n",
    "        return task_type_onehot\n",
    "\n",
    "    def _one_hot_cluster(self, cluster_id):\n",
    "        \"\"\"One-hot encode the cluster_id.\"\"\"\n",
    "        num_clusters = self.env_cfg[\"clustering\"][\"profiles\"][\"K\"]\n",
    "        cluster_onehot = [0] * num_clusters\n",
    "        cluster_onehot[cluster_id] = 1\n",
    "        return cluster_onehot\n",
    "\n",
    "    \n",
    "    def _all_done(self):\n",
    "        \"\"\"Episode ends when no tasks remain and no future arrivals exist.\"\"\"\n",
    "        \n",
    "        no_pending_tasks = (\n",
    "            len(self.queues['local']) == 0 and\n",
    "            len(self.queues['mec']) == 0 and\n",
    "            len(self.queues['cloud']) == 0\n",
    "        )\n",
    "\n",
    "        # No more task arrivals possible\n",
    "        no_future_arrivals = self.current_time >= self.max_time\n",
    "\n",
    "        return no_pending_tasks and no_future_arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 6 - Training and Evaluation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OffloadingEnvWrapper is dedicated to interacting with the environment and RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadingEnvWrapper(gym.Env):\n",
    "    \"\"\"Environment wrapper for using OffloadingEnv with Stable-Baselines3\"\"\"\n",
    "    # each env_cfg is a specific topology-scenario\n",
    "\n",
    "    def __init__(self, env_cfg, cluster_id, agent_id, selected_algo):\n",
    "        self.env_cfg = env_cfg  # Get the environment config for the current episode/topology/scenario\n",
    "        self.cluster_id = cluster_id  # Cluster ID for which the model is trained\n",
    "        self.agent_id = agent_id\n",
    "        self.selected_algo = selected_algo\n",
    "\n",
    "        # self.env = OffloadingEnv(env_cfg, cluster_id)  # Pass the env_cfg to the original environment\n",
    "        self.env = OffloadingEnv(env_cfg, cluster_id, agent_id)  # agent_id هم باید به آن داده شود\n",
    "\n",
    "        print(\"after self.env = OffloadingEnv(env_cfg)\")\n",
    "\n",
    "        # Set the appropriate RL algorithm for this clusters\n",
    "        if self.selected_algo == \"PPO\":\n",
    "            self.model = PPO(\"MlpPolicy\", self.env, verbose=1)\n",
    "        elif self.selected_algo == \"DQN\":\n",
    "            self.model = DQN(\"MlpPolicy\", self.env, verbose=1)\n",
    "        elif self.selected_algo == \"A2C\":\n",
    "            print(\"in if self.selected_algo == A2C step 6\")\n",
    "            self.model = A2C(\"MlpPolicy\", self.env, verbose=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported algorithm: {self.selected_algo}\")\n",
    "\n",
    "        print(\"self.model: \", self.model)\n",
    "        print(\"after self.model set up done in step 6\")\n",
    "\n",
    "        # Define the action space: LOCAL, MEC, CLOUD\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 3 actions for each agent (LOCAL, MEC, CLOUD)\n",
    "\n",
    "        # Extract the number of features from the cluster profile for observation space\n",
    "        # num_features = len(cluster_profile) - 1  # Remove cluster_id from features\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(num_features,), dtype=float)\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"inside reset method step 6\\n\")\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Perform one step in the environment for each agent.\n",
    "        actions: dict of {agent_id: action}\n",
    "        \"\"\"\n",
    "        return self.env.step(actions)\n",
    "\n",
    "    def render(self):\n",
    "        pass  # You can implement a rendering function here if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_algo_for_cluster(cluster_profile):\n",
    "    \"\"\"\n",
    "    Select an RL algorithm for each cluster based on its profile.\n",
    "    The profile can include information like lambda_mean, task types, etc.\n",
    "    \"\"\"\n",
    "    print(\"inside select_algo_for_cluster\")\n",
    "    if cluster_profile[\"lambda_mean\"] > 0.5 and cluster_profile[\"P_compute_intensive\"] > 0.5:\n",
    "        print(\"Selected algorithm: PPO\")\n",
    "        return \"PPO\"\n",
    "    elif cluster_profile[\"lambda_mean\"] < 0.5 and cluster_profile[\"P_general\"] > 0.5:\n",
    "        print(\"Selected algorithm: DQN\")\n",
    "        return \"DQN\"\n",
    "    else:\n",
    "        print(\"Selected algorithm: A2C\")\n",
    "        return \"A2C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "def parallel_training_step(envs, actions, step_num):\n",
    "    \"\"\"\n",
    "    Process one step for all agents in parallel.\n",
    "    \"\"\"\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    done_flags = []\n",
    "\n",
    "    for env, action in zip(envs, actions):\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        done_flags.append(done)\n",
    "\n",
    "    # Return the new states, rewards, and done flags\n",
    "    return next_states, rewards, done_flags\n",
    "\n",
    "def ec_update(envs, step_num, env_configs):\n",
    "    \"\"\"\n",
    "    Update the EC after every 10 time_slots.\n",
    "    Collect data and share with the cluster agents.\n",
    "    \"\"\"\n",
    "    if step_num % 10 == 0:  # Every 10 time_slots, EC performs an update\n",
    "        print(f\"EC update at step {step_num}\")\n",
    "        # Collect data from all agents in the cluster\n",
    "        cluster_data = []\n",
    "        for env in envs:\n",
    "            cluster_data.append({\n",
    "                'agent_id': env.agent_id,\n",
    "                'queue': env.queues,\n",
    "                'state': env._get_state()\n",
    "            })\n",
    "\n",
    "        # Now share the collected data with all other agents in the topology\n",
    "        for env in envs:\n",
    "            # Update the agent with the latest shared data\n",
    "            env.shared_data = cluster_data  # Just an example\n",
    "\n",
    "        # Update EC data in the environment\n",
    "        # This could be for example, updating the \"global state\" or performing some\n",
    "        # action that updates the environment's global configuration\n",
    "        pass\n",
    "\n",
    "def train_all_agents_in_parallel(env_configs):\n",
    "    \"\"\"\n",
    "    Train all agents in parallel in a given topology-scenario configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment instances for all agents in a specific topology-scenario\n",
    "    envs = []\n",
    "    actions = []\n",
    "    step_num = 0\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, cfg in by_scen.items():\n",
    "\n",
    "                cluster_df = cfg[\"agent_profiles\"]\n",
    "                centroids = cfg[\"clustering\"][\"profiles\"][\"centroids_scaled_df\"]\n",
    "\n",
    "                # Set up the environments for each agent based on cluster profile\n",
    "                for cid in range(cfg[\"clustering\"][\"profiles\"][\"K\"]):\n",
    "                    profile = centroids[centroids.cluster_id == cid].iloc[0]\n",
    "                    algo = select_algo_for_cluster(profile)\n",
    "\n",
    "                    agents = cluster_df[cluster_df.cluster_id == cid][\"agent_id\"]\n",
    "\n",
    "                    for aid in agents:\n",
    "                        print(f\"Initializing environment for {ep_name}/{topo_name}/{scen_name} | Cluster {cid} | Agent {aid}\")\n",
    "\n",
    "                        env = OffloadingEnv(cfg, cid, aid)\n",
    "                        envs.append(env)\n",
    "\n",
    "                        # Select the algorithm for this agent based on the cluster\n",
    "                        model_cls = dict(PPO=PPO, DQN=DQN, A2C=A2C)[algo]\n",
    "                        model = model_cls(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "                        # Initialize model training for each agent\n",
    "                        model.learn(total_timesteps=10000)\n",
    "                        \n",
    "                        # Store the models for each agent\n",
    "                        # In real-world usage, you could parallelize this part using multiprocessing or threading\n",
    "                        # models[(ep_name, topo_name, scen_name, cid, aid)] = model\n",
    "\n",
    "    # Run through the simulation (training) steps for all agents simultaneously\n",
    "    while step_num < 100:  # Example, run for 100 time steps\n",
    "        actions = [env.action_space.sample() for env in envs]  # Random actions, replace with policy actions\n",
    "        next_states, rewards, done_flags = parallel_training_step(envs, actions, step_num)\n",
    "\n",
    "        # Perform EC update every 10 time slots\n",
    "        ec_update(envs, step_num, env_configs)\n",
    "\n",
    "        step_num += 1\n",
    "\n",
    "    return envs  # Returning final environment states after training\n",
    "\n",
    "\n",
    "# Assuming `env_configs` is already available\n",
    "trained_envs = train_all_agents_in_parallel(env_configs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
