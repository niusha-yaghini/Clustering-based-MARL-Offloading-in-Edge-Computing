{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Prepare data and configure the environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1. Data Loading (Data I/O) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "dataset_dir = '../Data_Generator/datasets'\n",
    "topology_dir = '../Topology_Generator/topologies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global container\n",
    "datasets = {}\n",
    "\n",
    "def load_datasets_from_directory(dataset_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Episode-first loader for structure:\n",
    "\n",
    "        dataset_dir/\n",
    "          ep_000/\n",
    "            light/\n",
    "              episodes.csv\n",
    "              agents.csv\n",
    "              arrivals.csv\n",
    "              tasks.csv\n",
    "            moderate/\n",
    "              ...\n",
    "            heavy/\n",
    "              ...\n",
    "            dataset_metadata.json   (optional, per-episode meta)\n",
    "\n",
    "    Result:\n",
    "        datasets = {\n",
    "            \"ep_000\": {\n",
    "                \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "                \"moderate\":{\"...\"},\n",
    "                \"heavy\":   {\"...\"},\n",
    "                \"_meta\":   {...}  # if dataset_metadata.json exists\n",
    "            },\n",
    "            \"ep_001\": { ... },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    global datasets\n",
    "    datasets = {}\n",
    "\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        raise ValueError(f\"dataset_dir does not exist or is not a directory: {dataset_dir}\")\n",
    "\n",
    "    # Step 1 — detect ep_* directories\n",
    "    ep_dirs = sorted([\n",
    "        name for name in os.listdir(dataset_dir)\n",
    "        if os.path.isdir(os.path.join(dataset_dir, name)) and name.startswith(\"ep_\")\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        if not ep_dirs:\n",
    "            print(f\"[warn] no ep_* folders found under root '{dataset_dir}'\")\n",
    "        else:\n",
    "            print(f\"[info] detected episodes: {ep_dirs}\")\n",
    "\n",
    "    # Step 2 — per episode, detect scenarios and load CSVs\n",
    "    for ep_name in ep_dirs:\n",
    "        ep_path = os.path.join(dataset_dir, ep_name)\n",
    "        datasets[ep_name] = {}\n",
    "\n",
    "        # scenarios inside this episode (e.g. light/moderate/heavy)\n",
    "        scenario_names = sorted([\n",
    "            name for name in os.listdir(ep_path)\n",
    "            if os.path.isdir(os.path.join(ep_path, name))\n",
    "        ])\n",
    "\n",
    "        if verbose:\n",
    "            if not scenario_names:\n",
    "                print(f\"[warn] no scenario folders found under episode '{ep_name}'\")\n",
    "            else:\n",
    "                print(f\"[info] {ep_name}: scenarios detected -> {scenario_names}\")\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            scn_path = os.path.join(ep_path, scenario)\n",
    "            try:\n",
    "                dfs = {\n",
    "                    \"episodes\": pd.read_csv(os.path.join(scn_path, \"episodes.csv\")),\n",
    "                    \"agents\":   pd.read_csv(os.path.join(scn_path, \"agents.csv\")),\n",
    "                    \"arrivals\": pd.read_csv(os.path.join(scn_path, \"arrivals.csv\")),\n",
    "                    \"tasks\":    pd.read_csv(os.path.join(scn_path, \"tasks.csv\")),\n",
    "                }\n",
    "                datasets[ep_name][scenario] = dfs\n",
    "            except FileNotFoundError as e:\n",
    "                if verbose:\n",
    "                    print(f\"[error] missing CSV in {scn_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Step 3 — load per-episode metadata if present\n",
    "        meta_path = os.path.join(ep_path, \"dataset_metadata.json\")\n",
    "        if os.path.isfile(meta_path):\n",
    "            try:\n",
    "                with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    meta = json.load(f)\n",
    "                datasets[ep_name][\"_meta\"] = meta\n",
    "                if verbose:\n",
    "                    print(f\"[info] loaded metadata for {ep_name} from {meta_path}\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"[warn] could not load metadata for {ep_name}: {e}\")\n",
    "\n",
    "    # Optional summary printing\n",
    "    if verbose:\n",
    "        print(\"\\n=== Dataset Summary (episode-first) ===\")\n",
    "        print(f\"episodes detected: {len(datasets)}\")\n",
    "        for ep_name in sorted(datasets.keys()):\n",
    "            keys_here = sorted(datasets[ep_name].keys())\n",
    "            scenarios_here = [k for k in keys_here if not k.startswith(\"_\")]\n",
    "            print(f\"  - {ep_name}: scenarios = {scenarios_here}\")\n",
    "            for scn in scenarios_here:\n",
    "                dfs = datasets[ep_name][scn]\n",
    "                n_ep   = len(dfs[\"episodes\"])\n",
    "                n_ag   = len(dfs[\"agents\"])\n",
    "                n_arr  = len(dfs[\"arrivals\"])\n",
    "                n_task = len(dfs[\"tasks\"])\n",
    "                print(f\"      {scn:9s} → episodes:{n_ep:3d}  agents:{n_ag:4d}  arrivals:{n_arr:6d}  tasks:{n_task:6d}\")\n",
    "            if \"_meta\" in datasets[ep_name]:\n",
    "                print(f\"      meta: dataset_metadata.json loaded\")\n",
    "        print(\"=======================================\\n\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] detected episodes: ['ep_000']\n",
      "[info] ep_000: scenarios detected -> ['heavy', 'light', 'moderate']\n",
      "[info] loaded metadata for ep_000 from ../Data_Generator/datasets\\ep_000\\dataset_metadata.json\n",
      "\n",
      "=== Dataset Summary (episode-first) ===\n",
      "episodes detected: 1\n",
      "  - ep_000: scenarios = ['heavy', 'light', 'moderate']\n",
      "      heavy     → episodes:  1  agents:  18  arrivals:   841  tasks:   841\n",
      "      light     → episodes:  1  agents:  18  arrivals:    70  tasks:    70\n",
      "      moderate  → episodes:  1  agents:  18  arrivals:   261  tasks:   261\n",
      "      meta: dataset_metadata.json loaded\n",
      "=======================================\n",
      "\n",
      "\n",
      "[info] printing from episode='ep_000', scenario='heavy'\n",
      "\n",
      "agents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lam_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>0.708673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>0.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>0.228174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>0.310369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>0.548990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  agent_id       f_local      m_local   lam_sec\n",
       "0    heavy         0  1.741183e+09  5713.849721  0.708673\n",
       "1    heavy         1  1.352326e+09  4566.428755  0.234989\n",
       "2    heavy         2  1.726668e+09  5815.120004  0.228174\n",
       "3    heavy         3  1.543616e+09  3539.850245  0.310369\n",
       "4    heavy         4  1.130883e+09  4161.367769  0.548990"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   scenario  18 non-null     object \n",
      " 1   agent_id  18 non-null     int64  \n",
      " 2   f_local   18 non-null     float64\n",
      " 3   m_local   18 non-null     float64\n",
      " 4   lam_sec   18 non-null     float64\n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 848.0+ bytes\n",
      "\n",
      "arrivals:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>t_slot</th>\n",
       "      <th>t_time</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>task_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  t_slot  t_time  agent_id  task_id\n",
       "0    heavy           0       0     0.0         0        0\n",
       "1    heavy           0       0     0.0         1        1\n",
       "2    heavy           0       0     0.0         4        2\n",
       "3    heavy           0       0     0.0         7        3\n",
       "4    heavy           0       0     0.0        10        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841 entries, 0 to 840\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    841 non-null    object \n",
      " 1   episode_id  841 non-null    int64  \n",
      " 2   t_slot      841 non-null    int64  \n",
      " 3   t_time      841 non-null    float64\n",
      " 4   agent_id    841 non-null    int64  \n",
      " 5   task_id     841 non-null    int64  \n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 39.5+ KB\n",
      "\n",
      "episodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>Delta</th>\n",
       "      <th>T_slots</th>\n",
       "      <th>T_decision</th>\n",
       "      <th>T_drain</th>\n",
       "      <th>hours</th>\n",
       "      <th>N_agents</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>18</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  Delta  T_slots  T_decision  T_drain     hours  \\\n",
       "0    heavy           0    1.0      110         100       10  0.030556   \n",
       "\n",
       "   N_agents  seed  \n",
       "0        18   345  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   scenario    1 non-null      object \n",
      " 1   episode_id  1 non-null      int64  \n",
      " 2   Delta       1 non-null      float64\n",
      " 3   T_slots     1 non-null      int64  \n",
      " 4   T_decision  1 non-null      int64  \n",
      " 5   T_drain     1 non-null      int64  \n",
      " 6   hours       1 non-null      float64\n",
      " 7   N_agents    1 non-null      int64  \n",
      " 8   seed        1 non-null      int64  \n",
      "dtypes: float64(2), int64(6), object(1)\n",
      "memory usage: 200.0+ bytes\n",
      "\n",
      "tasks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539704</td>\n",
       "      <td>continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb modality  has_deadline  \\\n",
       "0   7.202096    9.727147e+08  7.005585e+09  66.611010   sensor             1   \n",
       "1   5.479984    1.314973e+09  7.206031e+09  77.928800    image             1   \n",
       "2   8.421977    2.500222e+09  2.105681e+10  72.966446     text             1   \n",
       "3   6.324986    1.779582e+09  1.125583e+10  56.492900   sensor             1   \n",
       "4  11.473269    1.087572e+09  1.247800e+10  73.389854   sensor             1   \n",
       "\n",
       "   deadline_s  deadline_time  non_atomic  split_ratio action_space_hint  \n",
       "0    0.800726       0.800726           0     0.000000          discrete  \n",
       "1    0.615113       0.615113           0     0.000000          discrete  \n",
       "2    0.323007       0.323007           1     0.539704        continuous  \n",
       "3    0.481587       0.481587           0     0.000000          discrete  \n",
       "4    0.594564       0.594564           0     0.000000          discrete  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841 entries, 0 to 840\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   scenario           841 non-null    object \n",
      " 1   episode_id         841 non-null    int64  \n",
      " 2   task_id            841 non-null    int64  \n",
      " 3   agent_id           841 non-null    int64  \n",
      " 4   t_arrival_slot     841 non-null    int64  \n",
      " 5   t_arrival_time     841 non-null    float64\n",
      " 6   b_mb               841 non-null    float64\n",
      " 7   rho_cyc_per_mb     841 non-null    float64\n",
      " 8   c_cycles           841 non-null    float64\n",
      " 9   mem_mb             841 non-null    float64\n",
      " 10  modality           841 non-null    object \n",
      " 11  has_deadline       841 non-null    int64  \n",
      " 12  deadline_s         290 non-null    float64\n",
      " 13  deadline_time      290 non-null    float64\n",
      " 14  non_atomic         841 non-null    int64  \n",
      " 15  split_ratio        841 non-null    float64\n",
      " 16  action_space_hint  841 non-null    object \n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 111.8+ KB\n",
      "\n",
      "meta (dataset_metadata.json):\n",
      "{\n",
      "  \"schema_version\": \"1.0.0\",\n",
      "  \"generated_at_utc\": \"2025-11-18T10:53:32Z\",\n",
      "  \"env\": {\n",
      "    \"python\": \"3.10.9\",\n",
      "    \"user\": \"niush\"\n",
      "  },\n",
      "  \"episodes_root\": \"d:\\\\Karamozi\\\\Meet 0.5\\\\Implementation\\\\Clustering-based-MARL-Offloading-in-Edge-Computing\\\\Data_Generator\\\\datasets\\\\ep_000\",\n",
      "  \"notes\": {\n",
      "    \"policy_agnostic\": true,\n",
      "    \"queueing\": \"not simulated here\",\n",
      "    \"timing\": {\n",
      "      \"description\": \"HOODIE-style: T_decision slots with arrivals + T_drain slots without arrivals.\"\n",
      "    },\n",
      "    \"clipping\": {\n",
      "      \"enabled\": true,\n",
      "      \"method\": \"lognormal analytic quantile cap\",\n",
      "      \"qcap\": 0.99,\n",
      "      \"z_table_keys\": [\n",
      "        0.9,\n",
      "        0.95,\n",
      "        0.975,\n",
      "        0.99,\n",
      "        0.995,\n",
      "        0.999\n",
      "      ]\n",
      "    },\n",
      "    \"action_space_hint\": \"derived from non_atomic; final decision belongs to environment\"\n",
      "  },\n",
      "  \"scenarios\": [\n",
      "    {\n",
      "      \"name\": \"light\",\n",
      "      \"fingerprint\": \"42319969435ba9ca\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 143\n",
      "      },\n",
      "      \"N_agents\": 18,\n",
      "      \"AgentRanges\": {\n",
      "        \"lam_sec_min\": 0.01,\n",
      "        \"lam_sec_max\": 0.05,\n",
      "        \"f_local_min\": 800000000.0,\n",
      "        \"f_local_max\": 2400000000.0,\n",
      "        \"m_local_min\": 3000.0,\n",
      "        \"m_local_max\": 8000.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 2.0,\n",
      "        \"b_sigma_g\": 1.55,\n",
      "        \"rho_median\": 1000000000.0,\n",
      "        \"rho_sigma_g\": 1.45,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.4,\n",
      "        \"p_deadline\": 0.15,\n",
      "        \"deadline_min\": 0.8,\n",
      "        \"deadline_max\": 2.0,\n",
      "        \"p_non_atomic\": 0.25,\n",
      "        \"split_ratio_min\": 0.25,\n",
      "        \"split_ratio_max\": 0.75,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per agent)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\",\n",
      "        \"f_local\": \"Hz\",\n",
      "        \"m_local\": \"MB\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"moderate\",\n",
      "      \"fingerprint\": \"f0d881a348966161\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 244\n",
      "      },\n",
      "      \"N_agents\": 18,\n",
      "      \"AgentRanges\": {\n",
      "        \"lam_sec_min\": 0.05,\n",
      "        \"lam_sec_max\": 0.2,\n",
      "        \"f_local_min\": 800000000.0,\n",
      "        \"f_local_max\": 2400000000.0,\n",
      "        \"m_local_min\": 3000.0,\n",
      "        \"m_local_max\": 8000.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 3.0,\n",
      "        \"b_sigma_g\": 1.6,\n",
      "        \"rho_median\": 1200000000.0,\n",
      "        \"rho_sigma_g\": 1.5,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.45,\n",
      "        \"p_deadline\": 0.25,\n",
      "        \"deadline_min\": 0.5,\n",
      "        \"deadline_max\": 1.5,\n",
      "        \"p_non_atomic\": 0.35,\n",
      "        \"split_ratio_min\": 0.3,\n",
      "        \"split_ratio_max\": 0.8,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per agent)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\",\n",
      "        \"f_local\": \"Hz\",\n",
      "        \"m_local\": \"MB\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"heavy\",\n",
      "      \"fingerprint\": \"b4c07655801e9555\",\n",
      "      \"Episode\": {\n",
      "        \"Delta\": 1.0,\n",
      "        \"T_slots\": 110,\n",
      "        \"T_decision\": 100,\n",
      "        \"T_drain\": 10,\n",
      "        \"seed\": 345\n",
      "      },\n",
      "      \"N_agents\": 18,\n",
      "      \"AgentRanges\": {\n",
      "        \"lam_sec_min\": 0.2,\n",
      "        \"lam_sec_max\": 0.8,\n",
      "        \"f_local_min\": 800000000.0,\n",
      "        \"f_local_max\": 2400000000.0,\n",
      "        \"m_local_min\": 3000.0,\n",
      "        \"m_local_max\": 8000.0\n",
      "      },\n",
      "      \"TaskDist\": {\n",
      "        \"b_median\": 5.0,\n",
      "        \"b_sigma_g\": 1.7,\n",
      "        \"rho_median\": 1500000000.0,\n",
      "        \"rho_sigma_g\": 1.55,\n",
      "        \"mem_median\": 64.0,\n",
      "        \"mem_sigma_g\": 1.5,\n",
      "        \"p_deadline\": 0.35,\n",
      "        \"deadline_min\": 0.3,\n",
      "        \"deadline_max\": 1.0,\n",
      "        \"p_non_atomic\": 0.45,\n",
      "        \"split_ratio_min\": 0.4,\n",
      "        \"split_ratio_max\": 0.85,\n",
      "        \"modality_probs\": null,\n",
      "        \"modality_labels\": null\n",
      "      },\n",
      "      \"units\": {\n",
      "        \"Delta\": \"seconds\",\n",
      "        \"lam_sec\": \"tasks per second (per agent)\",\n",
      "        \"per_slot_rate\": \"lam_sec * Delta\",\n",
      "        \"b_mb\": \"MB\",\n",
      "        \"rho_cyc_per_mb\": \"CPU cycles per MB\",\n",
      "        \"c_cycles\": \"CPU cycles\",\n",
      "        \"mem_mb\": \"MB\",\n",
      "        \"deadline_s\": \"seconds (relative); deadline_time = t_arrival_time + deadline_s\",\n",
      "        \"f_local\": \"Hz\",\n",
      "        \"m_local\": \"MB\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ---- load all datasets (episode-first) ----\n",
    "datasets = load_datasets_from_directory(dataset_dir, verbose=True)\n",
    "\n",
    "# ---- choose an episode and a scenario for printing ----\n",
    "ep_name = sorted(datasets.keys())[0] if datasets else None\n",
    "scenario = \"heavy\"  # \"light\"/\"moderate\"\n",
    "\n",
    "if ep_name is not None and scenario in datasets[ep_name]:\n",
    "    print(f\"\\n[info] printing from episode='{ep_name}', scenario='{scenario}'\")\n",
    "\n",
    "    print(\"\\nagents:\")\n",
    "    display(datasets[ep_name][scenario][\"agents\"].head())\n",
    "    datasets[ep_name][scenario][\"agents\"].info()\n",
    "\n",
    "    print(\"\\narrivals:\")\n",
    "    display(datasets[ep_name][scenario][\"arrivals\"].head())\n",
    "    datasets[ep_name][scenario][\"arrivals\"].info()\n",
    "\n",
    "    print(\"\\nepisodes:\")\n",
    "    display(datasets[ep_name][scenario][\"episodes\"].head())\n",
    "    datasets[ep_name][scenario][\"episodes\"].info()\n",
    "\n",
    "    print(\"\\ntasks:\")\n",
    "    display(datasets[ep_name][scenario][\"tasks\"].head())\n",
    "    datasets[ep_name][scenario][\"tasks\"].info()\n",
    "\n",
    "    if \"_meta\" in datasets[ep_name]:\n",
    "        print(\"\\nmeta (dataset_metadata.json):\")\n",
    "        print(json.dumps(datasets[ep_name][\"_meta\"], ensure_ascii=False, indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"[error] no datasets found or requested scenario is missing for the chosen episode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "topologies = {}\n",
    "\n",
    "def load_topologies_from_directory(topology_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Load all topologies under topology_dir, expecting structure:\n",
    "\n",
    "        topology_dir/\n",
    "          clustered/\n",
    "            topology.json\n",
    "            topology_meta.json\n",
    "            connection_matrix.csv\n",
    "          full_mesh/\n",
    "          sparse_ring/\n",
    "          ...\n",
    "\n",
    "    Fills global 'topologies' as:\n",
    "        {\n",
    "          \"clustered\": {\n",
    "              \"topology_data\": dict,\n",
    "              \"meta_data\": dict,\n",
    "              \"connection_matrix\": DataFrame\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    global topologies\n",
    "    topologies = {}\n",
    "\n",
    "    if not os.path.isdir(topology_dir):\n",
    "        raise ValueError(f\"topology_dir does not exist or is not a directory: {topology_dir}\")\n",
    "\n",
    "    for topology_name in os.listdir(topology_dir):\n",
    "        topology_path = os.path.join(topology_dir, topology_name)\n",
    "\n",
    "        # Only process directories\n",
    "        if not os.path.isdir(topology_path):\n",
    "            continue\n",
    "\n",
    "        topology_json_path = os.path.join(topology_path, \"topology.json\")\n",
    "        meta_json_path = os.path.join(topology_path, \"topology_meta.json\")\n",
    "        connection_matrix_csv_path = os.path.join(topology_path, \"connection_matrix.csv\")\n",
    "\n",
    "        if not (os.path.isfile(topology_json_path) and\n",
    "                os.path.isfile(meta_json_path) and\n",
    "                os.path.isfile(connection_matrix_csv_path)):\n",
    "            if verbose:\n",
    "                print(f\"[warn] skipping '{topology_name}' — missing one of required files.\")\n",
    "            continue\n",
    "\n",
    "        # --- Load JSON & CSV files ---\n",
    "        with open(topology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            topology_data = json.load(f)\n",
    "        with open(meta_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta_data = json.load(f)\n",
    "\n",
    "        # First column is row labels → index_col=0\n",
    "        connection_matrix = pd.read_csv(connection_matrix_csv_path, index_col=0)\n",
    "\n",
    "        topologies[topology_name] = {\n",
    "            \"topology_data\": topology_data,\n",
    "            \"meta_data\": meta_data,\n",
    "            \"connection_matrix\": connection_matrix\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[info] loaded topologies: {sorted(topologies.keys())}\")\n",
    "\n",
    "    return topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loaded topologies: ['clustered', 'full_mesh', 'sparse_ring']\n",
      "topology clustered -> connection_matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mec_0</th>\n",
       "      <th>mec_1</th>\n",
       "      <th>mec_2</th>\n",
       "      <th>mec_3</th>\n",
       "      <th>mec_4</th>\n",
       "      <th>mec_5</th>\n",
       "      <th>mec_6</th>\n",
       "      <th>mec_7</th>\n",
       "      <th>mec_8</th>\n",
       "      <th>mec_9</th>\n",
       "      <th>mec_10</th>\n",
       "      <th>mec_11</th>\n",
       "      <th>mec_12</th>\n",
       "      <th>mec_13</th>\n",
       "      <th>mec_14</th>\n",
       "      <th>mec_15</th>\n",
       "      <th>mec_16</th>\n",
       "      <th>mec_17</th>\n",
       "      <th>cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mec_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.798801</td>\n",
       "      <td>11.958295</td>\n",
       "      <td>11.470404</td>\n",
       "      <td>9.980195</td>\n",
       "      <td>8.814050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_1</th>\n",
       "      <td>9.798801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.136893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.609202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_2</th>\n",
       "      <td>11.958295</td>\n",
       "      <td>8.375143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>9.170700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.055348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_3</th>\n",
       "      <td>11.470404</td>\n",
       "      <td>11.535600</td>\n",
       "      <td>8.814632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>11.920252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.851664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mec_4</th>\n",
       "      <td>9.980195</td>\n",
       "      <td>10.702584</td>\n",
       "      <td>9.637714</td>\n",
       "      <td>10.377054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.105383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.076301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mec_0      mec_1      mec_2      mec_3      mec_4      mec_5  \\\n",
       "mec_0   0.000000   9.798801  11.958295  11.470404   9.980195   8.814050   \n",
       "mec_1   9.798801   0.000000   8.375143  11.535600  10.702584   9.136893   \n",
       "mec_2  11.958295   8.375143   0.000000   8.814632   9.637714   9.170700   \n",
       "mec_3  11.470404  11.535600   8.814632   0.000000  10.377054  11.920252   \n",
       "mec_4   9.980195  10.702584   9.637714  10.377054   0.000000   8.105383   \n",
       "\n",
       "       mec_6  mec_7  mec_8  mec_9  mec_10  mec_11  mec_12  mec_13  mec_14  \\\n",
       "mec_0    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_1    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_2    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_3    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mec_4    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       mec_15  mec_16  mec_17       cloud  \n",
       "mec_0     0.0     0.0     0.0   98.912800  \n",
       "mec_1     0.0     0.0     0.0  104.609202  \n",
       "mec_2     0.0     0.0     0.0   92.055348  \n",
       "mec_3     0.0     0.0     0.0   84.851664  \n",
       "mec_4     0.0     0.0     0.0   99.076301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18 entries, mec_0 to mec_17\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   mec_0   18 non-null     float64\n",
      " 1   mec_1   18 non-null     float64\n",
      " 2   mec_2   18 non-null     float64\n",
      " 3   mec_3   18 non-null     float64\n",
      " 4   mec_4   18 non-null     float64\n",
      " 5   mec_5   18 non-null     float64\n",
      " 6   mec_6   18 non-null     float64\n",
      " 7   mec_7   18 non-null     float64\n",
      " 8   mec_8   18 non-null     float64\n",
      " 9   mec_9   18 non-null     float64\n",
      " 10  mec_10  18 non-null     float64\n",
      " 11  mec_11  18 non-null     float64\n",
      " 12  mec_12  18 non-null     float64\n",
      " 13  mec_13  18 non-null     float64\n",
      " 14  mec_14  18 non-null     float64\n",
      " 15  mec_15  18 non-null     float64\n",
      " 16  mec_16  18 non-null     float64\n",
      " 17  mec_17  18 non-null     float64\n",
      " 18  cloud   18 non-null     float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 2.8+ KB\n",
      "\n",
      "topology clustered -> topology_data\n",
      "{'number_of_servers': 18, 'private_cpu_capacities': [1417026512.123894, 1465835517.1380253, 1400296152.8672054, 1219863115.6758468, 1270376631.237273, 1348019534.1092064, 1786161289.9722981, 1380932007.7716885, 1739680378.9314957, 1606151704.3138125, 1722792016.067419, 1452119690.3718696, 1575323717.1732426, 1675176049.772993, 1465269239.5654364, 1565743740.4168038, 1715259832.5839007, 1225832428.456285], 'public_cpu_capacities': [768946447.3896191, 753418540.8443304, 702715947.6715652, 545197668.8935852, 670994513.1813464, 680525151.7601619, 811159071.5491732, 782943570.2288362, 501997671.50295806, 685198791.9005744, 822519853.1111488, 716138690.9607652, 822355072.296565, 556428866.3646028, 805978940.2755054, 790274917.5537901, 602437544.1146357, 709495765.381235], 'cloud_computational_capacity': 30000000000.0, 'connection_matrix': [[0.0, 9.798800609449902, 11.958295388571093, 11.470404384171585, 9.980195439399376, 8.8140500666217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.91279981562592], [9.798800609449902, 0.0, 8.375143239602235, 11.535600106180025, 10.702584376249934, 9.136893120492793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.60920207554163], [11.958295388571093, 8.375143239602235, 0.0, 8.814631907625701, 9.637714071509851, 9.170700424264968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 92.05534806322484], [11.470404384171585, 11.535600106180025, 8.814631907625701, 0.0, 10.37705391001423, 11.920252079923895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.85166371762908], [9.980195439399376, 10.702584376249934, 9.637714071509851, 10.37705391001423, 0.0, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0763009651688], [8.8140500666217, 9.136893120492793, 9.170700424264968, 11.920252079923895, 8.10538323609413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.0792791055983], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 9.566567838535661, 11.01158338193066, 10.132206014485726, 11.731761213672993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.87637059619391], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.844690223161571, 0.0, 8.17265481777462, 11.317250164676846, 9.944330066939582, 11.698994881774604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.49730491373893], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.566567838535661, 8.17265481777462, 0.0, 11.919271797879857, 9.162931925532463, 11.22074688068535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.13111014716404], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.01158338193066, 11.317250164676846, 11.919271797879857, 0.0, 11.8680001020066, 11.393785145055949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 87.20532264682294], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.132206014485726, 9.944330066939582, 9.162931925532463, 11.8680001020066, 0.0, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 98.89380625765318], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.731761213672993, 11.698994881774604, 11.22074688068535, 11.393785145055949, 9.548734790752288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.16242880696842], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 10.155633210306574, 11.202595781660758, 10.69123291440641, 11.439129446848145, 94.68338717140213], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.573089728358958, 0.0, 10.795539716597093, 9.382485758460357, 8.156515169162681, 10.108953651086749, 86.2148806997884], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.155633210306574, 10.795539716597093, 0.0, 8.183423253938845, 9.055460579237527, 8.857865582666564, 100.51802149006446], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.202595781660758, 9.382485758460357, 8.183423253938845, 0.0, 10.922031113226154, 8.777507498932518, 85.56329833615376], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.69123291440641, 8.156515169162681, 9.055460579237527, 10.922031113226154, 0.0, 11.48804541896262, 109.44902761831553], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.439129446848145, 10.108953651086749, 8.857865582666564, 8.777507498932518, 11.48804541896262, 0.0, 119.21419244342577]], 'time_step': 1.0, 'topology_type': 'clustered', 'skip_k': 5, 'symmetric': True, 'num_clusters': 3}\n",
      "\n",
      "topology clustered -> meta_data\n",
      "{'generated_at_utc': '2025-11-05T07:48:30Z', 'fingerprint': '9a3a6cc2768b6834', 'env': {'python': '3.10.9', 'user': 'niush'}, 'units': {'compute': 'CPU cycles per slot', 'links': 'MB per slot', 'time_step': 'seconds'}, 'notes': {'inputs_unit': {'compute': 'CPU cycles per second', 'links': 'MB per second'}, 'conversion': 'per_slot = per_second * time_step', 'topology_semantics': {'skip_connections': \"k-nearest ring; each MEC connects to next 'skip_k' neighbors on a circle\"}}, 'hyperparameters': {'number_of_servers': 18, 'time_step': 1.0, 'private_cpu_min': 1200000000.0, 'private_cpu_max': 1800000000.0, 'public_cpu_min': 500000000.0, 'public_cpu_max': 900000000.0, 'cpu_total_min': None, 'cpu_total_max': None, 'public_share': None, 'cloud_capacity': 30000000000.0, 'cloud_capacity_min': None, 'cloud_capacity_max': None, 'horiz_cap_min': 8.0, 'horiz_cap_max': 12.0, 'cloud_cap_min': 80.0, 'cloud_cap_max': 120.0, 'topology_type': 'clustered', 'skip_k': 5, 'symmetric': True, 'num_clusters': 3, 'inter_cluster_frac': 0.0, 'seed': 20251229}}\n"
     ]
    }
   ],
   "source": [
    "load_topologies_from_directory(topology_dir, verbose=True)\n",
    "\n",
    "print('topology clustered -> connection_matrix')\n",
    "display(topologies['clustered']['connection_matrix'].head())\n",
    "topologies['clustered']['connection_matrix'].info()\n",
    "\n",
    "print('\\ntopology clustered -> topology_data')\n",
    "print(topologies['clustered']['topology_data'])\n",
    "\n",
    "print('\\ntopology clustered -> meta_data')\n",
    "print(topologies['clustered']['meta_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2. Data Validation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data, we must validate that required columns exist and that IDs match properly.\n",
    "\n",
    "**The code below performs three layers of checks:** \n",
    "\n",
    "- Validate each dataset (episodes/agents/arrivals/tasks)\n",
    "- Validate each topology (JSON and connection matrix)\n",
    "- Validate dataset–topology pairs for unit alignment and overall consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Generic helpers ----------\n",
    "def _require(cond: bool, msg: str, errors: list):\n",
    "    # Collect errors instead of stopping at first failure\n",
    "    if not cond:\n",
    "        errors.append(msg)\n",
    "\n",
    "def _has_cols(df: pd.DataFrame, cols: list) -> bool:\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# ---------- Dataset-level validation ----------\n",
    "def validate_one_dataset(dataset_key: str, ds: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate a single dataset pack (episodes/agents/arrivals/tasks) for one (episode, scenario).\n",
    "    'dataset_key' is just a label for error messages, e.g. 'ep_000/heavy'.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds.get(\"episodes\")\n",
    "    agents   = ds.get(\"agents\")\n",
    "    arrivals = ds.get(\"arrivals\")\n",
    "    tasks    = ds.get(\"tasks\")\n",
    "\n",
    "    # 1) Presence checks\n",
    "    _require(isinstance(episodes, pd.DataFrame), f\"[{dataset_key}] episodes missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(agents,   pd.DataFrame), f\"[{dataset_key}] agents missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(arrivals, pd.DataFrame), f\"[{dataset_key}] arrivals missing or not a DataFrame\", errors)\n",
    "    _require(isinstance(tasks,    pd.DataFrame), f\"[{dataset_key}] tasks missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 2) Required columns (based on your generators)\n",
    "    req_ep_cols  = [\"scenario\", \"episode_id\", \"Delta\", \"T_slots\", \"hours\", \"N_agents\", \"seed\"]\n",
    "    req_ag_cols  = [\"agent_id\", \"f_local\", \"m_local\", \"lam_sec\"]\n",
    "    req_ar_cols  = [\"scenario\", \"episode_id\", \"t_slot\", \"t_time\", \"agent_id\", \"task_id\"]\n",
    "    req_tk_cols  = [\n",
    "        \"scenario\",\"episode_id\",\"task_id\",\"agent_id\",\"t_arrival_slot\",\"t_arrival_time\",\n",
    "        \"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\",\"modality\",\n",
    "        \"has_deadline\",\"deadline_s\",\"deadline_time\",\"non_atomic\",\"split_ratio\",\"action_space_hint\"\n",
    "    ]\n",
    "    _require(_has_cols(episodes, req_ep_cols), f\"[{dataset_key}] episodes missing required columns\", errors)\n",
    "    _require(_has_cols(agents,   req_ag_cols), f\"[{dataset_key}] agents missing required columns\", errors)\n",
    "    _require(_has_cols(arrivals, req_ar_cols), f\"[{dataset_key}] arrivals missing required columns\", errors)\n",
    "    _require(_has_cols(tasks,    req_tk_cols), f\"[{dataset_key}] tasks missing required columns\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    # 3) Integrity checks\n",
    "    # unique task_id\n",
    "    _require(tasks[\"task_id\"].is_unique, f\"[{dataset_key}] task_id is not unique\", errors)\n",
    "\n",
    "    # agent id range & count vs episodes.N_agents\n",
    "    if len(agents):\n",
    "        min_id = agents[\"agent_id\"].min()\n",
    "        max_id = agents[\"agent_id\"].max()\n",
    "        expected_n = int(episodes[\"N_agents\"].iloc[0])\n",
    "        _require(min_id == 0, f\"[{dataset_key}] agent_id should start at 0 (got {min_id})\", errors)\n",
    "        _require(max_id == expected_n - 1,\n",
    "                 f\"[{dataset_key}] agent_id max should be N_agents-1 ({expected_n-1}), got {max_id}\", errors)\n",
    "\n",
    "    # cross refs\n",
    "    valid_agents = set(agents[\"agent_id\"].tolist())\n",
    "    bad_arr_agents = set(arrivals[\"agent_id\"]) - valid_agents\n",
    "    bad_task_agents = set(tasks[\"agent_id\"]) - valid_agents\n",
    "    _require(len(bad_arr_agents) == 0, f\"[{dataset_key}] arrivals contain unknown agent_id(s): {sorted(bad_arr_agents)}\", errors)\n",
    "    _require(len(bad_task_agents) == 0, f\"[{dataset_key}] tasks contain unknown agent_id(s): {sorted(bad_task_agents)}\", errors)\n",
    "\n",
    "    # non-negative task numerics\n",
    "    for col in [\"b_mb\",\"rho_cyc_per_mb\",\"c_cycles\",\"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            _require((tasks[col] >= 0).all(), f\"[{dataset_key}] tasks.{col} has negative values\", errors)\n",
    "\n",
    "    # deadline coherence\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        bad_deadline = tasks[(tasks[\"has_deadline\"] == 1) & ((tasks[\"deadline_s\"].isna()) | (tasks[\"deadline_s\"] <= 0))]\n",
    "        _require(len(bad_deadline) == 0, f\"[{dataset_key}] tasks with deadline have invalid deadline_s\", errors)\n",
    "\n",
    "    # single Delta / T_slots inside this (episode, scenario)\n",
    "    _require(episodes[\"Delta\"].nunique() == 1, f\"[{dataset_key}] multiple Delta values in episodes\", errors)\n",
    "    _require(episodes[\"T_slots\"].nunique() == 1, f\"[{dataset_key}] multiple T_slots in episodes\", errors)\n",
    "\n",
    "    # arrivals inside range\n",
    "    T_slots = int(episodes[\"T_slots\"].iloc[0])\n",
    "    _require(int(tasks[\"t_arrival_slot\"].max()) <= T_slots - 1,\n",
    "             f\"[{dataset_key}] t_arrival_slot exceeds T_slots-1\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Topology-level validation ----------\n",
    "def validate_one_topology(topology_name: str, topo_entry: dict) -> list:\n",
    "    errors = []\n",
    "    topo = topo_entry.get(\"topology_data\")\n",
    "    meta = topo_entry.get(\"meta_data\")\n",
    "    Mdf  = topo_entry.get(\"connection_matrix\")\n",
    "\n",
    "    _require(isinstance(topo, dict), f\"[{topology_name}] topology_data missing or not a dict\", errors)\n",
    "    _require(isinstance(meta, dict), f\"[{topology_name}] meta_data missing or not a dict\", errors)\n",
    "    _require(isinstance(Mdf,  pd.DataFrame), f\"[{topology_name}] connection_matrix CSV missing or not a DataFrame\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    req_keys = [\n",
    "        \"number_of_servers\",\"private_cpu_capacities\",\"public_cpu_capacities\",\n",
    "        \"cloud_computational_capacity\",\"connection_matrix\",\"time_step\"\n",
    "    ]\n",
    "    for k in req_keys:\n",
    "        _require(k in topo, f\"[{topology_name}] topology.json missing key: {k}\", errors)\n",
    "    if errors:\n",
    "        return errors\n",
    "\n",
    "    K = int(topo[\"number_of_servers\"])\n",
    "    _require(len(topo[\"private_cpu_capacities\"]) == K, f\"[{topology_name}] private_cpu_capacities length != K\", errors)\n",
    "    _require(len(topo[\"public_cpu_capacities\"])  == K, f\"[{topology_name}] public_cpu_capacities length != K\", errors)\n",
    "\n",
    "    Mjson = topo[\"connection_matrix\"]\n",
    "    _require(isinstance(Mjson, list) and len(Mjson) == K and (K == 0 or len(Mjson[0]) == K+1),\n",
    "             f\"[{topology_name}] connection_matrix in JSON must be K x (K+1)\", errors)\n",
    "    _require(Mdf.shape == (K, K+1), f\"[{topology_name}] connection_matrix.csv shape must be K x (K+1)\", errors)\n",
    "\n",
    "    vert_csv = Mdf.iloc[:, K]\n",
    "    _require((vert_csv > 0).all(), f\"[{topology_name}] MEC->Cloud capacities must be > 0\", errors)\n",
    "    horiz_csv = Mdf.iloc[:, :K]\n",
    "    _require((horiz_csv.values >= 0).all(), f\"[{topology_name}] MEC<->MEC capacities contain negatives\", errors)\n",
    "\n",
    "    _require(\"time_step\" in topo, f\"[{topology_name}] missing time_step\", errors)\n",
    "    return errors\n",
    "\n",
    "# ---------- Pairwise validation (dataset <-> topology) ----------\n",
    "def validate_dataset_topology_pair(ep_name: str, scenario: str, ds: dict,\n",
    "                                   topology_name: str, topo_entry: dict) -> list:\n",
    "    \"\"\"\n",
    "    Validate alignment between one (episode, scenario) dataset and one topology.\n",
    "    Ensures Delta == time_step and basic feasibility checks.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    episodes = ds[\"episodes\"]\n",
    "    topo     = topo_entry[\"topology_data\"]\n",
    "    K        = int(topo[\"number_of_servers\"])\n",
    "\n",
    "    # Delta vs time_step\n",
    "    Delta = float(episodes[\"Delta\"].iloc[0])\n",
    "    time_step = float(topo[\"time_step\"])\n",
    "    _require(abs(Delta - time_step) < 1e-9,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] Delta ({Delta}) != time_step ({time_step})\", errors)\n",
    "\n",
    "    # Non-negative compute capacities\n",
    "    priv = topo[\"private_cpu_capacities\"]\n",
    "    pub  = topo[\"public_cpu_capacities\"]\n",
    "    cloud = topo[\"cloud_computational_capacity\"]\n",
    "    _require(all(x >= 0 for x in priv) and all(x >= 0 for x in pub) and cloud >= 0,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] negative compute capacities detected\", errors)\n",
    "\n",
    "    # Simple agent→MEC mapping (modulo) is within bounds\n",
    "    N_agents = int(episodes[\"N_agents\"].iloc[0])\n",
    "    mapped = [(aid % K) for aid in range(N_agents)] if K > 0 else []\n",
    "    _require(all(0 <= m < K for m in mapped) if mapped else True,\n",
    "             f\"[{ep_name}/{scenario} x {topology_name}] agent->MEC mapping out of bounds\", errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Episode-level Delta consistency across scenarios ----------\n",
    "def validate_episode_delta_consistency(ep_name: str, ep_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    Check that all SCENARIOS (light/moderate/heavy/...) inside one episode\n",
    "    share the same Delta and T_slots.\n",
    "\n",
    "    ep_dict:\n",
    "        {\n",
    "          \"light\":   {\"episodes\": df, ...},\n",
    "          \"moderate\":{...},\n",
    "          \"heavy\":   {...},\n",
    "          \"_meta\":   {...}  # we should ignore this\n",
    "        }\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    deltas = set()\n",
    "    tslots = set()\n",
    "\n",
    "    for scenario, ds in ep_dict.items():\n",
    "        # Discard metadata or anything that doesn't have episodes\n",
    "        if not isinstance(ds, dict) or \"episodes\" not in ds:\n",
    "            continue\n",
    "\n",
    "        ep_df = ds[\"episodes\"]\n",
    "        if len(ep_df):\n",
    "            deltas.add(float(ep_df[\"Delta\"].iloc[0]))\n",
    "            tslots.add(int(ep_df[\"T_slots\"].iloc[0]))\n",
    "        else:\n",
    "            errors.append(f\"[{ep_name}/{scenario}] episodes.csv is empty\")\n",
    "\n",
    "    if len(deltas) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple Delta values across scenarios: {sorted(deltas)}\")\n",
    "    if len(tslots) > 1:\n",
    "        errors.append(f\"[{ep_name}] multiple T_slots values across scenarios: {sorted(tslots)}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "# ---------- Orchestrator over ALL datasets (episode-first) and ALL topologies ----------\n",
    "def validate_everything_episode_first(datasets: dict, topologies: dict) -> dict:\n",
    "    \"\"\"\n",
    "    'datasets' shape (episode-first):\n",
    "\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}  # optional per-episode metadata\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    report = {\"datasets\": {}, \"episodes_consistency\": {}, \"topologies\": {}, \"pairs\": {}}\n",
    "\n",
    "    # 1) Validate each (episode, scenario)\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        report[\"datasets\"][ep_name] = {}\n",
    "\n",
    "        # Only real scenarios (not _meta)\n",
    "        scenario_names = [\n",
    "            scn for scn, dpack in ep_pack.items()\n",
    "            if isinstance(dpack, dict) and \"episodes\" in dpack\n",
    "        ]\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            dpack = ep_pack[scenario]\n",
    "            key = f\"{ep_name}/{scenario}\"\n",
    "            errs = validate_one_dataset(key, dpack)\n",
    "            report[\"datasets\"][ep_name][scenario] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 2) Episode-level Delta/T_slots consistency across scenarios\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        errs = validate_episode_delta_consistency(ep_name, ep_pack)\n",
    "        report[\"episodes_consistency\"][ep_name] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 3) Validate each topology\n",
    "    for tname, tpack in topologies.items():\n",
    "        errs = validate_one_topology(tname, tpack)\n",
    "        report[\"topologies\"][tname] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "\n",
    "    # 4) Pairwise validation for every valid (ep, scenario) × valid topology\n",
    "    for ep_name, ep_pack in datasets.items():\n",
    "        # Real scenarios (same as in report[\"datasets\"][ep_name])\n",
    "        scenario_names = list(report[\"datasets\"][ep_name].keys())\n",
    "\n",
    "        for scenario in scenario_names:\n",
    "            dpack = ep_pack[scenario]\n",
    "            d_ok  = report[\"datasets\"][ep_name][scenario][\"ok\"]\n",
    "            ep_ok = report[\"episodes_consistency\"][ep_name][\"ok\"]\n",
    "\n",
    "            for tname, tres in report[\"topologies\"].items():\n",
    "                key = f\"{ep_name}/{scenario}__{tname}\"\n",
    "                if d_ok and ep_ok and tres[\"ok\"]:\n",
    "                    errs = validate_dataset_topology_pair(ep_name, scenario, dpack, tname, topologies[tname])\n",
    "                    report[\"pairs\"][key] = {\"ok\": len(errs) == 0, \"errors\": errs}\n",
    "                else:\n",
    "                    report[\"pairs\"][key] = {\n",
    "                        \"ok\": False,\n",
    "                        \"errors\": [\"Skipped due to upstream invalid dataset/episode/topology.\"]\n",
    "                    }\n",
    "\n",
    "    return report\n",
    "\n",
    "# ---------- Pretty printer ----------\n",
    "def print_validation_report_episode_first(report: dict):\n",
    "    print(\"=== DATASETS (episode/scenario) ===\")\n",
    "    for ep_name, ep_res in report[\"datasets\"].items():\n",
    "        for scenario, info in ep_res.items():\n",
    "            status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "            print(f\"[{status}] {ep_name}/{scenario}\")\n",
    "            for e in info[\"errors\"]:\n",
    "                print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\")\n",
    "    for ep_name, info in report[\"episodes_consistency\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {ep_name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== TOPOLOGIES ===\")\n",
    "    for name, info in report[\"topologies\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {name}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")\n",
    "\n",
    "    print(\"\\n=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\")\n",
    "    for key, info in report[\"pairs\"].items():\n",
    "        status = \"OK\" if info[\"ok\"] else \"FAIL\"\n",
    "        print(f\"[{status}] {key}\")\n",
    "        for e in info[\"errors\"]:\n",
    "            print(f\"  - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (episode/scenario) ===\n",
      "[OK] ep_000/heavy\n",
      "[OK] ep_000/light\n",
      "[OK] ep_000/moderate\n",
      "\n",
      "=== EPISODE-LEVEL CONSISTENCY (Delta & T_slots) ===\n",
      "[OK] ep_000\n",
      "\n",
      "=== TOPOLOGIES ===\n",
      "[OK] clustered\n",
      "[OK] full_mesh\n",
      "[OK] sparse_ring\n",
      "\n",
      "=== (EPISODE/SCENARIO) × TOPOLOGY PAIRS ===\n",
      "[OK] ep_000/heavy__clustered\n",
      "[OK] ep_000/heavy__full_mesh\n",
      "[OK] ep_000/heavy__sparse_ring\n",
      "[OK] ep_000/light__clustered\n",
      "[OK] ep_000/light__full_mesh\n",
      "[OK] ep_000/light__sparse_ring\n",
      "[OK] ep_000/moderate__clustered\n",
      "[OK] ep_000/moderate__full_mesh\n",
      "[OK] ep_000/moderate__sparse_ring\n"
     ]
    }
   ],
   "source": [
    "report = validate_everything_episode_first(datasets, topologies)\n",
    "print_validation_report_episode_first(report)\n",
    "\n",
    "all_ok = (\n",
    "    all(info[\"ok\"] for ep in report[\"datasets\"].values() for info in ep.values())\n",
    "    and all(info[\"ok\"] for info in report[\"episodes_consistency\"].values())\n",
    "    and all(info[\"ok\"] for info in report[\"topologies\"].values())\n",
    "    and all(info[\"ok\"] for info in report[\"pairs\"].values())\n",
    ")\n",
    "if not all_ok:\n",
    "    raise RuntimeError(\"Validation failed. See printed report for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3. Units Alignment </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we align units for all dataset episodes and scenarios\n",
    "and run consistency checks against all topologies.\n",
    "- Datasets: use Delta from episodes.csv; add per-slot helpers:\n",
    "    agents.f_local_slot (cycles/slot), tasks.deadline_slots (integer or NaN)\n",
    "    \n",
    "- Topologies: capacities are already per-slot (generator multiplied by Δ);\n",
    "    we only verify time_step == Delta and non-negative capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Helpers: safe getters =====\n",
    "def _get_delta(episodes_df: pd.DataFrame) -> float:\n",
    "    # Expect a single Delta value in episodes; take the first row\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain a 'Delta' column.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _ensure_numeric_positive(name: str, arr: np.ndarray):\n",
    "    # Basic sanity: finite and no negatives for capacities/links\n",
    "    if not np.isfinite(arr).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "    if (arr < 0).any():\n",
    "        raise ValueError(f\"{name} contains negative values.\")\n",
    "\n",
    "# ===== Alignment: per-dataset (one (episode, scenario) pack) =====\n",
    "def align_units_for_dataset(dataset: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given one dataset dict {\"episodes\",\"agents\",\"arrivals\",\"tasks\"},\n",
    "    return a copy with aligned/derived columns (per-slot helpers).\n",
    "    \"\"\"\n",
    "    episodes = dataset[\"episodes\"].copy()\n",
    "    agents   = dataset[\"agents\"].copy()\n",
    "    arrivals = dataset[\"arrivals\"].copy()\n",
    "    tasks    = dataset[\"tasks\"].copy()\n",
    "\n",
    "    Delta = _get_delta(episodes)\n",
    "\n",
    "    # Agents: add per-slot compute capacity helper (cycles/slot)\n",
    "    if \"f_local\" not in agents.columns:\n",
    "        raise ValueError(\"agents.csv must contain 'f_local'.\")\n",
    "    agents[\"f_local\"] = agents[\"f_local\"].astype(float)\n",
    "    agents[\"f_local_slot\"] = agents[\"f_local\"] * Delta\n",
    "\n",
    "    # Memory is MB; keep as float\n",
    "    if \"m_local\" in agents.columns:\n",
    "        agents[\"m_local\"] = agents[\"m_local\"].astype(float)\n",
    "\n",
    "    # Tasks: ensure integer arrival slot\n",
    "    if \"t_arrival_slot\" not in tasks.columns:\n",
    "        raise ValueError(\"tasks.csv must contain 't_arrival_slot'.\")\n",
    "    tasks[\"t_arrival_slot\"] = tasks[\"t_arrival_slot\"].astype(int)\n",
    "\n",
    "    # Build deadline_slots = ceil(deadline_s / Delta) when has_deadline == 1, else NaN\n",
    "    if \"has_deadline\" in tasks.columns and \"deadline_s\" in tasks.columns:\n",
    "        def _to_deadline_slots(row):\n",
    "            if int(row[\"has_deadline\"]) == 1 and np.isfinite(row[\"deadline_s\"]):\n",
    "                return int(math.ceil(float(row[\"deadline_s\"]) / Delta))\n",
    "            return np.nan\n",
    "        tasks[\"deadline_slots\"] = tasks.apply(_to_deadline_slots, axis=1)\n",
    "        # Keep as nullable integer when possible\n",
    "        try:\n",
    "            tasks[\"deadline_slots\"] = tasks[\"deadline_slots\"].astype(\"Int64\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Ensure key numeric task fields are floats\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\"]:\n",
    "        if col in tasks.columns:\n",
    "            tasks[col] = tasks[col].astype(float)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": episodes,\n",
    "        \"agents\":   agents,\n",
    "        \"arrivals\": arrivals,\n",
    "        \"tasks\":    tasks,\n",
    "    }\n",
    "\n",
    "# ===== Verification: per-topology against a target Delta =====\n",
    "def verify_topology_units(topology: Dict[str, Any], target_Delta: float) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ensure topology capacities are per-slot and consistent with dataset Delta:\n",
    "    - time_step == target_Delta\n",
    "    - shapes are valid (K x (K+1))\n",
    "    - capacities non-negative\n",
    "    Returns (ok, message).\n",
    "    \"\"\"\n",
    "    # time_step check\n",
    "    ts = float(topology.get(\"time_step\", -1.0))\n",
    "    if not np.isclose(ts, target_Delta, atol=1e-9):\n",
    "        return (False, f\"time_step mismatch (topology={ts}, dataset Delta={target_Delta})\")\n",
    "\n",
    "    # K and lists\n",
    "    K = int(topology.get(\"number_of_servers\", -1))\n",
    "    priv = np.array(topology.get(\"private_cpu_capacities\", []), dtype=float)\n",
    "    pub  = np.array(topology.get(\"public_cpu_capacities\", []), dtype=float)\n",
    "    cloud = float(topology.get(\"cloud_computational_capacity\", -1.0))\n",
    "    M = np.array(topology.get(\"connection_matrix\", []), dtype=float)\n",
    "\n",
    "    if K <= 0:\n",
    "        return (False, \"Invalid 'number_of_servers' (K<=0).\")\n",
    "    if priv.shape[0] != K or pub.shape[0] != K:\n",
    "        return (False, \"private/public capacities must have length K.\")\n",
    "    if M.shape != (K, K+1):\n",
    "        return (False, f\"connection_matrix shape must be (K, K+1), got {M.shape}.\")\n",
    "\n",
    "    # Non-negative checks\n",
    "    _ensure_numeric_positive(\"private_cpu_capacities\", priv)\n",
    "    _ensure_numeric_positive(\"public_cpu_capacities\",  pub)\n",
    "    if not np.isfinite(cloud) or cloud < 0:\n",
    "        return (False, \"cloud_computational_capacity must be non-negative and finite.\")\n",
    "    _ensure_numeric_positive(\"connection_matrix\", M)\n",
    "\n",
    "    return (True, \"topology verified (per-slot, consistent).\")\n",
    "\n",
    "# ===== Batch alignment for ALL datasets (episode-first) & ALL topologies =====\n",
    "def align_all_units_episode_first(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    topologies_by_name: Dict[str, Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input 'datasets_ep_first' shape:\n",
    "\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}   # optional per-episode metadata (NO episodes/agents/tasks)\n",
    "          },\n",
    "          \"ep_001\": {...}\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"datasets_aligned\": { ep_name: { scenario: aligned_pack_or_meta, ... }, ... },\n",
    "          \"topology_checks\":  { topo_name: { ep_name: { scenario: {ok, message} } } }\n",
    "        }\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"datasets_aligned\": {},\n",
    "        \"topology_checks\":  {}\n",
    "    }\n",
    "\n",
    "    # ---- 1) Align datasets (episode/scenario) ----\n",
    "    for ep_name, ep_pack in datasets_ep_first.items():\n",
    "        out[\"datasets_aligned\"][ep_name] = {}\n",
    "\n",
    "        for scenario, ds in ep_pack.items():\n",
    "            # If the dataset is real (has episodes) → align\n",
    "            if isinstance(ds, dict) and \"episodes\" in ds:\n",
    "                try:\n",
    "                    out[\"datasets_aligned\"][ep_name][scenario] = align_units_for_dataset(ds)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"[{ep_name}/{scenario}] dataset alignment failed: {e}\") from e\n",
    "            else:\n",
    "                # For example \"_meta\" or anything else → we keep it as is (no changes)\n",
    "                out[\"datasets_aligned\"][ep_name][scenario] = ds\n",
    "\n",
    "    # ---- 2) Verify each topology against each (episode, scenario) Delta ----\n",
    "    for topo_name, topo_bundle in topologies_by_name.items():\n",
    "        topo_obj = topo_bundle.get(\"topology_data\", None)\n",
    "        if not isinstance(topo_obj, dict):\n",
    "            raise RuntimeError(f\"[{topo_name}] 'topology_data' missing or not a dict.\")\n",
    "        out[\"topology_checks\"][topo_name] = {}\n",
    "\n",
    "        for ep_name, ep_pack in out[\"datasets_aligned\"].items():\n",
    "            out[\"topology_checks\"][topo_name][ep_name] = {}\n",
    "\n",
    "            for scenario, aligned in ep_pack.items():\n",
    "                # Only check scenarios that have episodes; ignore metadata\n",
    "                if not (isinstance(aligned, dict) and \"episodes\" in aligned):\n",
    "                    continue\n",
    "\n",
    "                Delta = _get_delta(aligned[\"episodes\"])\n",
    "                ok, msg = verify_topology_units(topo_obj, Delta)\n",
    "                out[\"topology_checks\"][topo_name][ep_name][scenario] = {\n",
    "                    \"ok\": bool(ok),\n",
    "                    \"message\": msg\n",
    "                }\n",
    "\n",
    "    return out\n",
    "\n",
    "# ===== Pretty printer (episode-first) =====\n",
    "def print_alignment_summary_episode_first(result: Dict[str, Any]):\n",
    "    # ===== DATASETS =====\n",
    "    print(\"=== DATASETS (aligned, episode/scenario) ===\")\n",
    "    for ep_name in sorted(result[\"datasets_aligned\"].keys()):\n",
    "        ep_pack = result[\"datasets_aligned\"][ep_name]\n",
    "\n",
    "        for scenario in sorted(ep_pack.keys()):\n",
    "            ds = ep_pack[scenario]\n",
    "            if not (isinstance(ds, dict) and \"episodes\" in ds):\n",
    "                continue\n",
    "\n",
    "            Delta    = _get_delta(ds[\"episodes\"])\n",
    "            n_tasks  = len(ds[\"tasks\"])\n",
    "            n_agents = len(ds[\"agents\"])\n",
    "            print(f\"[{ep_name}/{scenario}] Delta={Delta}  tasks={n_tasks}  agents={n_agents}\")\n",
    "\n",
    "    # ===== TOPOLOGIES =====\n",
    "    print(\"\\n=== TOPOLOGIES (checks vs each episode/scenario) ===\")\n",
    "    for topo_name, by_ep in result[\"topology_checks\"].items():\n",
    "        print(f\"Topology: {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            for scenario, r in sorted(by_ep[ep_name].items()):\n",
    "                flag = \"OK\" if r[\"ok\"] else \"FAIL\"\n",
    "                print(f\"  - {ep_name}/{scenario}: {flag}  -> {r['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASETS (aligned, episode/scenario) ===\n",
      "[ep_000/heavy] Delta=1.0  tasks=841  agents=18\n",
      "[ep_000/light] Delta=1.0  tasks=70  agents=18\n",
      "[ep_000/moderate] Delta=1.0  tasks=261  agents=18\n",
      "\n",
      "=== TOPOLOGIES (checks vs each episode/scenario) ===\n",
      "Topology: clustered\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: full_mesh\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "Topology: sparse_ring\n",
      "  - ep_000/heavy: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/light: OK  -> topology verified (per-slot, consistent).\n",
      "  - ep_000/moderate: OK  -> topology verified (per-slot, consistent).\n",
      "\n",
      " ===EXAMPLE===\n"
     ]
    }
   ],
   "source": [
    "result_align = align_all_units_episode_first(\n",
    "    datasets_ep_first=datasets,\n",
    "    topologies_by_name=topologies\n",
    ")\n",
    "print_alignment_summary_episode_first(result_align)\n",
    "\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "aligned_light_ep0 = result_align[\"datasets_aligned\"][\"ep_000\"][\"light\"]\n",
    "agents_ep0_light  = aligned_light_ep0[\"agents\"]   # has f_local_slot\n",
    "tasks_ep0_light   = aligned_light_ep0[\"tasks\"]    # has deadline_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.4. Build Scenario–Topology Pairs </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all datasets are paired with all topologies (Cartesian product). Each pair is checked for matching time parameters, then a basic bundle is created for further enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _delta_from_episodes(episodes_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Extract a single Delta value from episodes table.\"\"\"\n",
    "    if \"Delta\" not in episodes_df.columns:\n",
    "        raise ValueError(\"episodes.csv must contain 'Delta'.\")\n",
    "    return float(episodes_df[\"Delta\"].iloc[0])\n",
    "\n",
    "def _topology_time_step(topo_json: Dict[str, Any]) -> float:\n",
    "    \"\"\"Extract the topology time_step.\"\"\"\n",
    "    ts = topo_json.get(\"time_step\", None)\n",
    "    if ts is None:\n",
    "        raise ValueError(\"topology.json must contain 'time_step'.\")\n",
    "    return float(ts)\n",
    "\n",
    "def build_topology_episode_pairs(\n",
    "    datasets_ep_first: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    topologies: Dict[str, Dict[str, Any]],\n",
    "    strict_delta_match: bool = True\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Build pairs between every topology and every (episode, scenario) dataset.\n",
    "\n",
    "    datasets_ep_first شکل:\n",
    "        {\n",
    "          \"ep_000\": {\n",
    "             \"light\":   {\"episodes\": df, \"agents\": df, \"arrivals\": df, \"tasks\": df},\n",
    "             \"moderate\":{...},\n",
    "             \"heavy\":   {...},\n",
    "             \"_meta\":   {...}  # only meta data without episodes/agents/tasks\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    # Iterate topologies first (topology-centric)\n",
    "    for topo_name, topo_bundle in topologies.items():\n",
    "        topo_data = topo_bundle.get(\"topology_data\", None)\n",
    "        meta_data = topo_bundle.get(\"meta_data\", None)\n",
    "        cm_df     = topo_bundle.get(\"connection_matrix\", None)\n",
    "\n",
    "        if not isinstance(topo_data, dict):\n",
    "            raise ValueError(f\"[{topo_name}] topology_data missing or not a dict.\")\n",
    "        if cm_df is None:\n",
    "            raise ValueError(f\"[{topo_name}] connection_matrix DataFrame is missing.\")\n",
    "\n",
    "        # Validate K and connection matrix shape\n",
    "        K = int(topo_data.get(\"number_of_servers\", -1))\n",
    "        if K <= 0:\n",
    "            raise ValueError(f\"[{topo_name}] invalid 'number_of_servers' in topology.json\")\n",
    "        if not (cm_df.shape[0] == K and cm_df.shape[1] == K + 1):\n",
    "            raise ValueError(\n",
    "                f\"[{topo_name}] connection_matrix shape must be (K, K+1); got {cm_df.shape}\"\n",
    "            )\n",
    "\n",
    "        topo_ts = _topology_time_step(topo_data)\n",
    "\n",
    "        # Prepare container for this topology\n",
    "        pairs_by_topology[topo_name] = {}\n",
    "\n",
    "        # Compare with every (episode, scenario)\n",
    "        for ep_name, scenarios in datasets_ep_first.items():\n",
    "            pairs_by_topology[topo_name][ep_name] = {}\n",
    "            for scen_name, ds in scenarios.items():\n",
    "                if not (isinstance(ds, dict) and \"episodes\" in ds):\n",
    "                    continue\n",
    "\n",
    "                ds_Delta = _delta_from_episodes(ds[\"episodes\"])\n",
    "                delta_ok = bool(np.isclose(ds_Delta, topo_ts, atol=1e-12))\n",
    "                msg = \"OK\" if delta_ok else (\n",
    "                    f\"time_step mismatch (dataset Delta={ds_Delta}, topology time_step={topo_ts})\"\n",
    "                )\n",
    "                if (not delta_ok) and strict_delta_match:\n",
    "                    raise ValueError(f\"[{topo_name} × {ep_name}/{scen_name}] {msg}\")\n",
    "\n",
    "                # Store bundle\n",
    "                pairs_by_topology[topo_name][ep_name][scen_name] = {\n",
    "                    \"scenario\": scen_name,\n",
    "                    \"episode\": ep_name,\n",
    "                    \"topology\": topo_name,\n",
    "                    \"Delta\": ds_Delta,\n",
    "                    \"K\": K,\n",
    "                    \"dataset\": ds,\n",
    "                    \"topology_data\": topo_data,\n",
    "                    \"topology_meta_data\": meta_data,\n",
    "                    \"connection_matrix_df\": cm_df,\n",
    "                    \"checks\": {\"delta_match\": delta_ok, \"message\": msg}\n",
    "                }\n",
    "\n",
    "    return pairs_by_topology\n",
    "\n",
    "def print_pairs_summary_topology_first_ep(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> None:\n",
    "    \"\"\"Pretty-print summary as topology → episode → scenario.\"\"\"\n",
    "    print(\"=== TOPOLOGY × EPISODE × SCENARIO ===\")\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        print(f\"[TOPOLOGY] {topo_name}\")\n",
    "        for ep_name in sorted(by_ep.keys()):\n",
    "            scen_map = by_ep[ep_name]\n",
    "            if not scen_map:\n",
    "                print(f\"  ├─ Episode: {ep_name}  (no paired scenarios)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  ├─ Episode: {ep_name}\")\n",
    "            for scen_name in sorted(scen_map.keys()):\n",
    "                bundle = scen_map[scen_name]\n",
    "                flag  = \"OK\" if bundle[\"checks\"][\"delta_match\"] else \"FAIL\"\n",
    "                K     = bundle[\"K\"]\n",
    "                Delta = bundle[\"Delta\"]\n",
    "                msg   = bundle[\"checks\"][\"message\"]\n",
    "                print(f\"  │    - [{flag}] {scen_name:9s} | K={K:2d}  Δ={Delta:g}  -> {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPOLOGY × EPISODE × SCENARIO ===\n",
      "[TOPOLOGY] clustered\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n",
      "[TOPOLOGY] full_mesh\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n",
      "[TOPOLOGY] sparse_ring\n",
      "  ├─ Episode: ep_000\n",
      "  │    - [OK] heavy     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] light     | K=18  Δ=1  -> OK\n",
      "  │    - [OK] moderate  | K=18  Δ=1  -> OK\n",
      "\n",
      " ===EXAMPLE===\n"
     ]
    }
   ],
   "source": [
    "# --- Example driver (with your current variables) ---\n",
    "result_align = align_all_units_episode_first(datasets_ep_first=datasets,\n",
    "                                             topologies_by_name=topologies)\n",
    "\n",
    "datasets_aligned = result_align[\"datasets_aligned\"]\n",
    "\n",
    "pairs_by_topology = build_topology_episode_pairs(\n",
    "    datasets_ep_first=datasets_aligned,\n",
    "    topologies=topologies,\n",
    "    strict_delta_match=True\n",
    ")\n",
    "\n",
    "print_pairs_summary_topology_first_ep(pairs_by_topology)\n",
    "\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "tasks_light = pairs_by_topology[\"full_mesh\"][\"ep_000\"][\"light\"][\"dataset\"][\"tasks\"]\n",
    "cm_clustered = pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"connection_matrix_df\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.5. Agent→MEC mapping (for all pairs) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent → MEC Mapping assigns each agent to a specific MEC server.\n",
    "This creates a fixed mec_id for every agent (e.g., agent_id % K), which determines where its tasks are initially queued and processed in the MDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_agents_to_mecs(pairs_by_topology):\n",
    "    \"\"\"\n",
    "    Adds agent→MEC mapping to each (topology / ep / scenario) bundle.\n",
    "    - Rule: mec_id = agent_id % K\n",
    "    - Writes:\n",
    "        bundle[\"agent_to_mec\"]                  (pd.Series, index=agent_id)\n",
    "        bundle[\"dataset\"][\"agents\"][\"mec_id\"]   (added column)\n",
    "    \"\"\"\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            for scen_name, bundle in by_scen.items():\n",
    "\n",
    "                ds = bundle[\"dataset\"]\n",
    "                agents = ds[\"agents\"].copy()\n",
    "                K = int(bundle[\"K\"])\n",
    "\n",
    "                if \"agent_id\" not in agents.columns:\n",
    "                    raise ValueError(f\"[{topo_name}/{ep_name}/{scen_name}] agents.csv missing 'agent_id'.\")\n",
    "\n",
    "                # Ensure agent_id is contiguous & sorted (0..N_agents-1)\n",
    "                agents = agents.sort_values(\"agent_id\").reset_index(drop=True)\n",
    "                expected_n = int(bundle[\"dataset\"][\"episodes\"][\"N_agents\"].iloc[0])\n",
    "                if agents[\"agent_id\"].min() != 0 or agents[\"agent_id\"].max() != expected_n - 1:\n",
    "                    raise ValueError(f\"[{topo_name}/{ep_name}/{scen_name}] agent_id range not 0..N_agents-1.\")\n",
    "\n",
    "                # Mapping\n",
    "                mec_ids = (agents[\"agent_id\"].astype(int) % K).astype(int)\n",
    "                agents[\"mec_id\"] = mec_ids\n",
    "\n",
    "                # Store: dataset copy + Series with index=agent_id\n",
    "                ds[\"agents\"] = agents\n",
    "                bundle[\"agent_to_mec\"] = pd.Series(\n",
    "                    data=mec_ids.values,\n",
    "                    index=agents[\"agent_id\"].values,\n",
    "                    name=\"mec_id\"\n",
    "                )\n",
    "\n",
    "    return pairs_by_topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===EXAMPLE===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lam_sec</th>\n",
       "      <th>f_local_slot</th>\n",
       "      <th>mec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>0.228174</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>0.310369</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  agent_id       f_local      m_local   lam_sec  f_local_slot  \\\n",
       "0    heavy         0  1.741183e+09  5713.849721  0.708673  1.741183e+09   \n",
       "1    heavy         1  1.352326e+09  4566.428755  0.234989  1.352326e+09   \n",
       "2    heavy         2  1.726668e+09  5815.120004  0.228174  1.726668e+09   \n",
       "3    heavy         3  1.543616e+09  3539.850245  0.310369  1.543616e+09   \n",
       "4    heavy         4  1.130883e+09  4161.367769  0.548990  1.130883e+09   \n",
       "\n",
       "   mec_id  \n",
       "0       0  \n",
       "1       1  \n",
       "2       2  \n",
       "3       3  \n",
       "4       4  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply mapping\n",
    "pairs_by_topology = assign_agents_to_mecs(pairs_by_topology)\n",
    "\n",
    "# Quick sanity peek\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "pairs_by_topology[\"clustered\"][\"ep_000\"][\"heavy\"][\"dataset\"][\"agents\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.6. Environment Configuration </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we build a unified env_config for each scenario–topology pair.\n",
    "It bundles all required information for the MDP/RL environment—such as compute capacities, the Agent→MEC mapping, connection matrix, initial queue states, and action/state specifications—into a single consistent configuration used by the RL training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_core_from_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract core fields from a (topology × episode × scenario) bundle.\n",
    "    Ensures required fields exist and converts structures to numpy/DF formats.\n",
    "    \"\"\"\n",
    "    required = [\"dataset\", \"topology_data\", \"connection_matrix_df\", \"Delta\", \"K\"]\n",
    "    for k in required:\n",
    "        if k not in bundle:\n",
    "            raise ValueError(f\"Bundle missing required key: '{k}'\")\n",
    "\n",
    "    ds = bundle[\"dataset\"]\n",
    "    topo = bundle[\"topology_data\"]\n",
    "    Mdf = bundle[\"connection_matrix_df\"]\n",
    "\n",
    "    private_cpu = np.asarray(topo[\"private_cpu_capacities\"], dtype=float)\n",
    "    public_cpu = np.asarray(topo[\"public_cpu_capacities\"], dtype=float)\n",
    "    cloud_cpu = float(topo[\"cloud_computational_capacity\"])\n",
    "    M = Mdf.to_numpy(dtype=float)  # shape = (K, K+1), last column = MEC→Cloud\n",
    "\n",
    "    return dict(\n",
    "        Delta=float(bundle[\"Delta\"]),\n",
    "        K=int(bundle[\"K\"]),\n",
    "        episodes=ds[\"episodes\"],\n",
    "        agents=ds[\"agents\"],\n",
    "        arrivals=ds[\"arrivals\"],\n",
    "        tasks=ds[\"tasks\"],\n",
    "        private_cpu=private_cpu,\n",
    "        public_cpu=public_cpu,\n",
    "        cloud_cpu=cloud_cpu,\n",
    "        connection_matrix=M,\n",
    "        topology_type=topo.get(\"topology_type\", \"unknown\"),\n",
    "    )\n",
    "    \n",
    "def _build_default_queues(K: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initial queue states for MEC and Cloud tiers, in per-slot units:\n",
    "      - *_cycles store queued CPU cycles.\n",
    "      - mec_bytes_in_transit stores bytes currently being transmitted through MEC links.\n",
    "      - cloud_cycles stores queued cycles at the cloud.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mec_local_cycles\": np.zeros(K, dtype=float),\n",
    "        \"mec_public_cycles\": np.zeros(K, dtype=float),\n",
    "        \"mec_bytes_in_transit\": np.zeros(K, dtype=float),\n",
    "        \"cloud_cycles\": np.array([0.0], dtype=float),\n",
    "    }\n",
    "\n",
    "def _derive_action_space() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Basic discrete offloading action space (HOODIE-style):\n",
    "        0 = Execute locally\n",
    "        1 = Offload to another MEC server\n",
    "        2 = Offload to Cloud\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"type\": \"discrete\",\n",
    "        \"n\": 3,\n",
    "        \"labels\": {\n",
    "            0: \"LOCAL\",\n",
    "            1: \"MEC\",\n",
    "            2: \"CLOUD\",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def _derive_state_spec(K: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Declarative specification of the RL state structure.\n",
    "    The environment uses this to assemble numerical tensors each step.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"components\": {\n",
    "            \"queues\": {\n",
    "                \"mec_local_cycles\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"mec_public_cycles\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"mec_bytes_in_transit\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"cloud_cycles\": {\"shape\": (1,), \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"links\": {\n",
    "                \"connection_matrix\": {\"shape\": (K, K + 1), \"dtype\": \"float\"},\n",
    "            },\n",
    "            \"capacities\": {\n",
    "                \"private_cpu\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"public_cpu\": {\"shape\": (K,), \"dtype\": \"float\"},\n",
    "                \"cloud_cpu\": {\"shape\": (1,), \"dtype\": \"float\"},\n",
    "            },\n",
    "        },\n",
    "        \"note\": \"Declarative state description; environment assembles numerical tensors at runtime.\",\n",
    "    }\n",
    "    \n",
    "def build_env_config_for_bundle(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build the complete environment configuration structure for a single\n",
    "    (topology × episode × scenario) bundle.\n",
    "\n",
    "    Output 'env_config' includes:\n",
    "        - Time parameters: Delta, T_slots\n",
    "        - Topology specification: K, connection_matrix, topology_type\n",
    "        - Resource capacities: private/public/cloud CPU\n",
    "        - Agent-to-MEC assignment\n",
    "        - Aligned dataset tables (episodes, agents, arrivals, tasks)\n",
    "        - Initial queue states\n",
    "        - Action space and state specification\n",
    "        - Consistency check results\n",
    "    \"\"\"\n",
    "    core = _extract_core_from_bundle(bundle)\n",
    "\n",
    "    # Ensure agent-to-MEC mapping is present\n",
    "    if \"agent_to_mec\" not in bundle:\n",
    "        raise ValueError(\"Bundle missing 'agent_to_mec'. Run Stage 5 first.\")\n",
    "\n",
    "    # Normalize agent_to_mec to numpy array ordered by agent_id\n",
    "    agent_to_mec = bundle[\"agent_to_mec\"]\n",
    "    if isinstance(agent_to_mec, pd.Series):\n",
    "        if agent_to_mec.index.name != \"agent_id\":\n",
    "            agent_to_mec.index.name = \"agent_id\"\n",
    "\n",
    "        index_order = core[\"agents\"].sort_values(\"agent_id\")[\"agent_id\"].to_numpy()\n",
    "        agent_to_mec = agent_to_mec.reindex(index_order)\n",
    "        agent_to_mec_arr = agent_to_mec.to_numpy(dtype=int)\n",
    "    else:\n",
    "        agent_to_mec_arr = np.asarray(agent_to_mec, dtype=int)\n",
    "\n",
    "    # Validate correct length\n",
    "    N_agents = int(core[\"episodes\"][\"N_agents\"].iloc[0])\n",
    "    if len(agent_to_mec_arr) != N_agents:\n",
    "        raise ValueError(\n",
    "            f\"agent_to_mec length ({len(agent_to_mec_arr)}) != N_agents ({N_agents})\"\n",
    "        )\n",
    "\n",
    "    # Extract simulation horizon\n",
    "    if \"T_slots\" not in core[\"episodes\"].columns:\n",
    "        raise ValueError(\"episodes.csv must contain 'T_slots'.\")\n",
    "    T_slots = int(core[\"episodes\"][\"T_slots\"].iloc[0])\n",
    "\n",
    "    # Build initial states and specifications\n",
    "    queues_initial = _build_default_queues(core[\"K\"])\n",
    "    action_space = _derive_action_space()\n",
    "    state_spec = _derive_state_spec(core[\"K\"])\n",
    "\n",
    "    # Final environment configuration object\n",
    "    env_config = {\n",
    "        \"Delta\": core[\"Delta\"],\n",
    "        \"T_slots\": T_slots,\n",
    "        \"K\": core[\"K\"],\n",
    "        \"topology_type\": core[\"topology_type\"],\n",
    "        \"connection_matrix\": core[\"connection_matrix\"],\n",
    "\n",
    "        \"private_cpu\": core[\"private_cpu\"],\n",
    "        \"public_cpu\": core[\"public_cpu\"],\n",
    "        \"cloud_cpu\": core[\"cloud_cpu\"],\n",
    "\n",
    "        \"N_agents\": N_agents,\n",
    "        \"agent_to_mec\": agent_to_mec_arr,\n",
    "\n",
    "        # Datasets (aligned)\n",
    "        \"episodes\": core[\"episodes\"],\n",
    "        \"agents\": core[\"agents\"],\n",
    "        \"arrivals\": core[\"arrivals\"],\n",
    "        \"tasks\": core[\"tasks\"],\n",
    "\n",
    "        # Initial queue states and specifications\n",
    "        \"queues_initial\": queues_initial,\n",
    "        \"action_space\": action_space,\n",
    "        \"state_spec\": state_spec,\n",
    "\n",
    "        # Validation results from delta/time-step checks\n",
    "        \"checks\": bundle.get(\"checks\", {\"delta_match\": True, \"message\": \"n/a\"}),\n",
    "    }\n",
    "    return env_config\n",
    "\n",
    "def build_all_env_configs(\n",
    "    pairs_by_topology: Dict[str, Dict[str, Dict[str, Any]]]\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Build environment configurations for all (topology × episode × scenario) bundles.\n",
    "\n",
    "    Result shape (episode-first):\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]] = {}\n",
    "\n",
    "    for topo_name, by_ep in pairs_by_topology.items():\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            if ep_name not in out:\n",
    "                out[ep_name] = {}\n",
    "            if topo_name not in out[ep_name]:\n",
    "                out[ep_name][topo_name] = {}\n",
    "\n",
    "            for scen_name, bundle in by_scen.items():\n",
    "                if \"agent_to_mec\" not in bundle:\n",
    "                    raise RuntimeError(\n",
    "                        f\"[{topo_name}/{ep_name}/{scen_name}] missing 'agent_to_mec'. \"\n",
    "                        \"Run Stage 5 first.\"\n",
    "                    )\n",
    "                env_cfg = build_env_config_for_bundle(bundle)\n",
    "                out[ep_name][topo_name][scen_name] = env_cfg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE ===\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "T_slots: 110\n",
      "Initial queues: dict_keys(['mec_local_cycles', 'mec_public_cycles', 'mec_bytes_in_transit', 'cloud_cycles'])\n"
     ]
    }
   ],
   "source": [
    "# Build all environment configs\n",
    "env_configs = build_all_env_configs(pairs_by_topology)\n",
    "\n",
    "# Example\n",
    "print(\"\\n=== EXAMPLE ===\")\n",
    "print(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"agent_to_mec\"])\n",
    "print(\"T_slots:\", env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"T_slots\"])\n",
    "print(\"Initial queues:\", env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"queues_initial\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.7. Sanity Checks </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we verify that each env_config is internally consistent (queue shapes, capacities, agent→MEC mapping, and connection matrix are valid and ready for simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_env_config(env_config: Dict[str, Any]) -> list:\n",
    "    \"\"\"\n",
    "    Run basic sanity checks on a single env_config dictionary.\n",
    "    Returns a list of error strings; empty list means 'no issues found'.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    # 1) Agent → MEC alignment\n",
    "    N_agents = env_config[\"N_agents\"]\n",
    "    agent_to_mec = np.asarray(env_config[\"agent_to_mec\"], dtype=int)\n",
    "    if len(agent_to_mec) != N_agents:\n",
    "        errors.append(\"Length of agent_to_mec does not match N_agents.\")\n",
    "    # All MEC indices must be within [0, K-1]\n",
    "    K = env_config[\"K\"]\n",
    "    if (agent_to_mec < 0).any() or (agent_to_mec >= K).any():\n",
    "        errors.append(\"agent_to_mec contains indices outside [0, K-1].\")\n",
    "\n",
    "    # 2) Queue initial state shapes\n",
    "    q = env_config[\"queues_initial\"]\n",
    "    if q[\"mec_local_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_local_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_public_cycles\"].shape != (K,):\n",
    "        errors.append(\"mec_public_cycles queue shape mismatch.\")\n",
    "    if q[\"mec_bytes_in_transit\"].shape != (K,):\n",
    "        errors.append(\"mec_bytes_in_transit queue shape mismatch.\")\n",
    "    if q[\"cloud_cycles\"].shape != (1,):\n",
    "        errors.append(\"cloud_cycles shape mismatch (should be (1,)).\")\n",
    "\n",
    "    # 3) Non-negative compute capacities\n",
    "    if (env_config[\"private_cpu\"] < 0).any():\n",
    "        errors.append(\"private_cpu has negative values.\")\n",
    "    if (env_config[\"public_cpu\"] < 0).any():\n",
    "        errors.append(\"public_cpu has negative values.\")\n",
    "    if env_config[\"cloud_cpu\"] < 0:\n",
    "        errors.append(\"cloud_cpu is negative.\")\n",
    "\n",
    "    # 4) Connection matrix dimension (K x K+1)\n",
    "    M = env_config[\"connection_matrix\"]\n",
    "    if M.shape != (K, K + 1):\n",
    "        errors.append(\"connection_matrix shape mismatch (expected K x (K+1)).\")\n",
    "\n",
    "    # 5) Action space correctness\n",
    "    action_space = env_config.get(\"action_space\", {})\n",
    "    if action_space.get(\"type\", None) != \"discrete\":\n",
    "        errors.append(\"Action space must be discrete (LOCAL/MEC/CLOUD).\")\n",
    "    if action_space.get(\"n\", None) != 3:\n",
    "        errors.append(\"Action space 'n' must be 3 (LOCAL/MEC/CLOUD).\")\n",
    "\n",
    "    # 6) Basic time parameters\n",
    "    Delta = float(env_config.get(\"Delta\", -1.0))\n",
    "    T_slots = int(env_config.get(\"T_slots\", -1))\n",
    "    if not np.isfinite(Delta) or Delta <= 0:\n",
    "        errors.append(f\"Invalid Delta in env_config (got {Delta}).\")\n",
    "    if T_slots <= 0:\n",
    "        errors.append(f\"Invalid T_slots in env_config (got {T_slots}).\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "def sanity_check_all(env_configs: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Run sanity_check_env_config over all env_config instances.\n",
    "\n",
    "    env_configs shape (episode-first):\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "    \"\"\"\n",
    "    print(\"=== SANITY CHECK OVER ALL ENV CONFIGS ===\")\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                errs = sanity_check_env_config(env_cfg)\n",
    "                if errs:\n",
    "                    print(f\"[FAIL] {ep_name}/{topo_name}/{scen_name}:\")\n",
    "                    for e in errs:\n",
    "                        print(\"   -\", e)\n",
    "                else:\n",
    "                    print(f\"[OK]   {ep_name}/{topo_name}/{scen_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANITY CHECK OVER ALL ENV CONFIGS ===\n",
      "[OK]   ep_000/clustered/heavy\n",
      "[OK]   ep_000/clustered/light\n",
      "[OK]   ep_000/clustered/moderate\n",
      "[OK]   ep_000/full_mesh/heavy\n",
      "[OK]   ep_000/full_mesh/light\n",
      "[OK]   ep_000/full_mesh/moderate\n",
      "[OK]   ep_000/sparse_ring/heavy\n",
      "[OK]   ep_000/sparse_ring/light\n",
      "[OK]   ep_000/sparse_ring/moderate\n",
      "\n",
      "=== EXAMPLE TASK TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>modality</th>\n",
       "      <th>has_deadline</th>\n",
       "      <th>deadline_s</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>non_atomic</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>action_space_hint</th>\n",
       "      <th>deadline_slots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>image</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0.615113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539704</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>836</td>\n",
       "      <td>10</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>14.734158</td>\n",
       "      <td>7.538056e+08</td>\n",
       "      <td>1.110669e+10</td>\n",
       "      <td>54.079407</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459603</td>\n",
       "      <td>continuous</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>837</td>\n",
       "      <td>15</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>6.334291</td>\n",
       "      <td>1.689114e+09</td>\n",
       "      <td>1.069934e+10</td>\n",
       "      <td>64.994720</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>838</td>\n",
       "      <td>15</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.244217</td>\n",
       "      <td>2.773922e+09</td>\n",
       "      <td>1.177313e+10</td>\n",
       "      <td>133.458000</td>\n",
       "      <td>sensor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.392860</td>\n",
       "      <td>99.392860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>839</td>\n",
       "      <td>15</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12.960908</td>\n",
       "      <td>9.798001e+08</td>\n",
       "      <td>1.269910e+10</td>\n",
       "      <td>63.323215</td>\n",
       "      <td>video</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.762142</td>\n",
       "      <td>continuous</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>840</td>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>6.259753</td>\n",
       "      <td>1.291354e+09</td>\n",
       "      <td>8.083556e+09</td>\n",
       "      <td>108.886536</td>\n",
       "      <td>video</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744489</td>\n",
       "      <td>99.744490</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0      heavy           0        0         0               0             0.0   \n",
       "1      heavy           0        1         1               0             0.0   \n",
       "2      heavy           0        2         4               0             0.0   \n",
       "3      heavy           0        3         7               0             0.0   \n",
       "4      heavy           0        4        10               0             0.0   \n",
       "..       ...         ...      ...       ...             ...             ...   \n",
       "836    heavy           0      836        10              99            99.0   \n",
       "837    heavy           0      837        15              99            99.0   \n",
       "838    heavy           0      838        15              99            99.0   \n",
       "839    heavy           0      839        15              99            99.0   \n",
       "840    heavy           0      840        17              99            99.0   \n",
       "\n",
       "          b_mb  rho_cyc_per_mb      c_cycles      mem_mb modality  \\\n",
       "0     7.202096    9.727147e+08  7.005585e+09   66.611010   sensor   \n",
       "1     5.479984    1.314973e+09  7.206031e+09   77.928800    image   \n",
       "2     8.421977    2.500222e+09  2.105681e+10   72.966446     text   \n",
       "3     6.324986    1.779582e+09  1.125583e+10   56.492900   sensor   \n",
       "4    11.473269    1.087572e+09  1.247800e+10   73.389854   sensor   \n",
       "..         ...             ...           ...         ...      ...   \n",
       "836  14.734158    7.538056e+08  1.110669e+10   54.079407   sensor   \n",
       "837   6.334291    1.689114e+09  1.069934e+10   64.994720   sensor   \n",
       "838   4.244217    2.773922e+09  1.177313e+10  133.458000   sensor   \n",
       "839  12.960908    9.798001e+08  1.269910e+10   63.323215    video   \n",
       "840   6.259753    1.291354e+09  8.083556e+09  108.886536    video   \n",
       "\n",
       "     has_deadline  deadline_s  deadline_time  non_atomic  split_ratio  \\\n",
       "0               1    0.800726       0.800726           0     0.000000   \n",
       "1               1    0.615113       0.615113           0     0.000000   \n",
       "2               1    0.323007       0.323007           1     0.539704   \n",
       "3               1    0.481587       0.481587           0     0.000000   \n",
       "4               1    0.594564       0.594564           0     0.000000   \n",
       "..            ...         ...            ...         ...          ...   \n",
       "836             0         NaN            NaN           1     0.459603   \n",
       "837             0         NaN            NaN           0     0.000000   \n",
       "838             1    0.392860      99.392860           0     0.000000   \n",
       "839             0         NaN            NaN           1     0.762142   \n",
       "840             1    0.744489      99.744490           0     0.000000   \n",
       "\n",
       "    action_space_hint  deadline_slots  \n",
       "0            discrete               1  \n",
       "1            discrete               1  \n",
       "2          continuous               1  \n",
       "3            discrete               1  \n",
       "4            discrete               1  \n",
       "..                ...             ...  \n",
       "836        continuous            <NA>  \n",
       "837          discrete            <NA>  \n",
       "838          discrete               1  \n",
       "839        continuous            <NA>  \n",
       "840          discrete               1  \n",
       "\n",
       "[841 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run all sanity checks\n",
    "sanity_check_all(env_configs)\n",
    "\n",
    "print(\"\\n=== EXAMPLE TASK TABLE ===\")\n",
    "display(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] env_configs summary → ./artifacts/env_configs_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def _summarize_array(arr, max_items=6):\n",
    "    \"\"\"Return a short, readable summary string for numpy arrays.\"\"\"\n",
    "    try:\n",
    "        arr = np.asarray(arr)\n",
    "        base = f\"ndarray shape={arr.shape}, dtype={arr.dtype}\"\n",
    "        if arr.size == 0:\n",
    "            return base + \" | empty\"\n",
    "\n",
    "        # If small 1D vector, show full values\n",
    "        if arr.ndim == 1 and arr.size <= max_items:\n",
    "            return base + f\" | values={arr.tolist()}\"\n",
    "\n",
    "        # If numeric, show basic stats\n",
    "        if np.issubdtype(arr.dtype, np.number):\n",
    "            return (\n",
    "                base +\n",
    "                f\" | min={np.nanmin(arr):.4g}, max={np.nanmax(arr):.4g}, mean={np.nanmean(arr):.4g}\"\n",
    "            )\n",
    "\n",
    "        return base\n",
    "    except Exception as e:\n",
    "        return f\"(array summary failed: {e})\"\n",
    "\n",
    "def _summarize_df(df: pd.DataFrame, max_cols=10):\n",
    "    \"\"\"Return a short summary string for DataFrames.\"\"\"\n",
    "    try:\n",
    "        cols = df.columns.tolist()\n",
    "        cols_show = cols[:max_cols] + ([\"...\"] if len(cols) > max_cols else [])\n",
    "        return f\"DataFrame shape={df.shape}, columns={cols_show}\"\n",
    "    except Exception as e:\n",
    "        return f\"(dataframe summary failed: {e})\"\n",
    "\n",
    "def _summarize_any(name, obj, indent=\"    \"):\n",
    "    \"\"\"\n",
    "    Produce a few readable summary lines depending on the object type.\n",
    "    Used recursively for nested dicts (e.g., queues_initial).\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        lines.append(f\"{indent}{name}: {_summarize_df(obj)}\")\n",
    "\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        lines.append(f\"{indent}{name}: {_summarize_array(obj)}\")\n",
    "\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        preview = obj[:6] if len(obj) > 6 else obj\n",
    "        lines.append(f\"{indent}{name}: list len={len(obj)}, preview={preview}\")\n",
    "\n",
    "    elif isinstance(obj, dict):\n",
    "        lines.append(f\"{indent}{name}: dict keys={list(obj.keys())}\")\n",
    "\n",
    "        # Dive deeper for small dicts or queue dictionaries\n",
    "        if name == \"queues_initial\" or len(obj) <= 6:\n",
    "            for k, v in obj.items():\n",
    "                sub = _summarize_any(k, v, indent=indent + \"  \")\n",
    "                if isinstance(sub, list):\n",
    "                    lines.extend(sub)\n",
    "                else:\n",
    "                    lines.append(sub)\n",
    "\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        lines.append(f\"{indent}{name}: {repr(obj)}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback: try converting to array\n",
    "        try:\n",
    "            arr = np.asarray(obj)\n",
    "            lines.append(f\"{indent}{name}: {_summarize_array(arr)}\")\n",
    "        except Exception:\n",
    "            lines.append(f\"{indent}{name}: ({type(obj).__name__})\")\n",
    "\n",
    "    return lines\n",
    "\n",
    "def save_env_configs_text(env_configs, out_path=\"./artifacts/env_configs_summary.txt\"):\n",
    "    \"\"\"\n",
    "    Save a human-readable summary of all env_configs:\n",
    "        env_configs[episode][topology][scenario] = env_config\n",
    "\n",
    "    The summary includes:\n",
    "    - key scalar parameters (Delta, K, N_agents, topology_type)\n",
    "    - shapes and stats of numeric arrays\n",
    "    - summary of DataFrames (episodes, agents, arrivals, tasks)\n",
    "    - queue initialization\n",
    "    - RL descriptors (action_space, state_spec)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    lines = []\n",
    "    lines.append(\"=== ENV CONFIGS SUMMARY (episode → topology → scenario) ===\\n\")\n",
    "\n",
    "    # deterministic ordering for reproducible summaries\n",
    "    for ep_name in sorted(env_configs.keys()):\n",
    "        lines.append(f\"[EPISODE] {ep_name}\")\n",
    "        by_topo = env_configs[ep_name]\n",
    "\n",
    "        for topo_name in sorted(by_topo.keys()):\n",
    "            lines.append(f\"  [TOPOLOGY] {topo_name}\")\n",
    "            by_scen = by_topo[topo_name]\n",
    "\n",
    "            for scen_name in sorted(by_scen.keys()):\n",
    "                env_cfg = by_scen[scen_name]\n",
    "                lines.append(f\"    [SCENARIO] {scen_name}\")\n",
    "\n",
    "                # -- important scalars --\n",
    "                for key in [\"Delta\", \"K\", \"N_agents\", \"topology_type\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- main tensors/arrays --\n",
    "                for key in [\n",
    "                    \"connection_matrix\", \"private_cpu\", \"public_cpu\",\n",
    "                    \"cloud_cpu\", \"agent_to_mec\"\n",
    "                ]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- dataframes --\n",
    "                for key in [\"episodes\", \"agents\", \"arrivals\", \"tasks\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                # -- RL descriptors and queues --\n",
    "                for key in [\"queues_initial\", \"action_space\", \"state_spec\", \"checks\"]:\n",
    "                    if key in env_cfg:\n",
    "                        lines.extend(_summarize_any(key, env_cfg[key], indent=\"      \"))\n",
    "\n",
    "                lines.append(\"\")  # blank line after scenario\n",
    "\n",
    "            lines.append(\"\")  # blank line after topology\n",
    "\n",
    "        lines.append(\"\")  # blank line after episode\n",
    "\n",
    "    # Write file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(f\"[saved] env_configs summary → {out_path}\")\n",
    "\n",
    "# --------- Usage ---------\n",
    "save_env_configs_text(env_configs, out_path=\"./artifacts/env_configs_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Step 1, we have loaded the data, aligned the units, assigned agents to MECs, and prepared the environment configuration. Finally, we have performed consistency checks to ensure the data is correct. Next, we can move on to task labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 2: Task Labeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Basic Task Labeling (buckets, urgency, atomicity, ...) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers: quantile-based cut points ----------\n",
    "def _quantile_cutpoints(s: pd.Series, q_low=0.33, q_high=0.66) -> Tuple[float, float]:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(s.quantile(q_low)), float(s.quantile(q_high)))\n",
    "\n",
    "def _bucketize(value: float, q1: float, q2: float) -> str:\n",
    "    # Returns 'S', 'M', 'L' based on two cut points (q1<=q2)\n",
    "    if not np.isfinite(value) or not np.isfinite(q1) or not np.isfinite(q2):\n",
    "        return \"U\"  # Unknown\n",
    "    if value <= q1: return \"S\"\n",
    "    if value <= q2: return \"M\"\n",
    "    return \"L\"\n",
    "\n",
    "# ---------- threshold builder (adaptive to each tasks DF) ----------\n",
    "def build_task_label_thresholds(tasks_df: pd.DataFrame,\n",
    "                                q_low=0.33, q_high=0.66,\n",
    "                                urgent_slots_cap: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build adaptive thresholds from the data itself (per-episode/senario),\n",
    "    so 'light/moderate/heavy' are handled robustly.\n",
    "    \"\"\"\n",
    "    q_b_mb   = _quantile_cutpoints(tasks_df[\"b_mb\"], q_low, q_high) if \"b_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_rho    = _quantile_cutpoints(tasks_df[\"rho_cyc_per_mb\"], q_low, q_high) if \"rho_cyc_per_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_mem    = _quantile_cutpoints(tasks_df[\"mem_mb\"], q_low, q_high) if \"mem_mb\" in tasks_df else (np.nan, np.nan)\n",
    "    q_split  = _quantile_cutpoints(tasks_df.loc[tasks_df.get(\"non_atomic\", 0)==1, \"split_ratio\"], q_low, q_high) \\\n",
    "               if \"split_ratio\" in tasks_df else (np.nan, np.nan)\n",
    "\n",
    "    return {\n",
    "        \"b_mb\":   {\"q1\": q_b_mb[0],  \"q2\": q_b_mb[1]},\n",
    "        \"rho\":    {\"q1\": q_rho[0],   \"q2\": q_rho[1]},\n",
    "        \"mem\":    {\"q1\": q_mem[0],   \"q2\": q_mem[1]},\n",
    "        \"split\":  {\"q1\": q_split[0], \"q2\": q_split[1]},\n",
    "        # If deadline_slots ≤ urgent_slots_cap → 'hard' (latency sensitive)\n",
    "        \"urgent_slots_cap\": int(urgent_slots_cap),\n",
    "    }\n",
    "\n",
    "# ---------- main labeling for a single tasks DF ----------\n",
    "def label_tasks_df(tasks_df: pd.DataFrame, Delta: float, thresholds: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add label columns to tasks_df (returns a COPY).\n",
    "    Columns added:\n",
    "      - size_bucket, compute_bucket, mem_bucket\n",
    "      - deadline_slots (if missing), urgency (none/soft/hard)\n",
    "      - atomicity, split_bucket\n",
    "      - latency_sensitive, compute_heavy, io_heavy, memory_heavy (bools)\n",
    "      - routing_hint (LOCAL/MEC/CLOUD)\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    # --- ensure numeric types\n",
    "    for col in [\"b_mb\", \"rho_cyc_per_mb\", \"c_cycles\", \"mem_mb\", \"deadline_s\", \"split_ratio\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # --- deadline_slots (if not precomputed in Units Alignment)\n",
    "    if \"deadline_slots\" not in df.columns:\n",
    "        if \"has_deadline\" in df.columns and \"deadline_s\" in df.columns:\n",
    "            df[\"deadline_slots\"] = np.where(\n",
    "                (df[\"has_deadline\"] == 1) & np.isfinite(df[\"deadline_s\"]),\n",
    "                np.ceil(df[\"deadline_s\"] / float(Delta)).astype(\"float\"),\n",
    "                np.nan\n",
    "            )\n",
    "        else:\n",
    "            df[\"deadline_slots\"] = np.nan\n",
    "\n",
    "    # --- bucketize size/compute/memory\n",
    "    b_q1, b_q2   = thresholds[\"b_mb\"][\"q1\"], thresholds[\"b_mb\"][\"q2\"]\n",
    "    rho_q1, rho_q2 = thresholds[\"rho\"][\"q1\"], thresholds[\"rho\"][\"q2\"]\n",
    "    mem_q1, mem_q2 = thresholds[\"mem\"][\"q1\"], thresholds[\"mem\"][\"q2\"]\n",
    "\n",
    "    df[\"size_bucket\"]    = df[\"b_mb\"].apply(lambda x: _bucketize(x, b_q1, b_q2)) if \"b_mb\" in df else \"U\"\n",
    "    df[\"compute_bucket\"] = df[\"rho_cyc_per_mb\"].apply(lambda x: _bucketize(x, rho_q1, rho_q2)) if \"rho_cyc_per_mb\" in df else \"U\"\n",
    "    df[\"mem_bucket\"]     = df[\"mem_mb\"].apply(lambda x: _bucketize(x, mem_q1, mem_q2)) if \"mem_mb\" in df else \"U\"\n",
    "\n",
    "    # --- atomicity & split buckets\n",
    "    if \"non_atomic\" in df.columns:\n",
    "        df[\"atomicity\"] = np.where(df[\"non_atomic\"] == 1, \"splittable\", \"atomic\")\n",
    "    else:\n",
    "        df[\"atomicity\"] = \"atomic\"\n",
    "\n",
    "    if \"split_ratio\" in df.columns:\n",
    "        sp_q1, sp_q2 = thresholds[\"split\"][\"q1\"], thresholds[\"split\"][\"q2\"]\n",
    "        df[\"split_bucket\"] = np.where(\n",
    "            df[\"atomicity\"] == \"splittable\",\n",
    "            df[\"split_ratio\"].apply(lambda v: _bucketize(v, sp_q1, sp_q2)),\n",
    "            \"NA\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"split_bucket\"] = \"NA\"\n",
    "\n",
    "    # --- urgency levels\n",
    "    urgent_cap = int(thresholds.get(\"urgent_slots_cap\", 2))\n",
    "    def _urg(row):\n",
    "        if int(row.get(\"has_deadline\", 0)) != 1 or not np.isfinite(row.get(\"deadline_slots\", np.nan)):\n",
    "            return \"none\"\n",
    "        slots = int(row[\"deadline_slots\"])\n",
    "        if slots <= urgent_cap:  # very tight deadline\n",
    "            return \"hard\"\n",
    "        return \"soft\"\n",
    "    df[\"urgency\"] = df.apply(_urg, axis=1)\n",
    "\n",
    "    # --- boolean convenience labels\n",
    "    df[\"latency_sensitive\"] = (df[\"urgency\"] == \"hard\")\n",
    "    df[\"compute_heavy\"]     = (df[\"compute_bucket\"] == \"L\")\n",
    "    df[\"io_heavy\"]          = (df[\"size_bucket\"] == \"L\")\n",
    "    df[\"memory_heavy\"]      = (df[\"mem_bucket\"] == \"L\")\n",
    "\n",
    "    # --- a very simple routing hint (only for debugging/EDA; not used by the RL policy)\n",
    "    def _hint(row):\n",
    "        if row[\"compute_heavy\"] or row[\"memory_heavy\"]:\n",
    "            return \"CLOUD\"\n",
    "        if row[\"latency_sensitive\"]:\n",
    "            return \"MEC\"\n",
    "        return \"LOCAL\"\n",
    "    df[\"routing_hint\"] = df.apply(_hint, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- batch apply to env_configs (topology → episode → scenario) ----------\n",
    "def label_all_tasks_in_env_configs(env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "                                   q_low=0.33, q_high=0.66, urgent_slots_cap=2,\n",
    "                                   verbose=True) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    For each env_config:\n",
    "      - build thresholds from its own tasks DF\n",
    "      - label tasks\n",
    "      - put labeled DF back into env_config[\"tasks\"]\n",
    "      - return a concise summary per bundle\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "\n",
    "    for topo_name, by_ep in env_configs.items():\n",
    "        summary[topo_name] = {}\n",
    "        for ep_name, by_scen in by_ep.items():\n",
    "            summary[topo_name][ep_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                Delta = float(env_cfg[\"Delta\"])\n",
    "\n",
    "                # thresholds adaptive to this bundle\n",
    "                th = build_task_label_thresholds(tasks, q_low=q_low, q_high=q_high,\n",
    "                                                 urgent_slots_cap=urgent_slots_cap)\n",
    "                labeled = label_tasks_df(tasks, Delta=Delta, thresholds=th)\n",
    "                env_cfg[\"tasks\"] = labeled  # write back\n",
    "\n",
    "                # tiny summary\n",
    "                cnt = {\n",
    "                    \"n\": len(labeled),\n",
    "                    \"urg_hard\": int((labeled[\"urgency\"] == \"hard\").sum()),\n",
    "                    \"splittable\": int((labeled[\"atomicity\"] == \"splittable\").sum()),\n",
    "                    \"size_L\": int((labeled[\"size_bucket\"] == \"L\").sum()),\n",
    "                    \"compute_L\": int((labeled[\"compute_bucket\"] == \"L\").sum()),\n",
    "                    \"mem_L\": int((labeled[\"mem_bucket\"] == \"L\").sum()),\n",
    "                }\n",
    "                summary[topo_name][ep_name][scen_name] = cnt\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[label] {topo_name}/{ep_name}/{scen_name} -> \"\n",
    "                          f\"n={cnt['n']}, hard={cnt['urg_hard']}, split={cnt['splittable']}, \"\n",
    "                          f\"sizeL={cnt['size_L']}, compL={cnt['compute_L']}, memL={cnt['mem_L']}\")\n",
    "\n",
    "    return env_configs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[label] ep_000/clustered/heavy -> n=841, hard=290, split=375, sizeL=286, compL=286, memL=286\n",
      "[label] ep_000/clustered/light -> n=70, hard=14, split=23, sizeL=24, compL=24, memL=24\n",
      "[label] ep_000/clustered/moderate -> n=261, hard=54, split=84, sizeL=89, compL=89, memL=89\n",
      "[label] ep_000/full_mesh/heavy -> n=841, hard=290, split=375, sizeL=286, compL=286, memL=286\n",
      "[label] ep_000/full_mesh/light -> n=70, hard=14, split=23, sizeL=24, compL=24, memL=24\n",
      "[label] ep_000/full_mesh/moderate -> n=261, hard=54, split=84, sizeL=89, compL=89, memL=89\n",
      "[label] ep_000/sparse_ring/heavy -> n=841, hard=290, split=375, sizeL=286, compL=286, memL=286\n",
      "[label] ep_000/sparse_ring/light -> n=70, hard=14, split=23, sizeL=24, compL=24, memL=24\n",
      "[label] ep_000/sparse_ring/moderate -> n=261, hard=54, split=84, sizeL=89, compL=89, memL=89\n",
      "\n",
      " ===EXAMPLE===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>...</th>\n",
       "      <th>compute_bucket</th>\n",
       "      <th>mem_bucket</th>\n",
       "      <th>atomicity</th>\n",
       "      <th>split_bucket</th>\n",
       "      <th>urgency</th>\n",
       "      <th>latency_sensitive</th>\n",
       "      <th>compute_heavy</th>\n",
       "      <th>io_heavy</th>\n",
       "      <th>memory_heavy</th>\n",
       "      <th>routing_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>L</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>CLOUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>...</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>splittable</td>\n",
       "      <td>M</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>CLOUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>...</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>CLOUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>atomic</td>\n",
       "      <td>NA</td>\n",
       "      <td>hard</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>MEC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ... compute_bucket  \\\n",
       "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...              S   \n",
       "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...              M   \n",
       "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...              L   \n",
       "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...              L   \n",
       "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...              S   \n",
       "\n",
       "   mem_bucket   atomicity  split_bucket  urgency  latency_sensitive  \\\n",
       "0           M      atomic            NA     hard               True   \n",
       "1           L      atomic            NA     hard               True   \n",
       "2           M  splittable             M     hard               True   \n",
       "3           M      atomic            NA     hard               True   \n",
       "4           M      atomic            NA     hard               True   \n",
       "\n",
       "  compute_heavy  io_heavy memory_heavy routing_hint  \n",
       "0         False      True        False          MEC  \n",
       "1         False     False         True        CLOUD  \n",
       "2          True      True        False        CLOUD  \n",
       "3          True     False        False        CLOUD  \n",
       "4         False      True        False          MEC  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841 entries, 0 to 840\n",
      "Data columns (total 29 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   scenario           841 non-null    object \n",
      " 1   episode_id         841 non-null    int64  \n",
      " 2   task_id            841 non-null    int64  \n",
      " 3   agent_id           841 non-null    int64  \n",
      " 4   t_arrival_slot     841 non-null    int32  \n",
      " 5   t_arrival_time     841 non-null    float64\n",
      " 6   b_mb               841 non-null    float64\n",
      " 7   rho_cyc_per_mb     841 non-null    float64\n",
      " 8   c_cycles           841 non-null    float64\n",
      " 9   mem_mb             841 non-null    float64\n",
      " 10  modality           841 non-null    object \n",
      " 11  has_deadline       841 non-null    int64  \n",
      " 12  deadline_s         290 non-null    float64\n",
      " 13  deadline_time      290 non-null    float64\n",
      " 14  non_atomic         841 non-null    int64  \n",
      " 15  split_ratio        841 non-null    float64\n",
      " 16  action_space_hint  841 non-null    object \n",
      " 17  deadline_slots     290 non-null    Int64  \n",
      " 18  size_bucket        841 non-null    object \n",
      " 19  compute_bucket     841 non-null    object \n",
      " 20  mem_bucket         841 non-null    object \n",
      " 21  atomicity          841 non-null    object \n",
      " 22  split_bucket       841 non-null    object \n",
      " 23  urgency            841 non-null    object \n",
      " 24  latency_sensitive  841 non-null    bool   \n",
      " 25  compute_heavy      841 non-null    bool   \n",
      " 26  io_heavy           841 non-null    bool   \n",
      " 27  memory_heavy       841 non-null    bool   \n",
      " 28  routing_hint       841 non-null    object \n",
      "dtypes: Int64(1), bool(4), float64(8), int32(1), int64(5), object(10)\n",
      "memory usage: 165.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# env_configs: Produced in Step 6 (structure: episode → topology → scenario)\n",
    "env_configs, label_summary = label_all_tasks_in_env_configs(\n",
    "    env_configs,\n",
    "    q_low=0.33, q_high=0.66, urgent_slots_cap=2,  # tunable thresholds\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example access:\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "labeled_tasks = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"]\n",
    "display(labeled_tasks.head())\n",
    "print(labeled_tasks.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Task Type Classification </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-req: tasks already labeled by your previous step: \n",
    "#   size_bucket, compute_bucket, mem_bucket, urgency, atomicity, split_bucket, routing_hint, etc.\n",
    "\n",
    "def _derive_task_type_row(row: pd.Series) -> tuple[str, str, str, list, str]:\n",
    "    \"\"\"\n",
    "    Returns (task_type, task_subtype, type_reason, multi_flags, final_flag)\n",
    "    \"\"\"\n",
    "    # Collect boolean flags consistent with your earlier labeling:\n",
    "    urgency        = str(row.get(\"urgency\", \"none\"))         # \"hard\" | \"soft\" | \"none\"\n",
    "    latency_flag   = (urgency == \"hard\") or (urgency == \"soft\")\n",
    "    hard_deadline  = (urgency == \"hard\")\n",
    "\n",
    "    compute_heavy  = bool(row.get(\"compute_heavy\", False))   # compute_bucket == \"L\"\n",
    "    memory_heavy   = bool(row.get(\"memory_heavy\", False))    # mem_bucket == \"L\"\n",
    "    io_heavy       = bool(row.get(\"io_heavy\", False))        # size_bucket == \"L\"\n",
    "    non_atomic     = bool(row.get(\"atomicity\", \"atomic\") == \"splittable\")\n",
    "\n",
    "    # Keep all active signals for audit:\n",
    "    multi_flags = []\n",
    "    if hard_deadline:  multi_flags.append(\"deadline_hard\")\n",
    "    elif latency_flag: multi_flags.append(\"deadline_soft\")\n",
    "    if compute_heavy:  multi_flags.append(\"compute_heavy\")\n",
    "    if memory_heavy:   multi_flags.append(\"memory_heavy\")\n",
    "    if io_heavy:       multi_flags.append(\"io_heavy\")\n",
    "    if non_atomic:     multi_flags.append(\"splittable\")\n",
    "\n",
    "    # --- Priority resolution (Chapter 4) ---\n",
    "    # 1) Hard deadline dominates everything\n",
    "    if hard_deadline:\n",
    "        final_flag = \"deadline_hard\"\n",
    "        return (\"deadline_hard\", \"deadline_hard\", \"hard deadline (tight slots)\", multi_flags, final_flag)\n",
    "\n",
    "    # 2) Latency-sensitive (soft deadlines / delay-sensitive)\n",
    "    if latency_flag:\n",
    "        final_flag = \"latency_sensitive\"\n",
    "        return (\"latency_sensitive\", \"deadline_soft\", \"delay-sensitive (soft deadline)\", multi_flags, final_flag)\n",
    "\n",
    "    # 3) Compute-intensive (c or rho or mem heavy)\n",
    "    if compute_heavy or memory_heavy:\n",
    "        final_flag = \"compute_intensive\"\n",
    "        return (\"compute_intensive\", \"compute_or_memory_heavy\", \"high compute/memory demand\", multi_flags, final_flag)\n",
    "\n",
    "    # 4) Data-intensive (mainly large input size / high IO pressure)\n",
    "    if io_heavy:\n",
    "        final_flag = \"data_intensive\"\n",
    "        return (\"data_intensive\", \"large_input_bandwidth\", \"large data volume / IO heavy\", multi_flags, final_flag)\n",
    "\n",
    "    # 5) Otherwise general\n",
    "    final_flag = \"general\"\n",
    "    return (\"general\", \"general\", \"no dominant constraint\", multi_flags, final_flag)\n",
    "\n",
    "def apply_ch4_task_typing(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds Chapter-4 level task classes with priority rules into tasks_df (returns a COPY).\n",
    "    Columns added:\n",
    "      - task_type            (5-way class)\n",
    "      - task_subtype         (finer descriptor)\n",
    "      - type_reason          (short textual rationale)\n",
    "      - multi_flags          (list of all active boolean traits)\n",
    "      - final_flag           (single flag representing the task's priority class)\n",
    "    \"\"\"\n",
    "    df = tasks_df.copy()\n",
    "\n",
    "    # Ensure the expected helper columns exist (created in your previous labeling step).\n",
    "    required_cols = [\"urgency\", \"compute_heavy\", \"memory_heavy\", \"io_heavy\", \"atomicity\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"apply_ch4_task_typing: missing label columns: {missing}\")\n",
    "\n",
    "    out_type, out_sub, out_reason, out_flags, out_final_flag = [], [], [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        t, s, msg, flags, final_flag = _derive_task_type_row(r)\n",
    "        out_type.append(t)\n",
    "        out_sub.append(s)\n",
    "        out_reason.append(msg)\n",
    "        out_flags.append(flags)\n",
    "        out_final_flag.append(final_flag)\n",
    "\n",
    "    df[\"task_type\"]   = out_type\n",
    "    df[\"task_subtype\"]= out_sub\n",
    "    df[\"type_reason\"] = out_reason\n",
    "    df[\"multi_flags\"] = out_flags\n",
    "    df[\"final_flag\"]  = out_final_flag  # Add the final flag to represent the primary category\n",
    "\n",
    "    # For convenience: one-hot view (optional)\n",
    "    df[\"is_general\"]            = (df[\"task_type\"] == \"general\")\n",
    "    df[\"is_deadline_hard\"]      = (df[\"task_type\"] == \"deadline_hard\")\n",
    "    df[\"is_latency_sensitive\"]  = (df[\"task_type\"] == \"latency_sensitive\")\n",
    "    df[\"is_compute_intensive\"]  = (df[\"task_type\"] == \"compute_intensive\")\n",
    "    df[\"is_data_intensive\"]     = (df[\"task_type\"] == \"data_intensive\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_task_typing_in_env_configs(env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "                                     verbose: bool = True) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    env_configs structure (as we fixed earlier):\n",
    "      env_configs[ep_name][topology_name][scenario_name][\"tasks\"] -> DataFrame\n",
    "\n",
    "    This function:\n",
    "      - applies Chapter-4 task typing to every tasks DF\n",
    "      - writes back the enriched DataFrame\n",
    "      - prints a short summary if verbose=True\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                tasks = env_cfg[\"tasks\"]\n",
    "                enriched = apply_ch4_task_typing(tasks)\n",
    "                env_cfg[\"tasks\"] = enriched\n",
    "\n",
    "                if verbose:\n",
    "                    n = len(enriched)\n",
    "                    counts = enriched[\"task_type\"].value_counts().to_dict()\n",
    "                    print(f\"[typing] {ep_name}/{topo_name}/{scen_name}  n={n}  → {counts}\")\n",
    "    return env_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[typing] ep_000/clustered/heavy  n=841  → {'compute_intensive': 305, 'deadline_hard': 290, 'general': 153, 'data_intensive': 93}\n",
      "[typing] ep_000/clustered/light  n=70  → {'compute_intensive': 32, 'general': 15, 'deadline_hard': 14, 'data_intensive': 9}\n",
      "[typing] ep_000/clustered/moderate  n=261  → {'compute_intensive': 107, 'general': 69, 'deadline_hard': 54, 'data_intensive': 31}\n",
      "[typing] ep_000/full_mesh/heavy  n=841  → {'compute_intensive': 305, 'deadline_hard': 290, 'general': 153, 'data_intensive': 93}\n",
      "[typing] ep_000/full_mesh/light  n=70  → {'compute_intensive': 32, 'general': 15, 'deadline_hard': 14, 'data_intensive': 9}\n",
      "[typing] ep_000/full_mesh/moderate  n=261  → {'compute_intensive': 107, 'general': 69, 'deadline_hard': 54, 'data_intensive': 31}\n",
      "[typing] ep_000/sparse_ring/heavy  n=841  → {'compute_intensive': 305, 'deadline_hard': 290, 'general': 153, 'data_intensive': 93}\n",
      "[typing] ep_000/sparse_ring/light  n=70  → {'compute_intensive': 32, 'general': 15, 'deadline_hard': 14, 'data_intensive': 9}\n",
      "[typing] ep_000/sparse_ring/moderate  n=261  → {'compute_intensive': 107, 'general': 69, 'deadline_hard': 54, 'data_intensive': 31}\n",
      "\n",
      " ===EXAMPLE===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "      <th>final_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, splittable]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[memory_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>data_intensive</td>\n",
       "      <td>large_input_bandwidth</td>\n",
       "      <td>large data volume / IO heavy</td>\n",
       "      <td>[io_heavy, splittable]</td>\n",
       "      <td>data_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, io_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, io_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, splittable]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, io_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, io_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, memory_heavy]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>no dominant constraint</td>\n",
       "      <td>[]</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>compute_intensive</td>\n",
       "      <td>compute_or_memory_heavy</td>\n",
       "      <td>high compute/memory demand</td>\n",
       "      <td>[compute_heavy, splittable]</td>\n",
       "      <td>compute_intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_id          task_type             task_subtype  \\\n",
       "0         0      deadline_hard            deadline_hard   \n",
       "1         1      deadline_hard            deadline_hard   \n",
       "2         2      deadline_hard            deadline_hard   \n",
       "3         3      deadline_hard            deadline_hard   \n",
       "4         4      deadline_hard            deadline_hard   \n",
       "5         5      deadline_hard            deadline_hard   \n",
       "6         6            general                  general   \n",
       "7         7  compute_intensive  compute_or_memory_heavy   \n",
       "8         8      deadline_hard            deadline_hard   \n",
       "9         9      deadline_hard            deadline_hard   \n",
       "10       10  compute_intensive  compute_or_memory_heavy   \n",
       "11       11     data_intensive    large_input_bandwidth   \n",
       "12       12            general                  general   \n",
       "13       13  compute_intensive  compute_or_memory_heavy   \n",
       "14       14  compute_intensive  compute_or_memory_heavy   \n",
       "15       15      deadline_hard            deadline_hard   \n",
       "16       16  compute_intensive  compute_or_memory_heavy   \n",
       "17       17      deadline_hard            deadline_hard   \n",
       "18       18  compute_intensive  compute_or_memory_heavy   \n",
       "19       19            general                  general   \n",
       "20       20  compute_intensive  compute_or_memory_heavy   \n",
       "21       21      deadline_hard            deadline_hard   \n",
       "22       22            general                  general   \n",
       "23       23  compute_intensive  compute_or_memory_heavy   \n",
       "24       24      deadline_hard            deadline_hard   \n",
       "\n",
       "                     type_reason  \\\n",
       "0    hard deadline (tight slots)   \n",
       "1    hard deadline (tight slots)   \n",
       "2    hard deadline (tight slots)   \n",
       "3    hard deadline (tight slots)   \n",
       "4    hard deadline (tight slots)   \n",
       "5    hard deadline (tight slots)   \n",
       "6         no dominant constraint   \n",
       "7     high compute/memory demand   \n",
       "8    hard deadline (tight slots)   \n",
       "9    hard deadline (tight slots)   \n",
       "10    high compute/memory demand   \n",
       "11  large data volume / IO heavy   \n",
       "12        no dominant constraint   \n",
       "13    high compute/memory demand   \n",
       "14    high compute/memory demand   \n",
       "15   hard deadline (tight slots)   \n",
       "16    high compute/memory demand   \n",
       "17   hard deadline (tight slots)   \n",
       "18    high compute/memory demand   \n",
       "19        no dominant constraint   \n",
       "20    high compute/memory demand   \n",
       "21   hard deadline (tight slots)   \n",
       "22        no dominant constraint   \n",
       "23    high compute/memory demand   \n",
       "24   hard deadline (tight slots)   \n",
       "\n",
       "                                          multi_flags         final_flag  \n",
       "0                           [deadline_hard, io_heavy]      deadline_hard  \n",
       "1                       [deadline_hard, memory_heavy]      deadline_hard  \n",
       "2   [deadline_hard, compute_heavy, io_heavy, split...      deadline_hard  \n",
       "3                      [deadline_hard, compute_heavy]      deadline_hard  \n",
       "4                           [deadline_hard, io_heavy]      deadline_hard  \n",
       "5                         [deadline_hard, splittable]      deadline_hard  \n",
       "6                                                  []            general  \n",
       "7                          [memory_heavy, splittable]  compute_intensive  \n",
       "8                      [deadline_hard, compute_heavy]      deadline_hard  \n",
       "9                      [deadline_hard, compute_heavy]      deadline_hard  \n",
       "10                                    [compute_heavy]  compute_intensive  \n",
       "11                             [io_heavy, splittable]     data_intensive  \n",
       "12                                                 []            general  \n",
       "13                          [compute_heavy, io_heavy]  compute_intensive  \n",
       "14                          [compute_heavy, io_heavy]  compute_intensive  \n",
       "15          [deadline_hard, memory_heavy, splittable]      deadline_hard  \n",
       "16              [compute_heavy, io_heavy, splittable]  compute_intensive  \n",
       "17           [deadline_hard, compute_heavy, io_heavy]      deadline_hard  \n",
       "18                          [compute_heavy, io_heavy]  compute_intensive  \n",
       "19                                                 []            general  \n",
       "20                      [compute_heavy, memory_heavy]  compute_intensive  \n",
       "21                                    [deadline_hard]      deadline_hard  \n",
       "22                                                 []            general  \n",
       "23                        [compute_heavy, splittable]  compute_intensive  \n",
       "24            [deadline_hard, memory_heavy, io_heavy]      deadline_hard  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Run typing on your current env_configs (episode → topology → scenario) ----\n",
    "env_configs = apply_task_typing_in_env_configs(env_configs, verbose=True)\n",
    "\n",
    "# Example access:\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "display(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"][[\"task_id\",\"task_type\",\"task_subtype\",\"type_reason\",\"multi_flags\", \"final_flag\"]].head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "none → Tasks that do not have a specific deadline or time sensitivity </br>\n",
    "hard → Tasks that have a very limited deadline and delay is very important to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urgency\n",
      "none    551\n",
      "hard    290\n",
      "Name: count, dtype: int64\n",
      "\n",
      " task_type\n",
      "compute_intensive    305\n",
      "deadline_hard        290\n",
      "general              153\n",
      "data_intensive        93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "                        b_mb  rho_cyc_per_mb     mem_mb\n",
      "task_type                                             \n",
      "compute_intensive  4.969992    1.907796e+09  80.178630\n",
      "data_intensive     9.272635    1.200304e+09  53.564426\n",
      "deadline_hard      4.943993    1.406729e+09  64.256427\n",
      "general            4.141747    1.232642e+09  48.211580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>t_arrival_slot</th>\n",
       "      <th>t_arrival_time</th>\n",
       "      <th>b_mb</th>\n",
       "      <th>rho_cyc_per_mb</th>\n",
       "      <th>c_cycles</th>\n",
       "      <th>mem_mb</th>\n",
       "      <th>...</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_subtype</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>multi_flags</th>\n",
       "      <th>final_flag</th>\n",
       "      <th>is_general</th>\n",
       "      <th>is_deadline_hard</th>\n",
       "      <th>is_latency_sensitive</th>\n",
       "      <th>is_compute_intensive</th>\n",
       "      <th>is_data_intensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.202096</td>\n",
       "      <td>9.727147e+08</td>\n",
       "      <td>7.005585e+09</td>\n",
       "      <td>66.611010</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.479984</td>\n",
       "      <td>1.314973e+09</td>\n",
       "      <td>7.206031e+09</td>\n",
       "      <td>77.928800</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, memory_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.421977</td>\n",
       "      <td>2.500222e+09</td>\n",
       "      <td>2.105681e+10</td>\n",
       "      <td>72.966446</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy, io_heavy, split...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.324986</td>\n",
       "      <td>1.779582e+09</td>\n",
       "      <td>1.125583e+10</td>\n",
       "      <td>56.492900</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, compute_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heavy</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.473269</td>\n",
       "      <td>1.087572e+09</td>\n",
       "      <td>1.247800e+10</td>\n",
       "      <td>73.389854</td>\n",
       "      <td>...</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>hard deadline (tight slots)</td>\n",
       "      <td>[deadline_hard, io_heavy]</td>\n",
       "      <td>deadline_hard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
       "0    heavy           0        0         0               0             0.0   \n",
       "1    heavy           0        1         1               0             0.0   \n",
       "2    heavy           0        2         4               0             0.0   \n",
       "3    heavy           0        3         7               0             0.0   \n",
       "4    heavy           0        4        10               0             0.0   \n",
       "\n",
       "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...      task_type  \\\n",
       "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
       "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
       "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
       "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
       "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
       "\n",
       "    task_subtype                  type_reason  \\\n",
       "0  deadline_hard  hard deadline (tight slots)   \n",
       "1  deadline_hard  hard deadline (tight slots)   \n",
       "2  deadline_hard  hard deadline (tight slots)   \n",
       "3  deadline_hard  hard deadline (tight slots)   \n",
       "4  deadline_hard  hard deadline (tight slots)   \n",
       "\n",
       "                                         multi_flags     final_flag  \\\n",
       "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
       "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
       "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
       "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
       "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
       "\n",
       "   is_general is_deadline_hard  is_latency_sensitive is_compute_intensive  \\\n",
       "0       False             True                 False                False   \n",
       "1       False             True                 False                False   \n",
       "2       False             True                 False                False   \n",
       "3       False             True                 False                False   \n",
       "4       False             True                 False                False   \n",
       "\n",
       "  is_data_intensive  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841 entries, 0 to 840\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   scenario              841 non-null    object \n",
      " 1   episode_id            841 non-null    int64  \n",
      " 2   task_id               841 non-null    int64  \n",
      " 3   agent_id              841 non-null    int64  \n",
      " 4   t_arrival_slot        841 non-null    int32  \n",
      " 5   t_arrival_time        841 non-null    float64\n",
      " 6   b_mb                  841 non-null    float64\n",
      " 7   rho_cyc_per_mb        841 non-null    float64\n",
      " 8   c_cycles              841 non-null    float64\n",
      " 9   mem_mb                841 non-null    float64\n",
      " 10  modality              841 non-null    object \n",
      " 11  has_deadline          841 non-null    int64  \n",
      " 12  deadline_s            290 non-null    float64\n",
      " 13  deadline_time         290 non-null    float64\n",
      " 14  non_atomic            841 non-null    int64  \n",
      " 15  split_ratio           841 non-null    float64\n",
      " 16  action_space_hint     841 non-null    object \n",
      " 17  deadline_slots        290 non-null    Int64  \n",
      " 18  size_bucket           841 non-null    object \n",
      " 19  compute_bucket        841 non-null    object \n",
      " 20  mem_bucket            841 non-null    object \n",
      " 21  atomicity             841 non-null    object \n",
      " 22  split_bucket          841 non-null    object \n",
      " 23  urgency               841 non-null    object \n",
      " 24  latency_sensitive     841 non-null    bool   \n",
      " 25  compute_heavy         841 non-null    bool   \n",
      " 26  io_heavy              841 non-null    bool   \n",
      " 27  memory_heavy          841 non-null    bool   \n",
      " 28  routing_hint          841 non-null    object \n",
      " 29  task_type             841 non-null    object \n",
      " 30  task_subtype          841 non-null    object \n",
      " 31  type_reason           841 non-null    object \n",
      " 32  multi_flags           841 non-null    object \n",
      " 33  final_flag            841 non-null    object \n",
      " 34  is_general            841 non-null    bool   \n",
      " 35  is_deadline_hard      841 non-null    bool   \n",
      " 36  is_latency_sensitive  841 non-null    bool   \n",
      " 37  is_compute_intensive  841 non-null    bool   \n",
      " 38  is_data_intensive     841 non-null    bool   \n",
      "dtypes: Int64(1), bool(9), float64(8), int32(1), int64(5), object(15)\n",
      "memory usage: 202.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "labeled_tasks_completed = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"tasks\"]\n",
    "print(labeled_tasks_completed[\"urgency\"].value_counts())\n",
    "print(\"\\n\", labeled_tasks_completed[\"task_type\"].value_counts())\n",
    "print(\"\\n\", labeled_tasks_completed.groupby(\"task_type\")[[\"b_mb\",\"rho_cyc_per_mb\",\"mem_mb\"]].median())\n",
    "\n",
    "display(labeled_tasks_completed.head())\n",
    "print(labeled_tasks_completed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _ensure_dir(path: str):\n",
    "#     \"\"\"Create folder if it doesn't exist.\"\"\"\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def save_all_env_configs(env_configs, out_root: str = \"./artifacts/env_configs\"):\n",
    "#     \"\"\"\n",
    "#     Walk through env_configs (episode → topology → scenario) and save all data (tasks, agents, etc.) as CSV.\n",
    "#     Example output: ./artifacts/env_configs/ep_000/clustered/heavy/env_config.csv\n",
    "#     \"\"\"\n",
    "#     n_saved = 0\n",
    "#     for ep_name, by_topo in env_configs.items():\n",
    "#         for topo_name, by_scen in by_topo.items():\n",
    "#             for scen_name, env_cfg in by_scen.items():\n",
    "#                 out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "#                 _ensure_dir(out_dir)\n",
    "\n",
    "#                 # Save tasks, agents, arrivals, and other components\n",
    "#                 for df_name, df in env_cfg.items():\n",
    "#                     if isinstance(df, pd.DataFrame):\n",
    "#                         file_path_csv = os.path.join(out_dir, f\"{df_name}_env_config.csv\")\n",
    "#                         # file_path_pq = os.path.join(out_dir, f\"{df_name}_env_config.parquet\")\n",
    "\n",
    "#                         df.to_csv(file_path_csv, index=False)\n",
    "#                         # try:\n",
    "#                         #     df.to_parquet(file_path_pq, index=False)  # optional\n",
    "#                         # except Exception:\n",
    "#                         #     pass\n",
    "\n",
    "#                         print(f\"[saved] {file_path_csv}  (rows={len(df)})\")\n",
    "#                         n_saved += 1\n",
    "\n",
    "#     print(f\"Done. Saved {n_saved} dataframes for env_configs (episode, topology, scenario).\")\n",
    "\n",
    "# # Save ALL env_configs\n",
    "# save_all_env_configs(env_configs, out_root=\"./artifacts/env_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 3: Agent Profiling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we construct a behavioral profile for each agent, capturing its local compute resources, task arrival rate, and the distribution of task types it generates. These profiles are later used for clustering agents and assigning suitable reinforcement learning strategies to each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper 1: per-agent per-slot arrival counts ----\n",
    "def _per_agent_slot_counts(arrivals_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count how many tasks each agent generates in each time slot.\n",
    "    This is used to estimate lambda (arrival rate) statistics.\n",
    "    \"\"\"\n",
    "    if not {\"agent_id\", \"t_slot\"}.issubset(arrivals_df.columns):\n",
    "        raise ValueError(\"arrivals must contain 'agent_id' and 't_slot'.\")\n",
    "    grp = arrivals_df.groupby([\"agent_id\", \"t_slot\"], as_index=False).size()\n",
    "    grp.rename(columns={\"size\": \"count\"}, inplace=True)\n",
    "    return grp\n",
    "\n",
    "# ---- Helper 2: estimate λ-mean and λ-variance per agent (tight dtypes) ----\n",
    "def _lambda_stats_from_counts(counts_df: pd.DataFrame, Delta: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert per-slot counts to rate statistics:\n",
    "        lambda_mean = mean(count_per_slot) / Delta\n",
    "        lambda_var  = var(count_per_slot)  / Delta^2\n",
    "    \"\"\"\n",
    "    if counts_df.empty:\n",
    "        return pd.DataFrame(columns=[\"agent_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"])\n",
    "\n",
    "    agg = counts_df.groupby(\"agent_id\")[\"count\"].agg(\n",
    "        lambda_mean_slot=\"mean\",\n",
    "        lambda_var_slot=\"var\",\n",
    "        slots_observed=\"count\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # If only one observation exists, variance becomes NaN → treat as zero.\n",
    "    agg[\"lambda_var_slot\"] = agg[\"lambda_var_slot\"].fillna(0.0).astype(float)\n",
    "\n",
    "    # Convert to per-second rates\n",
    "    agg[\"lambda_mean\"] = (agg[\"lambda_mean_slot\"] / float(Delta)).astype(float)\n",
    "    agg[\"lambda_var\"]  = (agg[\"lambda_var_slot\"]  / float(Delta**2)).astype(float)\n",
    "\n",
    "    return agg[[\"agent_id\", \"lambda_mean\", \"lambda_var\", \"slots_observed\"]]\n",
    "\n",
    "# ---- Helper 3: task-type distribution per agent (robust + extra stats) ----\n",
    "_TASK_TYPES = [\"general\", \"latency_sensitive\", \"deadline_hard\", \"data_intensive\", \"compute_intensive\"]\n",
    "\n",
    "def _task_distribution_per_agent(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute distribution of task types per agent (probabilities sum to 1 for agents with tasks).\n",
    "    Also adds light median features useful for clustering: b_mb_med, rho_med, mem_med, hard_share.\n",
    "    \"\"\"\n",
    "    if not {\"agent_id\", \"task_type\"}.issubset(tasks_df.columns):\n",
    "        raise ValueError(\"tasks must contain 'agent_id' and 'task_type'.\")\n",
    "\n",
    "    # Raw counts per (agent_id, task_type)\n",
    "    cnt = tasks_df.groupby([\"agent_id\", \"task_type\"], as_index=False).size()\n",
    "    piv = cnt.pivot(index=\"agent_id\", columns=\"task_type\", values=\"size\").fillna(0.0)\n",
    "\n",
    "    # Ensure all expected classes exist\n",
    "    for t in _TASK_TYPES:\n",
    "        if t not in piv.columns:\n",
    "            piv[t] = 0.0\n",
    "\n",
    "    # True count across all seen labels\n",
    "    piv[\"n_tasks_agent\"] = piv[_TASK_TYPES].sum(axis=1).astype(float)\n",
    "\n",
    "    # Probabilities\n",
    "    for t in _TASK_TYPES:\n",
    "        piv[f\"P_{t}\"] = np.where(piv[\"n_tasks_agent\"] > 0, piv[t] / piv[\"n_tasks_agent\"], 0.0).astype(float)\n",
    "\n",
    "    # Optional extra features for clustering (guard on availability)\n",
    "    feats = {}\n",
    "    have_feats = {\"b_mb\", \"rho_cyc_per_mb\", \"mem_mb\", \"urgency\"}\n",
    "    if have_feats.issubset(tasks_df.columns):\n",
    "        agg = tasks_df.groupby(\"agent_id\").agg(\n",
    "            b_mb_med=(\"b_mb\", \"median\"),\n",
    "            rho_med=(\"rho_cyc_per_mb\", \"median\"),\n",
    "            mem_med=(\"mem_mb\", \"median\"),\n",
    "            hard_share=(\"urgency\", lambda s: float((s == \"hard\").mean()))\n",
    "        ).reset_index()\n",
    "        feats = agg.set_index(\"agent_id\")\n",
    "\n",
    "    # Join extra features (if any)\n",
    "    piv = piv.join(feats, how=\"left\")\n",
    "    for c in [\"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"]:\n",
    "        if c in piv.columns:\n",
    "            piv[c] = piv[c].fillna(0.0).astype(float)\n",
    "        else:\n",
    "            piv[c] = 0.0\n",
    "\n",
    "    # Probability mass sum (diagnostic)\n",
    "    prob_cols = [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    piv[\"TaskDist_sum\"] = piv[prob_cols].sum(axis=1).astype(float)\n",
    "\n",
    "    keep = [\"n_tasks_agent\", \"TaskDist_sum\", \"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"] + prob_cols\n",
    "    return piv[keep].reset_index()\n",
    "\n",
    "# ---- Helper 4: fraction of non-atomic (splittable) tasks ----\n",
    "def _non_atomic_share_per_agent(tasks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the share of splittable (non-atomic) tasks per agent.\n",
    "    \"\"\"\n",
    "    if not {\"agent_id\", \"non_atomic\"}.issubset(tasks_df.columns):\n",
    "        # If missing, assume zero for all agents that exist in tasks\n",
    "        agents = tasks_df.get(\"agent_id\")\n",
    "        if agents is None or len(agents) == 0:\n",
    "            return pd.DataFrame(columns=[\"agent_id\", \"non_atomic_share\"])\n",
    "        return pd.DataFrame({\"agent_id\": agents.unique(), \"non_atomic_share\": 0.0})\n",
    "\n",
    "    grp = tasks_df.groupby(\"agent_id\")[\"non_atomic\"].agg(\n",
    "        non_atomic_share=lambda s: float((s == 1).mean())\n",
    "    ).reset_index()\n",
    "    return grp\n",
    "\n",
    "# ---- Build agent profiles for ONE env_config ----\n",
    "def build_agent_profiles_for_env(env_cfg: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct per-agent profiles combining:\n",
    "      - Local resource capacity (f_local, m_local, f_local_slot)\n",
    "      - Arrival rate statistics (lambda_mean, lambda_var)\n",
    "      - Task type distribution (P_general, P_latency_sensitive, P_deadline_hard, P_data_intensive, P_compute_intensive)\n",
    "      - Splittability share (non_atomic_share)\n",
    "      - MEC mapping if available (mec_id)\n",
    "    \"\"\"\n",
    "    agents   = env_cfg[\"agents\"].copy()\n",
    "    arrivals = env_cfg[\"arrivals\"]\n",
    "    tasks    = env_cfg[\"tasks\"]\n",
    "    Delta    = float(env_cfg[\"Delta\"])\n",
    "\n",
    "    # Ensure cycles/slot exists\n",
    "    if \"f_local_slot\" not in agents.columns and \"f_local\" in agents.columns:\n",
    "        agents[\"f_local_slot\"] = agents[\"f_local\"].astype(float) * Delta\n",
    "\n",
    "    # 1) Arrival statistics\n",
    "    counts_df = _per_agent_slot_counts(arrivals)\n",
    "    lam_df    = _lambda_stats_from_counts(counts_df, Delta=Delta)\n",
    "\n",
    "    # 2) Task-type distribution (+ medians & hard_share)\n",
    "    dist_df   = _task_distribution_per_agent(tasks)\n",
    "\n",
    "    # 3) Splittable-task share\n",
    "    na_df     = _non_atomic_share_per_agent(tasks)\n",
    "\n",
    "    # 4) Agent→MEC mapping (optional)\n",
    "    mec_map = None\n",
    "    if \"agent_to_mec\" in env_cfg:\n",
    "        a2m = env_cfg[\"agent_to_mec\"]\n",
    "        if isinstance(a2m, pd.Series):\n",
    "            mec_map = a2m.rename(\"mec_id\").reset_index()\n",
    "            # if the index column name is lost, normalize it\n",
    "            if mec_map.columns.tolist() == [\"index\", \"mec_id\"]:\n",
    "                mec_map.rename(columns={\"index\": \"agent_id\"}, inplace=True)\n",
    "        else:\n",
    "            mec_map = pd.DataFrame({\n",
    "                \"agent_id\": np.arange(len(a2m), dtype=int),\n",
    "                \"mec_id\": np.asarray(a2m, dtype=int)\n",
    "            })\n",
    "\n",
    "    # Merge all components\n",
    "    base = agents[[\"agent_id\", \"f_local\", \"f_local_slot\", \"m_local\"]].copy()\n",
    "    base[[\"f_local\", \"f_local_slot\", \"m_local\"]] = base[[\"f_local\", \"f_local_slot\", \"m_local\"]].astype(float)\n",
    "\n",
    "    prof = (base\n",
    "            .merge(lam_df,  on=\"agent_id\", how=\"left\")\n",
    "            .merge(dist_df, on=\"agent_id\", how=\"left\")\n",
    "            .merge(na_df,   on=\"agent_id\", how=\"left\"))\n",
    "\n",
    "    if mec_map is not None:\n",
    "        prof = prof.merge(mec_map, on=\"agent_id\", how=\"left\")\n",
    "\n",
    "    # Fill missing for agents with no arrivals/tasks\n",
    "    fill_zero = [\n",
    "        \"lambda_mean\", \"lambda_var\", \"slots_observed\",\n",
    "        \"n_tasks_agent\", \"non_atomic_share\",\n",
    "        \"TaskDist_sum\", \"b_mb_med\", \"rho_med\", \"mem_med\", \"hard_share\"\n",
    "    ] + [f\"P_{t}\" for t in _TASK_TYPES]\n",
    "    for c in fill_zero:\n",
    "        if c in prof.columns:\n",
    "            prof[c] = prof[c].fillna(0.0).astype(float)\n",
    "\n",
    "    # Soft warning if probabilities don't sum to ~1 for agents with tasks\n",
    "    if \"n_tasks_agent\" in prof.columns and \"TaskDist_sum\" in prof.columns:\n",
    "        mask = (prof[\"n_tasks_agent\"] > 0) & (~np.isclose(prof[\"TaskDist_sum\"], 1.0, atol=1e-6))\n",
    "        if mask.any():\n",
    "            n_bad = int(mask.sum())\n",
    "            print(f\"[warn] TaskDist_sum != 1.0 for {n_bad} agent(s). (tolerance 1e-6)\")\n",
    "\n",
    "    return prof\n",
    "\n",
    "# ---- Batch profiling for ALL env_configs ----\n",
    "def build_all_agent_profiles(env_configs: Dict[str, Dict[str, Dict[str, Any]]]):\n",
    "    \"\"\"\n",
    "    Compute profiles for every (episode → topology → scenario) environment.\n",
    "    Stores result both in return dict AND env_configs[...] for convenience.\n",
    "    Output:\n",
    "      profiles[ep_name][topology_name][scen_name] = DataFrame\n",
    "    Also writes back to: env_configs[ep_name][topology_name][scen_name][\"agent_profiles\"]\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        out[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            out[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                prof = build_agent_profiles_for_env(env_cfg)\n",
    "                out[ep_name][topo_name][scen_name] = prof\n",
    "                env_cfg[\"agent_profiles\"] = prof  # attach for direct access\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===EXAMPLE===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>f_local_slot</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_agent</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "      <th>mec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>46.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.053006</td>\n",
       "      <td>1.346282e+09</td>\n",
       "      <td>63.366714</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.674345</td>\n",
       "      <td>1.242386e+09</td>\n",
       "      <td>77.133770</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.414545</td>\n",
       "      <td>1.479269e+09</td>\n",
       "      <td>62.402143</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.166774</td>\n",
       "      <td>1.297716e+09</td>\n",
       "      <td>53.336022</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>1.268293</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>41.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.142497</td>\n",
       "      <td>1.451104e+09</td>\n",
       "      <td>72.397343</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id       f_local  f_local_slot      m_local  lambda_mean  lambda_var  \\\n",
       "0         0  1.741183e+09  1.741183e+09  5713.849721     1.500000    0.566667   \n",
       "1         1  1.352326e+09  1.352326e+09  4566.428755     1.062500    0.062500   \n",
       "2         2  1.726668e+09  1.726668e+09  5815.120004     1.181818    0.251082   \n",
       "3         3  1.543616e+09  1.543616e+09  3539.850245     1.200000    0.166667   \n",
       "4         4  1.130883e+09  1.130883e+09  4161.367769     1.268293    0.351220   \n",
       "\n",
       "   slots_observed  n_tasks_agent  TaskDist_sum  b_mb_med       rho_med  \\\n",
       "0            46.0           69.0           1.0  5.053006  1.346282e+09   \n",
       "1            16.0           17.0           1.0  5.674345  1.242386e+09   \n",
       "2            22.0           26.0           1.0  4.414545  1.479269e+09   \n",
       "3            25.0           30.0           1.0  6.166774  1.297716e+09   \n",
       "4            41.0           52.0           1.0  5.142497  1.451104e+09   \n",
       "\n",
       "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
       "0  63.366714    0.304348   0.260870                  0.0         0.304348   \n",
       "1  77.133770    0.294118   0.176471                  0.0         0.294118   \n",
       "2  62.402143    0.153846   0.192308                  0.0         0.153846   \n",
       "3  53.336022    0.300000   0.200000                  0.0         0.300000   \n",
       "4  72.397343    0.250000   0.153846                  0.0         0.250000   \n",
       "\n",
       "   P_data_intensive  P_compute_intensive  non_atomic_share  mec_id  \n",
       "0          0.086957             0.347826          0.449275       0  \n",
       "1          0.058824             0.470588          0.411765       1  \n",
       "2          0.115385             0.538462          0.500000       2  \n",
       "3          0.166667             0.333333          0.433333       3  \n",
       "4          0.076923             0.519231          0.384615       4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>f_local</th>\n",
       "      <th>f_local_slot</th>\n",
       "      <th>m_local</th>\n",
       "      <th>lambda_mean</th>\n",
       "      <th>lambda_var</th>\n",
       "      <th>slots_observed</th>\n",
       "      <th>n_tasks_agent</th>\n",
       "      <th>TaskDist_sum</th>\n",
       "      <th>b_mb_med</th>\n",
       "      <th>rho_med</th>\n",
       "      <th>mem_med</th>\n",
       "      <th>hard_share</th>\n",
       "      <th>P_general</th>\n",
       "      <th>P_latency_sensitive</th>\n",
       "      <th>P_deadline_hard</th>\n",
       "      <th>P_data_intensive</th>\n",
       "      <th>P_compute_intensive</th>\n",
       "      <th>non_atomic_share</th>\n",
       "      <th>mec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>1.741183e+09</td>\n",
       "      <td>5713.849721</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>46.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.053006</td>\n",
       "      <td>1.346282e+09</td>\n",
       "      <td>63.366714</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>1.352326e+09</td>\n",
       "      <td>4566.428755</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.674345</td>\n",
       "      <td>1.242386e+09</td>\n",
       "      <td>77.133770</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>1.726668e+09</td>\n",
       "      <td>5815.120004</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.414545</td>\n",
       "      <td>1.479269e+09</td>\n",
       "      <td>62.402143</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>1.543616e+09</td>\n",
       "      <td>3539.850245</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.166774</td>\n",
       "      <td>1.297716e+09</td>\n",
       "      <td>53.336022</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>1.130883e+09</td>\n",
       "      <td>4161.367769</td>\n",
       "      <td>1.268293</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>41.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.142497</td>\n",
       "      <td>1.451104e+09</td>\n",
       "      <td>72.397343</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.761336e+09</td>\n",
       "      <td>1.761336e+09</td>\n",
       "      <td>7349.509397</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.822275</td>\n",
       "      <td>1.386577e+09</td>\n",
       "      <td>56.527901</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.093196e+09</td>\n",
       "      <td>1.093196e+09</td>\n",
       "      <td>6389.821853</td>\n",
       "      <td>1.381818</td>\n",
       "      <td>0.499663</td>\n",
       "      <td>55.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.733181</td>\n",
       "      <td>1.644036e+09</td>\n",
       "      <td>66.952274</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.022066e+09</td>\n",
       "      <td>1.022066e+09</td>\n",
       "      <td>5354.805771</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.053420</td>\n",
       "      <td>1.420189e+09</td>\n",
       "      <td>64.155410</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.280853e+09</td>\n",
       "      <td>1.280853e+09</td>\n",
       "      <td>3931.170813</td>\n",
       "      <td>1.377778</td>\n",
       "      <td>0.376768</td>\n",
       "      <td>45.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.871756</td>\n",
       "      <td>1.533771e+09</td>\n",
       "      <td>61.109354</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.260889e+09</td>\n",
       "      <td>1.260889e+09</td>\n",
       "      <td>5133.304392</td>\n",
       "      <td>1.185185</td>\n",
       "      <td>0.618234</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.111011</td>\n",
       "      <td>1.443404e+09</td>\n",
       "      <td>57.667799</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8.077840e+08</td>\n",
       "      <td>8.077840e+08</td>\n",
       "      <td>6774.364061</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.839872</td>\n",
       "      <td>1.428540e+09</td>\n",
       "      <td>70.264100</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.252718e+09</td>\n",
       "      <td>1.252718e+09</td>\n",
       "      <td>5907.761753</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.345289</td>\n",
       "      <td>1.493732e+09</td>\n",
       "      <td>70.253052</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.849537e+09</td>\n",
       "      <td>1.849537e+09</td>\n",
       "      <td>4251.816941</td>\n",
       "      <td>1.208333</td>\n",
       "      <td>0.259058</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.222549</td>\n",
       "      <td>1.631176e+09</td>\n",
       "      <td>61.159348</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.013685e+09</td>\n",
       "      <td>2.013685e+09</td>\n",
       "      <td>3452.486291</td>\n",
       "      <td>1.147059</td>\n",
       "      <td>0.129234</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.512488</td>\n",
       "      <td>1.359023e+09</td>\n",
       "      <td>64.629200</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.566043e+09</td>\n",
       "      <td>1.566043e+09</td>\n",
       "      <td>6514.168780</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.648449</td>\n",
       "      <td>1.377425e+09</td>\n",
       "      <td>54.745849</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.573528e+09</td>\n",
       "      <td>1.573528e+09</td>\n",
       "      <td>7018.517023</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>55.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.289393</td>\n",
       "      <td>1.480687e+09</td>\n",
       "      <td>62.983881</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.386095e+09</td>\n",
       "      <td>1.386095e+09</td>\n",
       "      <td>6122.347442</td>\n",
       "      <td>1.147059</td>\n",
       "      <td>0.189840</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.080036</td>\n",
       "      <td>1.375849e+09</td>\n",
       "      <td>58.932434</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.604826e+09</td>\n",
       "      <td>1.604826e+09</td>\n",
       "      <td>3449.292167</td>\n",
       "      <td>1.314286</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.105387</td>\n",
       "      <td>1.415512e+09</td>\n",
       "      <td>63.343077</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    agent_id       f_local  f_local_slot      m_local  lambda_mean  \\\n",
       "0          0  1.741183e+09  1.741183e+09  5713.849721     1.500000   \n",
       "1          1  1.352326e+09  1.352326e+09  4566.428755     1.062500   \n",
       "2          2  1.726668e+09  1.726668e+09  5815.120004     1.181818   \n",
       "3          3  1.543616e+09  1.543616e+09  3539.850245     1.200000   \n",
       "4          4  1.130883e+09  1.130883e+09  4161.367769     1.268293   \n",
       "5          5  1.761336e+09  1.761336e+09  7349.509397     1.200000   \n",
       "6          6  1.093196e+09  1.093196e+09  6389.821853     1.381818   \n",
       "7          7  1.022066e+09  1.022066e+09  5354.805771     1.250000   \n",
       "8          8  1.280853e+09  1.280853e+09  3931.170813     1.377778   \n",
       "9          9  1.260889e+09  1.260889e+09  5133.304392     1.185185   \n",
       "10        10  8.077840e+08  8.077840e+08  6774.364061     1.571429   \n",
       "11        11  1.252718e+09  1.252718e+09  5907.761753     1.120000   \n",
       "12        12  1.849537e+09  1.849537e+09  4251.816941     1.208333   \n",
       "13        13  2.013685e+09  2.013685e+09  3452.486291     1.147059   \n",
       "14        14  1.566043e+09  1.566043e+09  6514.168780     1.272727   \n",
       "15        15  1.573528e+09  1.573528e+09  7018.517023     1.636364   \n",
       "16        16  1.386095e+09  1.386095e+09  6122.347442     1.147059   \n",
       "17        17  1.604826e+09  1.604826e+09  3449.292167     1.314286   \n",
       "\n",
       "    lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  b_mb_med  \\\n",
       "0     0.566667            46.0           69.0           1.0  5.053006   \n",
       "1     0.062500            16.0           17.0           1.0  5.674345   \n",
       "2     0.251082            22.0           26.0           1.0  4.414545   \n",
       "3     0.166667            25.0           30.0           1.0  6.166774   \n",
       "4     0.351220            41.0           52.0           1.0  5.142497   \n",
       "5     0.341176            35.0           42.0           1.0  4.822275   \n",
       "6     0.499663            55.0           76.0           1.0  4.733181   \n",
       "7     0.250000            36.0           45.0           1.0  6.053420   \n",
       "8     0.376768            45.0           62.0           1.0  4.871756   \n",
       "9     0.618234            27.0           32.0           1.0  5.111011   \n",
       "10    0.625000            49.0           77.0           1.0  4.839872   \n",
       "11    0.110000            25.0           28.0           1.0  4.345289   \n",
       "12    0.259058            24.0           29.0           1.0  5.222549   \n",
       "13    0.129234            34.0           39.0           1.0  5.512488   \n",
       "14    0.329545            33.0           42.0           1.0  4.648449   \n",
       "15    0.791246            55.0           90.0           1.0  5.289393   \n",
       "16    0.189840            34.0           39.0           1.0  5.080036   \n",
       "17    0.457143            35.0           46.0           1.0  5.105387   \n",
       "\n",
       "         rho_med    mem_med  hard_share  P_general  P_latency_sensitive  \\\n",
       "0   1.346282e+09  63.366714    0.304348   0.260870                  0.0   \n",
       "1   1.242386e+09  77.133770    0.294118   0.176471                  0.0   \n",
       "2   1.479269e+09  62.402143    0.153846   0.192308                  0.0   \n",
       "3   1.297716e+09  53.336022    0.300000   0.200000                  0.0   \n",
       "4   1.451104e+09  72.397343    0.250000   0.153846                  0.0   \n",
       "5   1.386577e+09  56.527901    0.428571   0.214286                  0.0   \n",
       "6   1.644036e+09  66.952274    0.276316   0.236842                  0.0   \n",
       "7   1.420189e+09  64.155410    0.333333   0.200000                  0.0   \n",
       "8   1.533771e+09  61.109354    0.435484   0.161290                  0.0   \n",
       "9   1.443404e+09  57.667799    0.343750   0.187500                  0.0   \n",
       "10  1.428540e+09  70.264100    0.441558   0.129870                  0.0   \n",
       "11  1.493732e+09  70.253052    0.392857   0.178571                  0.0   \n",
       "12  1.631176e+09  61.159348    0.241379   0.103448                  0.0   \n",
       "13  1.359023e+09  64.629200    0.307692   0.128205                  0.0   \n",
       "14  1.377425e+09  54.745849    0.357143   0.261905                  0.0   \n",
       "15  1.480687e+09  62.983881    0.411111   0.100000                  0.0   \n",
       "16  1.375849e+09  58.932434    0.282051   0.256410                  0.0   \n",
       "17  1.415512e+09  63.343077    0.413043   0.173913                  0.0   \n",
       "\n",
       "    P_deadline_hard  P_data_intensive  P_compute_intensive  non_atomic_share  \\\n",
       "0          0.304348          0.086957             0.347826          0.449275   \n",
       "1          0.294118          0.058824             0.470588          0.411765   \n",
       "2          0.153846          0.115385             0.538462          0.500000   \n",
       "3          0.300000          0.166667             0.333333          0.433333   \n",
       "4          0.250000          0.076923             0.519231          0.384615   \n",
       "5          0.428571          0.119048             0.238095          0.476190   \n",
       "6          0.276316          0.092105             0.394737          0.473684   \n",
       "7          0.333333          0.177778             0.288889          0.444444   \n",
       "8          0.435484          0.080645             0.322581          0.419355   \n",
       "9          0.343750          0.093750             0.375000          0.468750   \n",
       "10         0.441558          0.064935             0.363636          0.480519   \n",
       "11         0.392857          0.071429             0.357143          0.428571   \n",
       "12         0.241379          0.172414             0.482759          0.551724   \n",
       "13         0.307692          0.128205             0.435897          0.333333   \n",
       "14         0.357143          0.047619             0.333333          0.547619   \n",
       "15         0.411111          0.144444             0.344444          0.466667   \n",
       "16         0.282051          0.102564             0.358974          0.487179   \n",
       "17         0.413043          0.217391             0.195652          0.260870   \n",
       "\n",
       "    mec_id  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "5        5  \n",
       "6        6  \n",
       "7        7  \n",
       "8        8  \n",
       "9        9  \n",
       "10      10  \n",
       "11      11  \n",
       "12      12  \n",
       "13      13  \n",
       "14      14  \n",
       "15      15  \n",
       "16      16  \n",
       "17      17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Build + quick peek (optional) ----\n",
    "agent_profiles = build_all_agent_profiles(env_configs)\n",
    "\n",
    "# Example: Access the profile table for a specific episode / topology / scenario\n",
    "print(\"\\n ===EXAMPLE===\")\n",
    "display(agent_profiles[\"ep_000\"][\"clustered\"][\"heavy\"].head())\n",
    "\n",
    "# Alternatively, read directly from env_configs:\n",
    "display(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"agent_profiles\"].head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Delta', 'T_slots', 'K', 'topology_type', 'connection_matrix', 'private_cpu', 'public_cpu', 'cloud_cpu', 'N_agents', 'agent_to_mec', 'episodes', 'agents', 'arrivals', 'tasks', 'queues_initial', 'action_space', 'state_spec', 'checks', 'agent_profiles'])\n",
      "   agent_id       f_local  f_local_slot      m_local  lambda_mean  lambda_var  \\\n",
      "0         0  1.741183e+09  1.741183e+09  5713.849721     1.500000    0.566667   \n",
      "1         1  1.352326e+09  1.352326e+09  4566.428755     1.062500    0.062500   \n",
      "2         2  1.726668e+09  1.726668e+09  5815.120004     1.181818    0.251082   \n",
      "3         3  1.543616e+09  1.543616e+09  3539.850245     1.200000    0.166667   \n",
      "4         4  1.130883e+09  1.130883e+09  4161.367769     1.268293    0.351220   \n",
      "\n",
      "   slots_observed  n_tasks_agent  TaskDist_sum  b_mb_med       rho_med  \\\n",
      "0            46.0           69.0           1.0  5.053006  1.346282e+09   \n",
      "1            16.0           17.0           1.0  5.674345  1.242386e+09   \n",
      "2            22.0           26.0           1.0  4.414545  1.479269e+09   \n",
      "3            25.0           30.0           1.0  6.166774  1.297716e+09   \n",
      "4            41.0           52.0           1.0  5.142497  1.451104e+09   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  63.366714    0.304348   0.260870                  0.0         0.304348   \n",
      "1  77.133770    0.294118   0.176471                  0.0         0.294118   \n",
      "2  62.402143    0.153846   0.192308                  0.0         0.153846   \n",
      "3  53.336022    0.300000   0.200000                  0.0         0.300000   \n",
      "4  72.397343    0.250000   0.153846                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share  mec_id  \n",
      "0          0.086957             0.347826          0.449275       0  \n",
      "1          0.058824             0.470588          0.411765       1  \n",
      "2          0.115385             0.538462          0.500000       2  \n",
      "3          0.166667             0.333333          0.433333       3  \n",
      "4          0.076923             0.519231          0.384615       4  \n",
      "Index(['agent_id', 'f_local', 'f_local_slot', 'm_local', 'lambda_mean',\n",
      "       'lambda_var', 'slots_observed', 'n_tasks_agent', 'TaskDist_sum',\n",
      "       'b_mb_med', 'rho_med', 'mem_med', 'hard_share', 'P_general',\n",
      "       'P_latency_sensitive', 'P_deadline_hard', 'P_data_intensive',\n",
      "       'P_compute_intensive', 'non_atomic_share', 'mec_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ep   = \"ep_000\"\n",
    "topo = \"clustered\"\n",
    "scen = \"heavy\"\n",
    "\n",
    "env_cfg = env_configs[ep][topo][scen]\n",
    "\n",
    "print(env_cfg.keys())\n",
    "\n",
    "prof = env_cfg[\"agent_profiles\"]\n",
    "print(prof.head())\n",
    "print(prof.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _ensure_dir(path: str):\n",
    "#     \"\"\"Create a folder if it does not already exist.\"\"\"\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def _serialize_non_df_components(env_cfg: dict) -> dict:\n",
    "#     \"\"\"\n",
    "#     Prepare a JSON-serializable dictionary for all non-DataFrame parts\n",
    "#     of env_config. Arrays are converted to lists.\n",
    "#     \"\"\"\n",
    "#     out = {}\n",
    "#     for key, value in env_cfg.items():\n",
    "#         if isinstance(value, pd.DataFrame):\n",
    "#             continue  # handled separately\n",
    "\n",
    "#         # numpy arrays → lists\n",
    "#         if isinstance(value, np.ndarray):\n",
    "#             out[key] = value.tolist()\n",
    "#             continue\n",
    "\n",
    "#         # dicts (queues, action_space, state_spec, checks)\n",
    "#         if isinstance(value, dict):\n",
    "#             try:\n",
    "#                 # recursively convert numpy arrays inside dicts\n",
    "#                 def _convert(obj):\n",
    "#                     if isinstance(obj, np.ndarray):\n",
    "#                         return obj.tolist()\n",
    "#                     if isinstance(obj, dict):\n",
    "#                         return {k: _convert(v) for k, v in obj.items()}\n",
    "#                     return obj\n",
    "#                 out[key] = _convert(value)\n",
    "#             except Exception as e:\n",
    "#                 out[key] = f\"(serialization error: {e})\"\n",
    "#             continue\n",
    "\n",
    "#         # scalars (int, float, str, None)\n",
    "#         if isinstance(value, (int, float, str, bool, type(None))):\n",
    "#             out[key] = value\n",
    "#             continue\n",
    "\n",
    "#         # fallback\n",
    "#         try:\n",
    "#             out[key] = json.loads(json.dumps(value))\n",
    "#         except Exception:\n",
    "#             out[key] = f\"(unserializable type: {type(value).__name__})\"\n",
    "\n",
    "#     return out\n",
    "\n",
    "# def save_all_env_configs(env_configs, out_root: str = \"./artifacts/env_configs\"):\n",
    "#     \"\"\"\n",
    "#     Save all env_configs to disk in a structured layout:\n",
    "#         artifacts/env_configs/ep_xxx/topology/scenario/\n",
    "#             tasks_env_config.csv\n",
    "#             agents_env_config.csv\n",
    "#             arrivals_env_config.csv\n",
    "#             episodes_env_config.csv\n",
    "#             env_meta.json   <-- (non-DF components)\n",
    "#     \"\"\"\n",
    "#     n_saved = 0\n",
    "\n",
    "#     for ep_name, by_topo in env_configs.items():\n",
    "#         for topo_name, by_scen in by_topo.items():\n",
    "#             for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "#                 out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "#                 _ensure_dir(out_dir)\n",
    "\n",
    "#                 # ---- Save DataFrame components ----\n",
    "#                 for df_name, df in env_cfg.items():\n",
    "#                     if isinstance(df, pd.DataFrame):\n",
    "#                         file_path_csv = os.path.join(out_dir, f\"{df_name}_env_config.csv\")\n",
    "#                         df.to_csv(file_path_csv, index=False)\n",
    "\n",
    "#                         print(f\"[saved] {file_path_csv}  (rows={len(df)})\")\n",
    "#                         n_saved += 1\n",
    "\n",
    "#                 # ---- Save non-DataFrame metadata ----\n",
    "#                 meta = _serialize_non_df_components(env_cfg)\n",
    "\n",
    "#                 meta_path = os.path.join(out_dir, \"env_meta.json\")\n",
    "#                 with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#                     json.dump(meta, f, indent=2)\n",
    "\n",
    "#                 print(f\"[saved] {meta_path}\")\n",
    "\n",
    "#     print(f\"\\nDone. Saved {n_saved} DataFrames + meta files for all env_configs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_all_env_configs(env_configs, out_root=\"./artifacts/env_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 4: Clustering Agents </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.1. Feature Matrix </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform clustering, the characteristics of each agent must first be stored in a feature matrix. These characteristics include:\n",
    "\n",
    "1) Local resources\n",
    "2) task generation pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_FEATURES_V1 = [\n",
    "    # ---- (1) Local resources ----\n",
    "    \"f_local_slot\",   # Local CPU cycles per slot\n",
    "    \"m_local\",        # Local memory capacity\n",
    "    \"lambda_mean\",    # Mean task arrival rate\n",
    "    \"lambda_var\",     # Variance of task arrival rate\n",
    "\n",
    "    # ---- (2) Task generation pattern ----\n",
    "    # Derived from labeled task distribution across final_flag categories\n",
    "    \"P_deadline_hard\",\n",
    "    \"P_latency_sensitive\",\n",
    "    \"P_compute_intensive\",\n",
    "    \"P_data_intensive\",\n",
    "    \"P_general\",\n",
    "\n",
    "    # ---- (optional statistical descriptors of generated tasks) ----\n",
    "    \"b_mb_med\",       # Median input size\n",
    "    \"rho_med\",        # Median compute demand (cycles/MB)\n",
    "    \"mem_med\",        # Median memory demand (MB)\n",
    "    \"non_atomic_share\", # Share of splittable tasks\n",
    "    \"hard_share\"        # Share of hard-deadline tasks\n",
    "]\n",
    "\n",
    "# Features that need normalization\n",
    "FEATURES_TO_STANDARDIZE = [\n",
    "    \"f_local_slot\",   # Local CPU cycles per slot\n",
    "    \"m_local\",        # Local memory capacity\n",
    "    \"lambda_mean\",    # Mean task arrival rate\n",
    "    \"lambda_var\",     # Variance of task arrival rate\n",
    "    \"b_mb_med\",       # Median input size\n",
    "    \"rho_med\",        # Median compute demand (cycles/MB)\n",
    "    \"mem_med\",        # Median memory demand (MB)\n",
    "]\n",
    "\n",
    "# Utility: Keep only existing columns; others will be filled with zeros\n",
    "def _safe_cols(df: pd.DataFrame, cols: List[str]) -> List[str]:\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "# Normalize features that need normalization\n",
    "def normalize_features(X: np.ndarray, cols: List[str], feature_list: List[str]) -> np.ndarray:\n",
    "    # Identify which columns need to be standardized\n",
    "    standardize_cols = [col for col in feature_list if col in FEATURES_TO_STANDARDIZE]\n",
    "    if len(standardize_cols) > 0:\n",
    "        # Apply scaling only to the columns that need standardization\n",
    "        scaler = StandardScaler()\n",
    "        col_indices = [cols.index(col) for col in standardize_cols]\n",
    "        X[:, col_indices] = scaler.fit_transform(X[:, col_indices])\n",
    "    return X\n",
    "\n",
    "# Build feature matrix for one environment configuration\n",
    "def make_agent_feature_matrix_for_env(\n",
    "    env_cfg: Dict[str, Any],\n",
    "    feature_list: Optional[List[str]] = None,\n",
    "    standardize: bool = True,\n",
    ") -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build the feature matrix (X) for all agents in one environment configuration.\n",
    "    Each row represents an agent; each column a numerical feature.\n",
    "    \n",
    "    Returns:\n",
    "        X_scaled       : np.ndarray (n_agents × n_features)\n",
    "        used_cols      : list of feature names in order\n",
    "        agent_ids      : np.ndarray of agent identifiers\n",
    "    \"\"\"\n",
    "    if \"agent_profiles\" not in env_cfg or not isinstance(env_cfg[\"agent_profiles\"], pd.DataFrame):\n",
    "        raise ValueError(\"env_cfg['agent_profiles'] must contain a valid DataFrame.\")\n",
    "\n",
    "    prof = env_cfg[\"agent_profiles\"].copy()\n",
    "    if \"agent_id\" not in prof.columns:\n",
    "        raise ValueError(\"agent_profiles must include column 'agent_id'.\")\n",
    "\n",
    "    if feature_list is None:\n",
    "        feature_list = AGENT_FEATURES_V1\n",
    "\n",
    "    # Keep valid features and fill missing ones with zeros\n",
    "    cols = _safe_cols(prof, feature_list)\n",
    "    X = prof.reindex(columns=cols).fillna(0.0).astype(float).to_numpy()\n",
    "    agent_ids = prof[\"agent_id\"].to_numpy(dtype=int)\n",
    "\n",
    "    # Standardize only the required features\n",
    "    if standardize:\n",
    "        X = normalize_features(X, cols, feature_list)\n",
    "\n",
    "    return X, cols, agent_ids\n",
    "\n",
    "# Attach computed features to the environment configuration\n",
    "def attach_features_to_env(env_cfg: Dict[str, Any],\n",
    "                           feature_list: Optional[List[str]] = None,\n",
    "                           standardize: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Attach the constructed feature matrix and related metadata\n",
    "    to env_cfg[\"clustering\"][\"features\"]. \n",
    "    \"\"\"\n",
    "    X, cols, agent_ids = make_agent_feature_matrix_for_env(env_cfg, feature_list, standardize)\n",
    "\n",
    "    env_cfg.setdefault(\"clustering\", {})\n",
    "    env_cfg[\"clustering\"][\"features\"] = {\n",
    "        \"X\": X,                          # Feature matrix (scaled)\n",
    "        \"feature_cols\": cols,            # List of column names\n",
    "        \"agent_ids\": agent_ids,          # Agent identifiers\n",
    "        \"n_agents\": int(X.shape[0]),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "    return env_cfg\n",
    "\n",
    "# Apply feature construction to all topology-scenario combinations\n",
    "def attach_features_to_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    feature_list: Optional[List[str]] = None,\n",
    "    standardize: bool = True,\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Iterate through all (episode → topology → scenario) combinations\n",
    "    and build the feature matrix for each one.\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                env_configs[ep_name][topo_name][scen_name] = attach_features_to_env(\n",
    "                    env_cfg, feature_list, standardize\n",
    "                )\n",
    "                fz = env_configs[ep_name][topo_name][scen_name][\"clustering\"][\"features\"]\n",
    "                print(f\"[features] {ep_name}/{topo_name}/{scen_name} \"\n",
    "                      f\"-> X.shape={fz['X'].shape}  (agents={fz['n_agents']}, feats={fz['n_features']})\")\n",
    "    return env_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assert_no_nan_inf(X: np.ndarray, where: str):\n",
    "    if not np.isfinite(X).all():\n",
    "        bad = np.isnan(X).sum(), np.isinf(X).sum()\n",
    "        raise AssertionError(f\"{where}: Feature matrix contains NaN or Inf. counts={bad}\")\n",
    "\n",
    "def _assert_agent_count_match(env_cfg: Dict[str, Any], where: str):\n",
    "    n_agents_ep = int(env_cfg[\"episodes\"][\"N_agents\"].iloc[0])\n",
    "    n_agents_prof = len(env_cfg[\"agent_profiles\"])\n",
    "    fz = env_cfg[\"clustering\"][\"features\"]\n",
    "    if not (fz[\"n_agents\"] == n_agents_prof == n_agents_ep):\n",
    "        raise AssertionError(\n",
    "            f\"{where}: Agent count mismatch. episodes={n_agents_ep}, \"\n",
    "            f\"profiles={n_agents_prof}, X={fz['n_agents']}\"\n",
    "        )\n",
    "\n",
    "def _assert_feature_prob_sum_hint(env_cfg: Dict[str, Any], tol=1e-3):\n",
    "    prof = env_cfg[\"agent_profiles\"]\n",
    "    if \"TaskDist_sum\" in prof.columns and \"n_tasks_agent\" in prof.columns:\n",
    "        mask = prof[\"n_tasks_agent\"] > 0\n",
    "        if mask.any():\n",
    "            mean_sum = float(prof.loc[mask, \"TaskDist_sum\"].mean())\n",
    "            if abs(mean_sum - 1.0) > tol:\n",
    "                print(f\"[warn] Mean(TaskDist_sum)={mean_sum:.4f} ≠ 1 (tol={tol})\")\n",
    "\n",
    "def run_feature_matrix_sanity_checks(env_configs: Dict[str, Dict[str, Dict[str, Any]]]):\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                where = f\"{ep_name}/{topo_name}/{scen_name}\"\n",
    "                if \"clustering\" not in env_cfg or \"features\" not in env_cfg[\"clustering\"]:\n",
    "                    raise AssertionError(f\"{where}: Missing features.\")\n",
    "                X = env_cfg[\"clustering\"][\"features\"][\"X\"]\n",
    "                _assert_no_nan_inf(X, where)\n",
    "                _assert_agent_count_match(env_cfg, where)\n",
    "                _assert_feature_prob_sum_hint(env_cfg)\n",
    "                if X.shape[1] == 0:\n",
    "                    raise AssertionError(f\"{where}: Empty feature matrix.\")\n",
    "    print(\"[checks] All sanity checks passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features] ep_000/clustered/heavy -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/clustered/light -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/clustered/moderate -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/full_mesh/heavy -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/full_mesh/light -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/full_mesh/moderate -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/sparse_ring/heavy -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/sparse_ring/light -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[features] ep_000/sparse_ring/moderate -> X.shape=(18, 14)  (agents=18, feats=14)\n",
      "[checks] All sanity checks passed successfully.\n",
      "\n",
      "=== EXAMPLE: features of ep_000 / clustered / heavy ===\n",
      "X.shape: (18, 14)\n",
      "feature_cols: ['f_local_slot', 'm_local', 'lambda_mean', 'lambda_var', 'P_deadline_hard', 'P_latency_sensitive', 'P_compute_intensive', 'P_data_intensive', 'P_general', 'b_mb_med', 'rho_med', 'mem_med', 'non_atomic_share', 'hard_share']\n",
      "agent_ids (first 10): [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Build feature matrices for all envs\n",
    "env_configs = attach_features_to_all_envs(env_configs, feature_list=AGENT_FEATURES_V1, standardize=True)\n",
    "\n",
    "# Run sanity checks\n",
    "run_feature_matrix_sanity_checks(env_configs)\n",
    "\n",
    "# Example inspection\n",
    "print(\"\\n=== EXAMPLE: features of ep_000 / clustered / heavy ===\")\n",
    "fz = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"features\"]\n",
    "print(\"X.shape:\", fz[\"X\"].shape)\n",
    "print(\"feature_cols:\", fz[\"feature_cols\"])\n",
    "print(\"agent_ids (first 10):\", fz[\"agent_ids\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing features: []\n",
      "   f_local_slot      m_local  lambda_mean  lambda_var  P_deadline_hard  \\\n",
      "0  1.741183e+09  5713.849721     1.500000    0.566667         0.304348   \n",
      "1  1.352326e+09  4566.428755     1.062500    0.062500         0.294118   \n",
      "2  1.726668e+09  5815.120004     1.181818    0.251082         0.153846   \n",
      "3  1.543616e+09  3539.850245     1.200000    0.166667         0.300000   \n",
      "4  1.130883e+09  4161.367769     1.268293    0.351220         0.250000   \n",
      "\n",
      "   P_latency_sensitive  P_compute_intensive  P_data_intensive  P_general  \\\n",
      "0                  0.0             0.347826          0.086957   0.260870   \n",
      "1                  0.0             0.470588          0.058824   0.176471   \n",
      "2                  0.0             0.538462          0.115385   0.192308   \n",
      "3                  0.0             0.333333          0.166667   0.200000   \n",
      "4                  0.0             0.519231          0.076923   0.153846   \n",
      "\n",
      "   b_mb_med       rho_med    mem_med  non_atomic_share  hard_share  \n",
      "0  5.053006  1.346282e+09  63.366714          0.449275    0.304348  \n",
      "1  5.674345  1.242386e+09  77.133770          0.411765    0.294118  \n",
      "2  4.414545  1.479269e+09  62.402143          0.500000    0.153846  \n",
      "3  6.166774  1.297716e+09  53.336022          0.433333    0.300000  \n",
      "4  5.142497  1.451104e+09  72.397343          0.384615    0.250000  \n"
     ]
    }
   ],
   "source": [
    "print(\"missing features:\",\n",
    "      [c for c in AGENT_FEATURES_V1 if c not in prof.columns])\n",
    "\n",
    "print(prof[ [c for c in AGENT_FEATURES_V1 if c in prof.columns] ].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (18, 14)\n",
      "feature_cols actually used: ['f_local_slot', 'm_local', 'lambda_mean', 'lambda_var', 'P_deadline_hard', 'P_latency_sensitive', 'P_compute_intensive', 'P_data_intensive', 'P_general', 'b_mb_med', 'rho_med', 'mem_med', 'non_atomic_share', 'hard_share']\n",
      "agent_ids[:10]: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "fz = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"features\"]\n",
    "print(\"X.shape:\", fz[\"X\"].shape)\n",
    "print(\"feature_cols actually used:\", fz[\"feature_cols\"])\n",
    "print(\"agent_ids[:10]:\", fz[\"agent_ids\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.2. Optimal Number of Clusters </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the optimal number of clusters (K), we use a hybrid method that combines evaluation indices such as WCSS, Silhouette, DBI, and CH Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4.2.1 Candidate K values ----------\n",
    "def _candidate_K_values(\n",
    "    n_agents: int,\n",
    "    k_min: int = 2,\n",
    "    max_K_fraction: float = 0.25,\n",
    "    max_K_abs: int = 10\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a reasonable candidate set for K given n_agents.\n",
    "\n",
    "    - Lower bound is k_min (default 2).\n",
    "    - Upper bound is min(max_K_abs, floor(max_K_fraction * n_agents), n_agents - 1).\n",
    "    - If n_agents is too small, returns an empty list.\n",
    "    \"\"\"\n",
    "    if n_agents <= k_min:\n",
    "        return []\n",
    "\n",
    "    k_max_by_fraction = int(np.floor(max_K_fraction * n_agents))\n",
    "    k_max = min(max_K_abs, n_agents - 1, max(k_min, k_max_by_fraction))\n",
    "\n",
    "    if k_max < k_min:\n",
    "        return []\n",
    "\n",
    "    return list(range(k_min, k_max + 1))\n",
    "\n",
    "# ---------- 4.2.2 Evaluate KMeans for a single K ----------\n",
    "def _evaluate_kmeans_for_K(\n",
    "    X: np.ndarray,\n",
    "    K: int,\n",
    "    random_state: int = 42\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run KMeans for a given K and compute clustering metrics.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"K\": int,\n",
    "          \"inertia\": float,\n",
    "          \"silhouette\": float or np.nan,\n",
    "          \"davies_bouldin\": float or np.nan,\n",
    "          \"calinski_harabasz\": float or np.nan,\n",
    "        }\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    result = {\n",
    "        \"K\": int(K),\n",
    "        \"inertia\": np.nan,\n",
    "        \"silhouette\": np.nan,\n",
    "        \"davies_bouldin\": np.nan,\n",
    "        \"calinski_harabasz\": np.nan,\n",
    "    }\n",
    "\n",
    "    if K <= 1 or K > n_samples:\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        km = KMeans(\n",
    "            n_clusters=K,\n",
    "            random_state=random_state,\n",
    "            n_init=\"auto\"\n",
    "        )\n",
    "        labels = km.fit_predict(X)\n",
    "        result[\"inertia\"] = float(km.inertia_)\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "        if unique_labels.shape[0] > 1:\n",
    "            # Silhouette\n",
    "            try:\n",
    "                result[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"silhouette\"] = np.nan\n",
    "\n",
    "            # Davies–Bouldin\n",
    "            try:\n",
    "                result[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"davies_bouldin\"] = np.nan\n",
    "\n",
    "            # Calinski–Harabasz\n",
    "            try:\n",
    "                result[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "            except Exception:\n",
    "                result[\"calinski_harabasz\"] = np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] KMeans failed for K={K}: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- 4.2.3 Min-max normalization ----------\n",
    "def _min_max_normalize(\n",
    "    arr: np.ndarray,\n",
    "    invert: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Min-max normalize a 1D array to [0, 1].\n",
    "\n",
    "    - If all values are NaN or the range is zero, returns NaN array.\n",
    "    - If invert=True, larger original values map to lower normalized ones.\n",
    "      (Useful if 'smaller is better' in the original metric.)\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    if np.all(np.isnan(arr)):\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    valid = ~np.isnan(arr)\n",
    "    if valid.sum() <= 1:\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    vmin = np.nanmin(arr[valid])\n",
    "    vmax = np.nanmax(arr[valid])\n",
    "    if vmax - vmin == 0:\n",
    "        return np.full_like(arr, np.nan)\n",
    "\n",
    "    norm = (arr - vmin) / (vmax - vmin)\n",
    "    if invert:\n",
    "        norm = 1.0 - norm\n",
    "    return norm\n",
    "\n",
    "# ---------- 4.2.4 Add composite score ----------\n",
    "def _add_composite_score(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.4,\n",
    "    gamma: float = 0.2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given metrics_df with:\n",
    "        K, inertia, silhouette, calinski_harabasz, davies_bouldin\n",
    "\n",
    "    Add:\n",
    "        sil_norm, ch_norm, db_norm, score\n",
    "    where:\n",
    "        score = alpha * sil_norm + beta * ch_norm - gamma * db_norm\n",
    "    \"\"\"\n",
    "    df = metrics_df.copy().reset_index(drop=True)\n",
    "\n",
    "    sil = df[\"silhouette\"].to_numpy(dtype=float)\n",
    "    ch  = df[\"calinski_harabasz\"].to_numpy(dtype=float)\n",
    "    db  = df[\"davies_bouldin\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Silhouette: higher is better\n",
    "    df[\"sil_norm\"] = _min_max_normalize(sil, invert=False)\n",
    "\n",
    "    # Calinski–Harabasz: higher is better\n",
    "    df[\"ch_norm\"] = _min_max_normalize(ch, invert=False)\n",
    "\n",
    "    # Davies–Bouldin: lower is better → we subtract db_norm in the score\n",
    "    df[\"db_norm\"] = _min_max_normalize(db, invert=False)\n",
    "\n",
    "    df[\"score\"] = (\n",
    "        alpha * df[\"sil_norm\"].fillna(0.0)\n",
    "        + beta * df[\"ch_norm\"].fillna(0.0)\n",
    "        - gamma * df[\"db_norm\"].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- 4.2.5 Elbow score from curvature ----------\n",
    "def _compute_elbow_rank(metrics_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a simple 'elbow_score' based on curvature of inertia vs K.\n",
    "\n",
    "    - Normalize inertia to [0,1].\n",
    "    - Use discrete second derivative:\n",
    "        curvature_i ≈ |y_{i-1} - 2*y_i + y_{i+1}|\n",
    "    - Endpoints get curvature 0.\n",
    "    \"\"\"\n",
    "    df = metrics_df.sort_values(\"K\").reset_index(drop=True)\n",
    "    inertia = df[\"inertia\"].to_numpy(dtype=float)\n",
    "\n",
    "    if inertia.shape[0] < 3:\n",
    "        return np.zeros_like(inertia, dtype=float)\n",
    "\n",
    "    inertia_norm = _min_max_normalize(inertia, invert=False)\n",
    "    if np.all(np.isnan(inertia_norm)):\n",
    "        return np.zeros_like(inertia, dtype=float)\n",
    "\n",
    "    curv = np.zeros_like(inertia_norm, dtype=float)\n",
    "    for i in range(1, len(inertia_norm) - 1):\n",
    "        y_prev = inertia_norm[i - 1]\n",
    "        y_curr = inertia_norm[i]\n",
    "        y_next = inertia_norm[i + 1]\n",
    "        if np.isnan(y_prev) or np.isnan(y_curr) or np.isnan(y_next):\n",
    "            curv[i] = 0.0\n",
    "        else:\n",
    "            curv[i] = abs(y_prev - 2.0 * y_curr + y_next)\n",
    "\n",
    "    curv_norm = _min_max_normalize(curv, invert=False)\n",
    "    curv_norm = np.nan_to_num(curv_norm, nan=0.0)\n",
    "    return curv_norm\n",
    "\n",
    "# ---------- 4.2.6 Select best_K and K_elbow ----------\n",
    "def _select_best_K_from_df(metrics_with_scores: pd.DataFrame) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Given metrics_with_scores with columns:\n",
    "        K, score, elbow_score\n",
    "\n",
    "    Returns:\n",
    "        best_K  : K with max composite score  (tie → smallest K)\n",
    "        K_elbow : K with max elbow_score     (tie → smallest K)\n",
    "    \"\"\"\n",
    "    df = metrics_with_scores.sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "    # best_K from composite score\n",
    "    if df[\"score\"].notna().any():\n",
    "        idx_best = df[\"score\"].idxmax()\n",
    "    else:\n",
    "        idx_best = df[\"K\"].idxmin()\n",
    "    best_K = int(df.loc[idx_best, \"K\"])\n",
    "\n",
    "    # K_elbow from elbow_score\n",
    "    if \"elbow_score\" in df.columns and df[\"elbow_score\"].notna().any():\n",
    "        idx_elb = df[\"elbow_score\"].idxmax()\n",
    "    else:\n",
    "        idx_elb = idx_best\n",
    "    K_elbow = int(df.loc[idx_elb, \"K\"])\n",
    "\n",
    "    return best_K, K_elbow\n",
    "\n",
    "# ---------- 4.2.7 Plot elbow + composite score ----------\n",
    "def _plot_elbow_and_scores(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    ep_name: str,\n",
    "    topo_name: str,\n",
    "    scen_name: str,\n",
    "    out_root: str = \"./artifacts/clustering\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Plot:\n",
    "      - inertia (WCSS) vs K  → classic elbow\n",
    "      - composite score vs K\n",
    "\n",
    "    Save under:\n",
    "      ./artifacts/clustering/<ep>/<topology>/<scenario>/elbow_and_score.png\n",
    "    \"\"\"\n",
    "    df = metrics_df.sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "    Ks      = df[\"K\"].to_numpy(dtype=int)\n",
    "    inertia = df[\"inertia\"].to_numpy(dtype=float)\n",
    "    scores  = df[\"score\"].to_numpy(dtype=float)\n",
    "\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"elbow_and_score.png\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Left axis: inertia (WCSS)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(Ks, inertia, marker=\"o\", linestyle=\"-\", label=\"WCSS (inertia)\")\n",
    "    ax1.set_xlabel(\"Number of clusters K\")\n",
    "    ax1.set_ylabel(\"WCSS (inertia)\")\n",
    "\n",
    "    # Right axis: composite score\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(Ks, scores, marker=\"s\", linestyle=\"--\", label=\"Composite score\")\n",
    "    ax2.set_ylabel(\"Composite score\")\n",
    "\n",
    "    title = f\"Elbow & Score: {ep_name} / {topo_name} / {scen_name}\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# ---------- 4.2.8 Main driver over all env_configs ----------\n",
    "def step4_2_select_K_for_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    random_state: int = 42,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.4,\n",
    "    gamma: float = 0.2,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Step 4.2: K-selection for all environments.\n",
    "\n",
    "    For each:\n",
    "        env_configs[ep_name][topology_name][scenario_name][\"clustering\"][\"features\"][\"X\"]\n",
    "    we:\n",
    "      - build candidate K list\n",
    "      - run KMeans for each K\n",
    "      - compute metrics + composite score\n",
    "      - compute elbow_score\n",
    "      - select best_K and K_elbow\n",
    "      - plot & save elbow figure\n",
    "      - store results in:\n",
    "            env_cfg[\"clustering\"][\"k_selection\"]\n",
    "\n",
    "    Returns:\n",
    "      K_selection[ep_name][topology_name][scenario_name] = {\n",
    "        \"best_K\", \"K_elbow\", \"metrics_df\", \"elbow_plot_path\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    K_selection: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        K_selection[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            K_selection[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                # Check clustering features exist\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"no clustering features found.\")\n",
    "                    continue\n",
    "\n",
    "                X = np.asarray(feats[\"X\"], dtype=float)\n",
    "                if X.ndim != 2 or X.shape[0] == 0:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"empty or invalid feature matrix.\")\n",
    "                    continue\n",
    "\n",
    "                n_agents = X.shape[0]\n",
    "                K_candidates = _candidate_K_values(n_agents)\n",
    "                if not K_candidates:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.2/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"not enough agents for clustering (n_agents={n_agents}).\")\n",
    "                    continue\n",
    "\n",
    "                # 1) Evaluate KMeans for all candidate K\n",
    "                metrics_list = []\n",
    "                for K in K_candidates:\n",
    "                    m = _evaluate_kmeans_for_K(X, K, random_state=random_state)\n",
    "                    metrics_list.append(m)\n",
    "                metrics_df_raw = pd.DataFrame(metrics_list).sort_values(\"K\").reset_index(drop=True)\n",
    "\n",
    "                # 2) Add composite score\n",
    "                metrics_df_full = _add_composite_score(\n",
    "                    metrics_df_raw,\n",
    "                    alpha=alpha,\n",
    "                    beta=beta,\n",
    "                    gamma=gamma\n",
    "                )\n",
    "\n",
    "                # 3) Add elbow_score\n",
    "                metrics_df_full[\"elbow_score\"] = _compute_elbow_rank(metrics_df_full)\n",
    "\n",
    "                # 4) Select best_K and K_elbow\n",
    "                best_K, K_elbow = _select_best_K_from_df(metrics_df_full)\n",
    "\n",
    "                # 5) Plot elbow + composite score\n",
    "                elbow_plot_path = _plot_elbow_and_scores(\n",
    "                    metrics_df_full,\n",
    "                    ep_name=ep_name,\n",
    "                    topo_name=topo_name,\n",
    "                    scen_name=scen_name,\n",
    "                    out_root=\"./artifacts/clustering\"\n",
    "                )\n",
    "\n",
    "                # 6) Attach to env_config\n",
    "                env_cfg.setdefault(\"clustering\", {})\n",
    "                env_cfg[\"clustering\"][\"k_selection\"] = {\n",
    "                    \"metrics_df\": metrics_df_full,\n",
    "                    \"best_K\": best_K,\n",
    "                    \"K_elbow\": K_elbow,\n",
    "                    \"elbow_plot_path\": elbow_plot_path,\n",
    "                }\n",
    "\n",
    "                # 7) Record summary\n",
    "                K_selection[ep_name][topo_name][scen_name] = {\n",
    "                    \"best_K\": best_K,\n",
    "                    \"K_elbow\": K_elbow,\n",
    "                    \"metrics_df\": metrics_df_full,\n",
    "                    \"elbow_plot_path\": elbow_plot_path,\n",
    "                }\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[4.2] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                          f\"n_agents={n_agents}, candidates={K_candidates}\")\n",
    "                    print(f\"      → best_K  (composite score) = {best_K}\")\n",
    "                    print(f\"      → K_elbow (inertia elbow)    = {K_elbow}\")\n",
    "                    print(f\"      → elbow plot saved at: {elbow_plot_path}\")\n",
    "                    cols_show = [\n",
    "                        \"K\", \"inertia\", \"silhouette\",\n",
    "                        \"calinski_harabasz\", \"davies_bouldin\", \"score\"\n",
    "                    ]\n",
    "                    print(metrics_df_full[cols_show].round(4))\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "    return K_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.2] ep_000/clustered/heavy: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 2\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/clustered/light: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/clustered/moderate: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\clustered\\moderate\\elbow_and_score.png\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/full_mesh/heavy: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 2\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\full_mesh\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/full_mesh/light: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\full_mesh\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/full_mesh/moderate: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\full_mesh\\moderate\\elbow_and_score.png\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/sparse_ring/heavy: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 2\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\sparse_ring\\heavy\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/sparse_ring/light: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\sparse_ring\\light\\elbow_and_score.png\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "------------------------------------------------------------\n",
      "[4.2] ep_000/sparse_ring/moderate: n_agents=18, candidates=[2, 3, 4]\n",
      "      → best_K  (composite score) = 4\n",
      "      → K_elbow (inertia elbow)    = 3\n",
      "      → elbow plot saved at: ./artifacts/clustering\\ep_000\\sparse_ring\\moderate\\elbow_and_score.png\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== STEP 4.2 EXAMPLE: ep_000 / clustered / heavy ===\n",
      "Chosen best_K  (composite score): 2\n",
      "Elbow-based K_elbow             : 3\n",
      "Elbow plot path                 : ./artifacts/clustering\\ep_000\\clustered\\heavy\\elbow_and_score.png\n",
      "\n",
      "Metrics per K:\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4.2.9 Example driver ----------\n",
    "# After Step 4.1 (attach_features_to_all_envs + sanity checks), run:\n",
    "K_selection = step4_2_select_K_for_all_envs(\n",
    "    env_configs,\n",
    "    random_state=42,\n",
    "    alpha=0.4,\n",
    "    beta=0.4,\n",
    "    gamma=0.2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== STEP 4.2 EXAMPLE: ep_000 / clustered / heavy ===\")\n",
    "ex_ep   = \"ep_000\"\n",
    "ex_topo = \"clustered\"\n",
    "ex_scen = \"heavy\"\n",
    "\n",
    "if (ex_ep in K_selection and\n",
    "    ex_topo in K_selection[ex_ep] and\n",
    "    ex_scen in K_selection[ex_ep][ex_topo]):\n",
    "\n",
    "    ex_sel = K_selection[ex_ep][ex_topo][ex_scen]\n",
    "    print(\"Chosen best_K  (composite score):\", ex_sel[\"best_K\"])\n",
    "    print(\"Elbow-based K_elbow             :\", ex_sel[\"K_elbow\"])\n",
    "    print(\"Elbow plot path                 :\", ex_sel[\"elbow_plot_path\"])\n",
    "    print(\"\\nMetrics per K:\")\n",
    "    print(\n",
    "        ex_sel[\"metrics_df\"][\n",
    "            [\"K\", \"inertia\", \"silhouette\", \"calinski_harabasz\", \"davies_bouldin\", \"score\"]\n",
    "        ].round(4)\n",
    "    )\n",
    "else:\n",
    "    print(\"[warn] Example triple (ep_000/clustered/heavy) not found in K_selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkups !!! (the charts are alike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. profile differences between light/moderate/heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_light vs X_heavy allclose: False\n",
      "X_light vs X_mod   allclose: False\n",
      "shapes: (18, 14) (18, 14) (18, 14)\n"
     ]
    }
   ],
   "source": [
    "ep   = \"ep_000\"\n",
    "topo = \"clustered\"\n",
    "\n",
    "X_light = env_configs[ep][topo][\"light\"][\"clustering\"][\"features\"][\"X\"]\n",
    "X_mod   = env_configs[ep][topo][\"moderate\"][\"clustering\"][\"features\"][\"X\"]\n",
    "X_heavy = env_configs[ep][topo][\"heavy\"][\"clustering\"][\"features\"][\"X\"]\n",
    "\n",
    "print(\"X_light vs X_heavy allclose:\", np.allclose(X_light, X_heavy))\n",
    "print(\"X_light vs X_mod   allclose:\", np.allclose(X_light, X_mod))\n",
    "print(\"shapes:\", X_light.shape, X_mod.shape, X_heavy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profiles equal (light vs heavy): False\n",
      "profiles equal (light vs mod)  : False\n"
     ]
    }
   ],
   "source": [
    "prof_light = env_configs[ep][topo][\"light\"][\"agent_profiles\"]\n",
    "prof_mod   = env_configs[ep][topo][\"moderate\"][\"agent_profiles\"]\n",
    "prof_heavy = env_configs[ep][topo][\"heavy\"][\"agent_profiles\"]\n",
    "\n",
    "print(\"profiles equal (light vs heavy):\", prof_light.equals(prof_heavy))\n",
    "print(\"profiles equal (light vs mod)  :\", prof_light.equals(prof_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Distributions differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ep_000 / clustered / light ===\n",
      "\n",
      "lambda stats:\n",
      "       lambda_mean  lambda_var\n",
      "count    18.000000        18.0\n",
      "mean      0.944444         0.0\n",
      "std       0.235702         0.0\n",
      "min       0.000000         0.0\n",
      "25%       1.000000         0.0\n",
      "50%       1.000000         0.0\n",
      "75%       1.000000         0.0\n",
      "max       1.000000         0.0\n",
      "\n",
      "P(task_type) stats:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        18.000000                 18.0            18.000000   \n",
      "mean          0.160626                  0.0             0.482055   \n",
      "std           0.208667                  0.0             0.354560   \n",
      "min           0.000000                  0.0             0.000000   \n",
      "25%           0.000000                  0.0             0.175000   \n",
      "50%           0.000000                  0.0             0.563492   \n",
      "75%           0.276786                  0.0             0.666667   \n",
      "max           0.600000                  0.0             1.000000   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         18.000000  18.000000  \n",
      "mean           0.115035   0.186728  \n",
      "std            0.169650   0.194114  \n",
      "min            0.000000   0.000000  \n",
      "25%            0.000000   0.000000  \n",
      "50%            0.000000   0.155556  \n",
      "75%            0.223214   0.333333  \n",
      "max            0.500000   0.500000  \n",
      "\n",
      "median task resource stats:\n",
      "        b_mb_med       rho_med     mem_med\n",
      "count  18.000000  1.800000e+01   18.000000\n",
      "mean    2.084643  8.647649e+08   66.605700\n",
      "std     0.670773  2.802916e+08   23.835542\n",
      "min     0.000000  0.000000e+00    0.000000\n",
      "25%     1.729540  8.452011e+08   59.718946\n",
      "50%     2.250744  8.754100e+08   65.891000\n",
      "75%     2.555328  9.590286e+08   68.213495\n",
      "max     2.880812  1.292143e+09  117.456750\n",
      "\n",
      "=== ep_000 / clustered / moderate ===\n",
      "\n",
      "lambda stats:\n",
      "       lambda_mean  lambda_var\n",
      "count    18.000000   18.000000\n",
      "mean      1.083422    0.097138\n",
      "std       0.093102    0.155043\n",
      "min       1.000000    0.000000\n",
      "25%       1.000000    0.000000\n",
      "50%       1.062745    0.062745\n",
      "75%       1.151754    0.136216\n",
      "max       1.333333    0.666667\n",
      "\n",
      "P(task_type) stats:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        18.000000                 18.0            18.000000   \n",
      "mean          0.218033                  0.0             0.418659   \n",
      "std           0.148835                  0.0             0.111795   \n",
      "min           0.000000                  0.0             0.227273   \n",
      "25%           0.127083                  0.0             0.343750   \n",
      "50%           0.205263                  0.0             0.394444   \n",
      "75%           0.227273                  0.0             0.488636   \n",
      "max           0.625000                  0.0             0.714286   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         18.000000  18.000000  \n",
      "mean           0.110752   0.252555  \n",
      "std            0.083903   0.115463  \n",
      "min            0.000000   0.000000  \n",
      "25%            0.063542   0.181818  \n",
      "50%            0.095455   0.252381  \n",
      "75%            0.164474   0.328947  \n",
      "max            0.318182   0.444444  \n",
      "\n",
      "median task resource stats:\n",
      "        b_mb_med       rho_med    mem_med\n",
      "count  18.000000  1.800000e+01  18.000000\n",
      "mean    2.943108  1.272108e+09  62.762239\n",
      "std     0.401258  1.549395e+08   8.642827\n",
      "min     2.286221  9.939298e+08  47.539972\n",
      "25%     2.688564  1.228825e+09  57.041995\n",
      "50%     2.948457  1.273165e+09  61.580323\n",
      "75%     3.158139  1.312963e+09  67.126681\n",
      "max     3.641778  1.631809e+09  79.851110\n",
      "\n",
      "=== ep_000 / clustered / heavy ===\n",
      "\n",
      "lambda stats:\n",
      "       lambda_mean  lambda_var\n",
      "count    18.000000   18.000000\n",
      "mean      1.279147    0.354169\n",
      "std       0.158546    0.202965\n",
      "min       1.062500    0.062500\n",
      "25%       1.182660    0.204880\n",
      "50%       1.229167    0.335361\n",
      "75%       1.361905    0.489033\n",
      "max       1.636364    0.791246\n",
      "\n",
      "P(task_type) stats:\n",
      "       P_deadline_hard  P_latency_sensitive  P_compute_intensive  \\\n",
      "count        18.000000                 18.0            18.000000   \n",
      "mean          0.331478                  0.0             0.372255   \n",
      "std           0.078748                  0.0             0.090474   \n",
      "min           0.153846                  0.0             0.195652   \n",
      "25%           0.285068                  0.0             0.333333   \n",
      "50%           0.320513                  0.0             0.358059   \n",
      "75%           0.406548                  0.0             0.425607   \n",
      "max           0.441558                  0.0             0.538462   \n",
      "\n",
      "       P_data_intensive  P_general  \n",
      "count         18.000000  18.000000  \n",
      "mean           0.112060   0.184208  \n",
      "std            0.047242   0.050132  \n",
      "min            0.047619   0.100000  \n",
      "25%            0.077854   0.155707  \n",
      "50%            0.098157   0.183036  \n",
      "75%            0.140385   0.210714  \n",
      "max            0.217391   0.261905  \n",
      "\n",
      "median task resource stats:\n",
      "        b_mb_med       rho_med    mem_med\n",
      "count  18.000000  1.800000e+01  18.000000\n",
      "mean    5.115904  1.433704e+09  63.408871\n",
      "std     0.495433  1.026481e+08   6.259155\n",
      "min     4.345289  1.242386e+09  53.336022\n",
      "25%     4.826675  1.376243e+09  59.476664\n",
      "50%     5.092712  1.424364e+09  63.163479\n",
      "75%     5.272682  1.480332e+09  66.371505\n",
      "max     6.166774  1.644036e+09  77.133770\n"
     ]
    }
   ],
   "source": [
    "targets = [\n",
    "    (\"clustered\", \"light\"),\n",
    "    (\"clustered\", \"moderate\"),\n",
    "    (\"clustered\", \"heavy\"),\n",
    "]\n",
    "\n",
    "for topo, scen in targets:\n",
    "    print(f\"\\n=== {ep} / {topo} / {scen} ===\")\n",
    "    prof = env_configs[ep][topo][scen][\"agent_profiles\"]\n",
    "\n",
    "    print(\"\\nlambda stats:\")\n",
    "    print(prof[[\"lambda_mean\", \"lambda_var\"]].describe())\n",
    "\n",
    "    print(\"\\nP(task_type) stats:\")\n",
    "    cols_p = [f\"P_{t}\" for t in [\"deadline_hard\",\"latency_sensitive\",\n",
    "                                  \"compute_intensive\",\"data_intensive\",\"general\"]]\n",
    "    print(prof[cols_p].describe())\n",
    "\n",
    "    print(\"\\nmedian task resource stats:\")\n",
    "    print(prof[[\"b_mb_med\",\"rho_med\",\"mem_med\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. metrics similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== clustered / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "\n",
      "=== clustered / moderate ===\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "\n",
      "=== clustered / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n",
      "\n",
      "=== full_mesh / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "\n",
      "=== full_mesh / moderate ===\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "\n",
      "=== full_mesh / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n",
      "\n",
      "=== sparse_ring / light ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  91.7149      0.1643             3.8306          1.7772  0.2000\n",
      "1  3  49.9723      0.1095             9.5603          1.1129  0.3769\n",
      "2  4  37.5562      0.1389             9.4581          1.0263  0.6076\n",
      "\n",
      "=== sparse_ring / moderate ===\n",
      "   K   inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  114.2401      0.0701             1.8846          2.7151 -0.1928\n",
      "1  3   93.4918      0.0686             2.7439          2.0089  0.0584\n",
      "2  4   67.7316      0.1483             4.1315          1.3760  0.8000\n",
      "\n",
      "=== sparse_ring / heavy ===\n",
      "   K  inertia  silhouette  calinski_harabasz  davies_bouldin   score\n",
      "0  2  97.7594      0.2195             4.7061          1.2290  0.6000\n",
      "1  3  84.0406      0.1519             3.7904          1.0165  0.0000\n",
      "2  4  66.5900      0.1526             4.1995          1.2224 -0.0105\n"
     ]
    }
   ],
   "source": [
    "for topo in [\"clustered\", \"full_mesh\", \"sparse_ring\"]:\n",
    "    for scen in [\"light\", \"moderate\", \"heavy\"]:\n",
    "        sel = K_selection[\"ep_000\"][topo][scen]\n",
    "        dfm = sel[\"metrics_df\"]\n",
    "        print(f\"\\n=== {topo} / {scen} ===\")\n",
    "        print(dfm[[\"K\",\"inertia\",\"silhouette\",\n",
    "                   \"calinski_harabasz\",\"davies_bouldin\",\"score\"]].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.3. Implementing K-Means Clustering </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the optimal number of clusters (K_opt), we use the K-Means algorithm for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_3_run_final_kmeans_for_all_envs(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "\n",
    "    clustering_results = {}\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        clustering_results[ep_name] = {}\n",
    "\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            clustering_results[ep_name][topo_name] = {}\n",
    "\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                k_sel = clust.get(\"k_selection\", None)\n",
    "\n",
    "                # sanity check\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.3/skip] {ep_name}/{topo_name}/{scen_name}: no feature matrix.\")\n",
    "                    continue\n",
    "\n",
    "                if k_sel is None or \"best_K\" not in k_sel:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.3/skip] {ep_name}/{topo_name}/{scen_name}: no K chosen.\")\n",
    "                    continue\n",
    "\n",
    "                X = feats[\"X\"]\n",
    "                agent_ids = feats[\"agent_ids\"]\n",
    "                best_K = int(k_sel[\"best_K\"])\n",
    "\n",
    "                if best_K <= 1 or best_K > X.shape[0]:\n",
    "                    if verbose:\n",
    "                        print(f\"[4.3/skip] invalid best_K={best_K} for {ep_name}/{topo_name}/{scen_name}.\")\n",
    "                    continue\n",
    "\n",
    "                # Final K-Means fit\n",
    "                km = KMeans(\n",
    "                    n_clusters=best_K,\n",
    "                    random_state=random_state,\n",
    "                    n_init=\"auto\"\n",
    "                )\n",
    "                labels = km.fit_predict(X)\n",
    "                centers = km.cluster_centers_\n",
    "\n",
    "                # Store results\n",
    "                env_cfg[\"clustering\"][\"final\"] = {\n",
    "                    \"K\": best_K,\n",
    "                    \"labels\": labels,\n",
    "                    \"centers\": centers,\n",
    "                    \"agent_ids\": agent_ids,\n",
    "                }\n",
    "\n",
    "                clustering_results[ep_name][topo_name][scen_name] = {\n",
    "                    \"K\": best_K,\n",
    "                    \"labels\": labels,\n",
    "                    \"centers\": centers,\n",
    "                    \"agent_ids\": agent_ids,\n",
    "                }\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[4.3] {ep_name}/{topo_name}/{scen_name}:\")\n",
    "                    print(f\"      best_K = {best_K}\")\n",
    "                    print(f\"      labels distribution:\", np.bincount(labels))\n",
    "                    print(f\"      centers shape:\", centers.shape)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "    return clustering_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3] ep_000/clustered/heavy:\n",
      "      best_K = 2\n",
      "      labels distribution: [ 3 15]\n",
      "      centers shape: (2, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/clustered/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [ 4 11  1  2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/clustered/moderate:\n",
      "      best_K = 4\n",
      "      labels distribution: [4 8 3 3]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/full_mesh/heavy:\n",
      "      best_K = 2\n",
      "      labels distribution: [ 3 15]\n",
      "      centers shape: (2, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/full_mesh/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [ 4 11  1  2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/full_mesh/moderate:\n",
      "      best_K = 4\n",
      "      labels distribution: [4 8 3 3]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/sparse_ring/heavy:\n",
      "      best_K = 2\n",
      "      labels distribution: [ 3 15]\n",
      "      centers shape: (2, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/sparse_ring/light:\n",
      "      best_K = 4\n",
      "      labels distribution: [ 4 11  1  2]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "[4.3] ep_000/sparse_ring/moderate:\n",
      "      best_K = 4\n",
      "      labels distribution: [4 8 3 3]\n",
      "      centers shape: (4, 14)\n",
      "--------------------------------------------------\n",
      "\n",
      "=== STEP 4.3 EXAMPLE: ep_000 / clustered / heavy ===\n",
      "K: 2\n",
      "Label counts: [ 3 15]\n",
      "Centers shape: (2, 14)\n"
     ]
    }
   ],
   "source": [
    "clustering_final = step4_3_run_final_kmeans_for_all_envs(\n",
    "    env_configs,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example inspection\n",
    "print(\"\\n=== STEP 4.3 EXAMPLE: ep_000 / clustered / heavy ===\")\n",
    "ex = clustering_final[\"ep_000\"][\"clustered\"][\"heavy\"]\n",
    "print(\"K:\", ex[\"K\"])\n",
    "print(\"Label counts:\", np.bincount(ex[\"labels\"]))\n",
    "print(\"Centers shape:\", ex[\"centers\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization — PCA: Display clusters in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_3_plot_clusters_pca(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    out_root: str = \"./artifacts/clustering\",\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each environment (ep/topology/scenario), take:\n",
    "        - X (scaled feature matrix)\n",
    "        - labels (final K-Means labels)\n",
    "    Project X to 2D via PCA and save scatter plot.\n",
    "\n",
    "    Output saved as:\n",
    "        <out_root>/<ep>/<topology>/<scenario>/cluster_plot_pca.png\n",
    "    \"\"\"\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                # Must have clustering results\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                final = clust.get(\"final\", None)\n",
    "\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    continue\n",
    "                if final is None or \"labels\" not in final:\n",
    "                    if verbose:\n",
    "                        print(f\"[PCA/skip] {ep_name}/{topo_name}/{scen_name}: no final KMeans labels.\")\n",
    "                    continue\n",
    "\n",
    "                X = feats[\"X\"]\n",
    "                labels = final[\"labels\"]\n",
    "                K = final[\"K\"]\n",
    "\n",
    "                n_agents = X.shape[0]\n",
    "                # PCA requires n_samples >= n_components (here 2)\n",
    "                if n_agents < 2:\n",
    "                    if verbose:\n",
    "                        print(f\"[PCA/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"n_agents={n_agents} < 2, cannot run PCA.\")\n",
    "                    continue\n",
    "\n",
    "                # PCA projection\n",
    "                pca = PCA(n_components=2, random_state=42)\n",
    "                X_2d = pca.fit_transform(X)\n",
    "\n",
    "                # Plot\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_path = os.path.join(out_dir, \"cluster_plot_pca.png\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "\n",
    "                for cl in np.unique(labels):\n",
    "                    mask = (labels == cl)\n",
    "                    plt.scatter(\n",
    "                        X_2d[mask, 0],\n",
    "                        X_2d[mask, 1],\n",
    "                        label=f\"Cluster {cl}\",\n",
    "                        alpha=0.75,\n",
    "                        s=50\n",
    "                    )\n",
    "\n",
    "                plt.title(f\"PCA Clusters: {ep_name} / {topo_name} / {scen_name}  (K={K})\")\n",
    "                plt.xlabel(\"PCA Component 1\")\n",
    "                plt.ylabel(\"PCA Component 2\")\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_path, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[PCA] Saved PCA cluster plot → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\moderate\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\heavy\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\light\\cluster_plot_pca.png\n",
      "[PCA] Saved PCA cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\moderate\\cluster_plot_pca.png\n"
     ]
    }
   ],
   "source": [
    "step4_3_plot_clusters_pca(env_configs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization — spacet-SNE: Display clusters in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_3_plot_clusters_tsne(\n",
    "    env_configs: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    out_root: str = \"./artifacts/clustering\",\n",
    "    perplexity: int = 5,\n",
    "    early_exaggeration: int = 12,\n",
    "    n_iter: int = 1500,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw 2D t-SNE visualization for final KMeans clusters.\n",
    "    \n",
    "    Saves figure as:\n",
    "        <out_root>/<ep>/<topology>/<scenario>/cluster_plot_tsne.png\n",
    "    \"\"\"\n",
    "\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "\n",
    "                clust = env_cfg.get(\"clustering\", {})\n",
    "                feats = clust.get(\"features\", None)\n",
    "                final = clust.get(\"final\", None)\n",
    "\n",
    "                if feats is None or \"X\" not in feats:\n",
    "                    continue\n",
    "                if final is None or \"labels\" not in final:\n",
    "                    if verbose:\n",
    "                        print(f\"[t-SNE/skip] {ep_name}/{topo_name}/{scen_name}: no cluster labels.\")\n",
    "                    continue\n",
    "\n",
    "                X = feats[\"X\"]\n",
    "                labels = final[\"labels\"]\n",
    "                K = final[\"K\"]\n",
    "\n",
    "                n_agents = X.shape[0]\n",
    "                if n_agents <= perplexity:\n",
    "                    if verbose:\n",
    "                        print(f\"[t-SNE/skip] {ep_name}/{topo_name}/{scen_name}: \"\n",
    "                              f\"n_agents={n_agents} <= perplexity={perplexity}\")\n",
    "                    continue\n",
    "\n",
    "                # Run t-SNE\n",
    "                tsne = TSNE(\n",
    "                    n_components=2,\n",
    "                    perplexity=perplexity,\n",
    "                    early_exaggeration=early_exaggeration,\n",
    "                    n_iter=n_iter,\n",
    "                    init='pca',\n",
    "                    learning_rate='auto',\n",
    "                    random_state=42,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "\n",
    "                X_2d = tsne.fit_transform(X)\n",
    "\n",
    "                # Plot\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_path = os.path.join(out_dir, \"cluster_plot_tsne.png\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "\n",
    "                for cl in np.unique(labels):\n",
    "                    mask = (labels == cl)\n",
    "                    plt.scatter(\n",
    "                        X_2d[mask, 0],\n",
    "                        X_2d[mask, 1],\n",
    "                        label=f\"Cluster {cl}\",\n",
    "                        s=50,\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "\n",
    "                plt.title(f\"t-SNE Clusters: {ep_name} / {topo_name} / {scen_name}  (K={K})\")\n",
    "                plt.xlabel(\"t-SNE Dim 1\")\n",
    "                plt.ylabel(\"t-SNE Dim 2\")\n",
    "                plt.grid(True)\n",
    "                plt.legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_path, dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[t-SNE] Saved t-SNE cluster plot → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\full_mesh\\moderate\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\heavy\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\light\\cluster_plot_tsne.png\n",
      "[t-SNE] Saved t-SNE cluster plot → ./artifacts/clustering\\ep_000\\sparse_ring\\moderate\\cluster_plot_tsne.png\n"
     ]
    }
   ],
   "source": [
    "step4_3_plot_clusters_tsne(env_configs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.4. Cluster Interpretation & Profiling </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation, Summaries, and Cluster Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_profiles_for_env(\n",
    "    ep_name: str,\n",
    "    topo_name: str,\n",
    "    scen_name: str,\n",
    "    env_cfg: Dict[str, Any],\n",
    "    out_root: str = \"./artifacts/clustering\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build cluster representative profiles using:\n",
    "      - labels from Step 4.3 (env_cfg['clustering']['final'])\n",
    "      - scaled centers from Step 4.3\n",
    "      - inverse-transformed centers using scaler from Step 4.1\n",
    "      - agent_profiles from Step 3\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract dependencies\n",
    "    clust = env_cfg.get(\"clustering\", {})\n",
    "    feats = clust.get(\"features\", None)\n",
    "    final = clust.get(\"final\", None)   # This must exist (Step 4.3)\n",
    "\n",
    "    if feats is None or \"X\" not in feats:\n",
    "        raise ValueError(f\"[4.4] Missing features for {ep_name}/{topo_name}/{scen_name}\")\n",
    "\n",
    "    if final is None or \"labels\" not in final or \"centers\" not in final:\n",
    "        raise ValueError(\n",
    "            f\"[4.4] Missing final KMeans results for {ep_name}/{topo_name}/{scen_name}. \"\n",
    "            f\"Did you forget to run Step 4.3?\"\n",
    "        )\n",
    "\n",
    "    best_K         = int(final[\"K\"])\n",
    "    labels         = np.asarray(final[\"labels\"], dtype=int)\n",
    "    centers_scaled = np.asarray(final[\"centers\"], dtype=float)\n",
    "\n",
    "    agent_ids    = feats[\"agent_ids\"]\n",
    "    feature_cols = feats[\"feature_cols\"]\n",
    "\n",
    "    prof = env_cfg[\"agent_profiles\"].copy()\n",
    "\n",
    "    # 2) Build assignment table (agent_id → cluster_id)\n",
    "    assign_df = pd.DataFrame({\n",
    "        \"agent_id\": agent_ids,\n",
    "        \"cluster_id\": labels\n",
    "    })\n",
    "\n",
    "    prof = prof.merge(assign_df, on=\"agent_id\", how=\"left\")\n",
    "\n",
    "    # === NEW: Inject cluster_id into tasks DataFrame ===\n",
    "    tasks_df = env_cfg.get(\"tasks\", None)\n",
    "    if tasks_df is not None and \"agent_id\" in tasks_df.columns:\n",
    "        tasks_df = tasks_df.merge(assign_df, on=\"agent_id\", how=\"left\")\n",
    "        # Convert to int (and fill agents with no tasks with -1)\n",
    "        tasks_df[\"cluster_id\"] = tasks_df[\"cluster_id\"].fillna(-1).astype(int)\n",
    "        # write back\n",
    "        env_cfg[\"tasks\"] = tasks_df\n",
    "        print(f\"[4.4] Added 'cluster_id' to tasks for {ep_name}/{topo_name}/{scen_name} \"\n",
    "              f\"(rows={len(tasks_df)})\")\n",
    "\n",
    "    # Debug: Check if cluster_id is in tasks_df and cluster_summary\n",
    "    print(f\"Debug: Tasks DataFrame for {ep_name}/{topo_name}/{scen_name} includes 'cluster_id':\")\n",
    "    print(tasks_df.head())\n",
    "\n",
    "    # 3) Cluster-level summary (numeric columns only, excluding cluster_id from aggregation)\n",
    "    numeric_cols = prof.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Separate group key from aggregated columns\n",
    "    agg_cols = [c for c in numeric_cols if c != \"cluster_id\"]\n",
    "\n",
    "    cluster_summary = (\n",
    "        prof[[\"cluster_id\"] + agg_cols]\n",
    "        .groupby(\"cluster_id\", as_index=False)\n",
    "        .mean()\n",
    "        .sort_values(\"cluster_id\")\n",
    "    )\n",
    "\n",
    "    cluster_sizes = (\n",
    "        prof.groupby(\"cluster_id\")[\"agent_id\"]\n",
    "        .count()\n",
    "        .rename(\"n_agents_cluster\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    cluster_summary = cluster_summary.merge(cluster_sizes, on=\"cluster_id\", how=\"left\")\n",
    "\n",
    "    # Debug: Check if cluster_summary has 'cluster_id'\n",
    "    print(f\"Debug: Cluster summary for {ep_name}/{topo_name}/{scen_name} includes 'cluster_id':\")\n",
    "    print(cluster_summary.head())\n",
    "\n",
    "    # 4) Decode centroids back to original scale (no need for scaler now)\n",
    "    centers_original = centers_scaled.copy()\n",
    "\n",
    "    centroids_scaled_df = pd.DataFrame(centers_scaled, columns=feature_cols)\n",
    "    centroids_scaled_df.insert(0, \"cluster_id\", np.arange(best_K))\n",
    "\n",
    "    centroids_original_df = pd.DataFrame(centers_original, columns=feature_cols)\n",
    "    centroids_original_df.insert(0, \"cluster_id\", np.arange(best_K))\n",
    "\n",
    "    # 5) Save to disk\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    assign_path    = os.path.join(out_dir, \"cluster_assignments.csv\")\n",
    "    summary_path   = os.path.join(out_dir, \"cluster_summary.csv\")\n",
    "    cent_sc_path   = os.path.join(out_dir, \"centroids_scaled.csv\")\n",
    "    cent_orig_path = os.path.join(out_dir, \"centroids_original.csv\")\n",
    "\n",
    "    assign_df.to_csv(assign_path, index=False)\n",
    "    cluster_summary.to_csv(summary_path, index=False)\n",
    "    centroids_scaled_df.to_csv(cent_sc_path, index=False)\n",
    "    centroids_original_df.to_csv(cent_orig_path, index=False)\n",
    "\n",
    "    print(f\"[4.4] {ep_name}/{topo_name}/{scen_name} → cluster profiles built.\")\n",
    "    print(cluster_sizes.set_index(\"cluster_id\")[\"n_agents_cluster\"])\n",
    "\n",
    "    # 6) Attach final results to env_cfg\n",
    "    env_cfg[\"clustering\"][\"profiles\"] = {\n",
    "        \"K\": best_K,\n",
    "        \"cluster_assignments\": assign_df,\n",
    "        \"cluster_summary\": cluster_summary,\n",
    "        \"centroids_scaled_df\": centroids_scaled_df,\n",
    "        \"centroids_original_df\": centroids_original_df,\n",
    "        \"centroids_scaled\": centers_scaled,\n",
    "        \"centroids_original\": centers_original,\n",
    "    }\n",
    "\n",
    "    return env_cfg[\"clustering\"][\"profiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_cluster_profiles(env_configs):\n",
    "    out = {}\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        out[ep_name] = {}\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            out[ep_name][topo_name] = {}\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                try:\n",
    "                    prof = build_cluster_profiles_for_env(\n",
    "                        ep_name, topo_name, scen_name, env_cfg\n",
    "                    )\n",
    "                    out[ep_name][topo_name][scen_name] = prof\n",
    "                except Exception as e:\n",
    "                    print(f\"[4.4/warn] skipping {ep_name}/{topo_name}/{scen_name}: {e}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/clustered/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/clustered/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/clustered/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/clustered/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/full_mesh/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/full_mesh/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/full_mesh/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/full_mesh/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/heavy (rows=841)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/heavy includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    heavy           0        0         0               0             0.0   \n",
      "1    heavy           0        1         1               0             0.0   \n",
      "2    heavy           0        2         4               0             0.0   \n",
      "3    heavy           0        3         7               0             0.0   \n",
      "4    heavy           0        4        10               0             0.0   \n",
      "\n",
      "        b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...   task_subtype  \\\n",
      "0   7.202096    9.727147e+08  7.005585e+09  66.611010  ...  deadline_hard   \n",
      "1   5.479984    1.314973e+09  7.206031e+09  77.928800  ...  deadline_hard   \n",
      "2   8.421977    2.500222e+09  2.105681e+10  72.966446  ...  deadline_hard   \n",
      "3   6.324986    1.779582e+09  1.125583e+10  56.492900  ...  deadline_hard   \n",
      "4  11.473269    1.087572e+09  1.247800e+10  73.389854  ...  deadline_hard   \n",
      "\n",
      "                   type_reason  \\\n",
      "0  hard deadline (tight slots)   \n",
      "1  hard deadline (tight slots)   \n",
      "2  hard deadline (tight slots)   \n",
      "3  hard deadline (tight slots)   \n",
      "4  hard deadline (tight slots)   \n",
      "\n",
      "                                         multi_flags     final_flag  \\\n",
      "0                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "1                      [deadline_hard, memory_heavy]  deadline_hard   \n",
      "2  [deadline_hard, compute_heavy, io_heavy, split...  deadline_hard   \n",
      "3                     [deadline_hard, compute_heavy]  deadline_hard   \n",
      "4                          [deadline_hard, io_heavy]  deadline_hard   \n",
      "\n",
      "   is_general  is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0       False              True                False                 False   \n",
      "1       False              True                False                 False   \n",
      "2       False              True                False                 False   \n",
      "3       False              True                False                 False   \n",
      "4       False              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          1  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/heavy includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/heavy → cluster profiles built.\n",
      "cluster_id\n",
      "0     3\n",
      "1    15\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/light (rows=70)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/light includes 'cluster_id':\n",
      "  scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0    light           0        0         1               0             0.0   \n",
      "1    light           0        1        17               0             0.0   \n",
      "2    light           0        2        17               7             7.0   \n",
      "3    light           0        3         3               9             9.0   \n",
      "4    light           0        4        15              13            13.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles     mem_mb  ...  \\\n",
      "0  2.484967    5.525261e+08  1.373009e+09  62.412148  ...   \n",
      "1  2.509962    1.082130e+09  2.716106e+09  74.606000  ...   \n",
      "2  1.509317    1.757732e+09  2.652974e+09  30.173971  ...   \n",
      "3  2.114311    4.582521e+08  9.688874e+08  52.914295  ...   \n",
      "4  1.707707    6.574788e+08  1.122781e+09  95.441150  ...   \n",
      "\n",
      "              task_subtype                  type_reason  \\\n",
      "0            deadline_hard  hard deadline (tight slots)   \n",
      "1  compute_or_memory_heavy   high compute/memory demand   \n",
      "2  compute_or_memory_heavy   high compute/memory demand   \n",
      "3                  general       no dominant constraint   \n",
      "4  compute_or_memory_heavy   high compute/memory demand   \n",
      "\n",
      "                   multi_flags         final_flag  is_general  \\\n",
      "0  [deadline_hard, splittable]      deadline_hard       False   \n",
      "1     [memory_heavy, io_heavy]  compute_intensive       False   \n",
      "2  [compute_heavy, splittable]  compute_intensive       False   \n",
      "3                           []            general        True   \n",
      "4               [memory_heavy]  compute_intensive       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0              True                False                 False   \n",
      "1             False                False                  True   \n",
      "2             False                False                  True   \n",
      "3             False                False                 False   \n",
      "4             False                False                  True   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0             False          0  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          1  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/light includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   5.750000  1.409116e+09  1.409116e+09  4154.655081   \n",
      "1           1  10.363636  1.624885e+09  1.624885e+09  6070.368989   \n",
      "2           2   0.000000  9.214683e+08  9.214683e+08  5152.146294   \n",
      "3           3   8.000000  2.024295e+09  2.024295e+09  6808.176989   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0          1.0         0.0        5.500000       5.500000           1.0  ...   \n",
      "1          1.0         0.0        4.090909       4.090909           1.0  ...   \n",
      "2          0.0         0.0        0.000000       0.000000           0.0  ...   \n",
      "3          1.0         0.0        1.500000       1.500000           1.0  ...   \n",
      "\n",
      "      mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0   66.611208    0.367262    0.28750                  0.0         0.367262   \n",
      "1   64.030346    0.083838    0.20101                  0.0         0.083838   \n",
      "2    0.000000    0.000000    0.00000                  0.0         0.000000   \n",
      "3  114.061982    0.250000    0.00000                  0.0         0.250000   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.160714             0.184524          0.267262   5.750000   \n",
      "1          0.129798             0.585354          0.308081  10.363636   \n",
      "2          0.000000             0.000000          0.000000   0.000000   \n",
      "3          0.000000             0.750000          0.250000   8.000000   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                11  \n",
      "2                 1  \n",
      "3                 2  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/light → cluster profiles built.\n",
      "cluster_id\n",
      "0     4\n",
      "1    11\n",
      "2     1\n",
      "3     2\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "[4.4] Added 'cluster_id' to tasks for ep_000/sparse_ring/moderate (rows=261)\n",
      "Debug: Tasks DataFrame for ep_000/sparse_ring/moderate includes 'cluster_id':\n",
      "   scenario  episode_id  task_id  agent_id  t_arrival_slot  t_arrival_time  \\\n",
      "0  moderate           0        0         3               1             1.0   \n",
      "1  moderate           0        1        12               1             1.0   \n",
      "2  moderate           0        2        13               1             1.0   \n",
      "3  moderate           0        3        13               1             1.0   \n",
      "4  moderate           0        4         5               3             3.0   \n",
      "\n",
      "       b_mb  rho_cyc_per_mb      c_cycles      mem_mb  ...  \\\n",
      "0  4.463301    1.368071e+09  6.106115e+09   59.895400  ...   \n",
      "1  2.032145    1.310597e+09  2.663323e+09   39.163837  ...   \n",
      "2  2.471461    1.423734e+09  3.518704e+09   71.265870  ...   \n",
      "3  2.097559    7.160129e+08  1.501879e+09   73.560420  ...   \n",
      "4  2.683642    9.887779e+08  2.653526e+09  114.356410  ...   \n",
      "\n",
      "            task_subtype                   type_reason  \\\n",
      "0  large_input_bandwidth  large data volume / IO heavy   \n",
      "1                general        no dominant constraint   \n",
      "2                general        no dominant constraint   \n",
      "3                general        no dominant constraint   \n",
      "4          deadline_hard   hard deadline (tight slots)   \n",
      "\n",
      "                     multi_flags      final_flag  is_general  \\\n",
      "0                     [io_heavy]  data_intensive       False   \n",
      "1                   [splittable]         general        True   \n",
      "2                             []         general        True   \n",
      "3                   [splittable]         general        True   \n",
      "4  [deadline_hard, memory_heavy]   deadline_hard       False   \n",
      "\n",
      "   is_deadline_hard is_latency_sensitive  is_compute_intensive  \\\n",
      "0             False                False                 False   \n",
      "1             False                False                 False   \n",
      "2             False                False                 False   \n",
      "3             False                False                 False   \n",
      "4              True                False                 False   \n",
      "\n",
      "  is_data_intensive cluster_id  \n",
      "0              True          2  \n",
      "1             False          1  \n",
      "2             False          1  \n",
      "3             False          1  \n",
      "4             False          3  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Debug: Cluster summary for ep_000/sparse_ring/moderate includes 'cluster_id':\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0   6.750000  1.597698e+09  1.597698e+09  3894.551248   \n",
      "1           1   8.500000  1.794483e+09  1.794483e+09  4565.237078   \n",
      "2           2   6.666667  1.307978e+09  1.307978e+09  4274.561123   \n",
      "3           3  12.666667  1.209908e+09  1.209908e+09  6454.403261   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.139706    0.126867       15.000000      17.000000           1.0  ...   \n",
      "1     1.027778    0.027778       13.125000      13.500000           1.0  ...   \n",
      "2     1.216374    0.315789       14.666667      17.333333           1.0  ...   \n",
      "3     1.023810    0.023810       10.666667      11.000000           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  69.170107    0.272467   0.273576                  0.0         0.272467   \n",
      "1  63.167430    0.137106   0.287617                  0.0         0.137106   \n",
      "2  56.132347    0.193182   0.261364                  0.0         0.193182   \n",
      "3  59.767801    0.386111   0.122222                  0.0         0.386111   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.078840             0.375117          0.389648   6.750000   \n",
      "1          0.122178             0.453099          0.273837   8.500000   \n",
      "2          0.178030             0.367424          0.253788   6.666667   \n",
      "3          0.055556             0.436111          0.394444  12.666667   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 4  \n",
      "1                 8  \n",
      "2                 3  \n",
      "3                 3  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "[4.4] ep_000/sparse_ring/moderate → cluster profiles built.\n",
      "cluster_id\n",
      "0    4\n",
      "1    8\n",
      "2    3\n",
      "3    3\n",
      "Name: n_agents_cluster, dtype: int64\n",
      "\n",
      "=== EXAMPLE: cluster summary for ep_000 / clustered / heavy ===\n",
      "K = 2\n",
      "\n",
      "Cluster summary (first few cols):\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  \n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  \n"
     ]
    }
   ],
   "source": [
    "# ---- Run Step 4.4 on all environments ----\n",
    "cluster_profiles = build_all_cluster_profiles(env_configs)\n",
    "\n",
    "print(\"\\n=== EXAMPLE: cluster summary for ep_000 / clustered / heavy ===\")\n",
    "ex_ep   = \"ep_000\"\n",
    "ex_topo = \"clustered\"\n",
    "ex_scen = \"heavy\"\n",
    "\n",
    "if (ex_ep in cluster_profiles and\n",
    "    ex_topo in cluster_profiles[ex_ep] and\n",
    "    ex_scen in cluster_profiles[ex_ep][ex_topo]):\n",
    "\n",
    "    ex_prof = cluster_profiles[ex_ep][ex_topo][ex_scen]\n",
    "    print(\"K =\", ex_prof[\"K\"])\n",
    "    print(\"\\nCluster summary (first few cols):\")\n",
    "    print(ex_prof[\"cluster_summary\"].iloc[:, :10])\n",
    "else:\n",
    "    print(\"[warn] Example triple not found in cluster_profiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Structure of env_configs:\n",
    "\n",
    "env_configs[ep][topo][scen][\"clustering\"] = {</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"features\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;# from step 4.1</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"k_selection\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.2</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"final\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.3</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\"profiles\": {...},\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;# from step 4.4</br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features', 'k_selection', 'final', 'profiles'])\n",
      "   cluster_id   agent_id       f_local  f_local_slot      m_local  \\\n",
      "0           0  10.333333  1.158169e+09  1.158169e+09  6727.567646   \n",
      "1           1   8.133333  1.499515e+09  1.499515e+09  5017.552016   \n",
      "\n",
      "   lambda_mean  lambda_var  slots_observed  n_tasks_agent  TaskDist_sum  ...  \\\n",
      "0     1.529870    0.638636       53.000000      81.000000           1.0  ...   \n",
      "1     1.229003    0.297275       31.866667      39.866667           1.0  ...   \n",
      "\n",
      "     mem_med  hard_share  P_general  P_latency_sensitive  P_deadline_hard  \\\n",
      "0  66.733418    0.376328   0.155571                  0.0         0.376328   \n",
      "1  62.743961    0.322508   0.189935                  0.0         0.322508   \n",
      "\n",
      "   P_data_intensive  P_compute_intensive  non_atomic_share     mec_id  \\\n",
      "0          0.100495             0.367606          0.473623  10.333333   \n",
      "1          0.114373             0.373184          0.439802   8.133333   \n",
      "\n",
      "   n_agents_cluster  \n",
      "0                 3  \n",
      "1                15  \n",
      "\n",
      "[2 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "test = env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"]\n",
    "print(test.keys())\n",
    "\n",
    "print(env_configs[\"ep_000\"][\"clustered\"][\"heavy\"][\"clustering\"][\"profiles\"][\"cluster_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_profile_heatmap(env_cfg,\n",
    "                                 ep_name: str,\n",
    "                                 topo_name: str,\n",
    "                                 scen_name: str,\n",
    "                                 out_root=\"./artifacts/clustering\"):\n",
    "    \"\"\"\n",
    "    Draw heatmap of cluster profile means for a given env_cfg.\n",
    "\n",
    "    Uses:\n",
    "        env_cfg[\"clustering\"][\"profiles\"][\"cluster_summary\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract cluster profiles\n",
    "    clust = env_cfg.get(\"clustering\", {})\n",
    "    profs = clust.get(\"profiles\", None)\n",
    "\n",
    "    if profs is None or \"cluster_summary\" not in profs:\n",
    "        print(f\"[heatmap/skip] No cluster profiles for {ep_name}/{topo_name}/{scen_name}\")\n",
    "        return None\n",
    "\n",
    "    cluster_summary = profs[\"cluster_summary\"].copy()\n",
    "    K = profs[\"K\"]\n",
    "\n",
    "    # remove non-feature columns if present\n",
    "    drop_cols = [\"cluster_id\", \"n_agents_cluster\"]\n",
    "    feature_cols = [c for c in cluster_summary.columns if c not in drop_cols]\n",
    "\n",
    "    df = cluster_summary[feature_cols].copy()\n",
    "\n",
    "    # 2) Normalize per-column\n",
    "    df_norm = (df - df.min()) / (df.max() - df.min() + 1e-9)\n",
    "\n",
    "    # 3) Output directory\n",
    "    out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"cluster_profile_heatmap.png\")\n",
    "\n",
    "    # 4) Plot heatmap\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(\n",
    "        df_norm,\n",
    "        annot=False,\n",
    "        cmap=\"viridis\",\n",
    "        xticklabels=df_norm.columns,\n",
    "        yticklabels=[f\"Cluster {i}\" for i in range(K)]\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Cluster Profile Heatmap: {ep_name}/{topo_name}/{scen_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[heatmap] Saved → {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\clustered\\moderate\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\full_mesh\\moderate\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\heavy\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\light\\cluster_profile_heatmap.png\n",
      "[heatmap] Saved → ./artifacts/clustering\\ep_000\\sparse_ring\\moderate\\cluster_profile_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "for ep in env_configs:\n",
    "    for topo in env_configs[ep]:\n",
    "        for scen in env_configs[ep][topo]:\n",
    "            plot_cluster_profile_heatmap(\n",
    "                env_configs[ep][topo][scen],\n",
    "                ep, topo, scen\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\clustered\\moderate\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\full_mesh\\moderate\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\heavy\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\light\\clustering.csv\n",
      "[save] agent_profiles saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\agent_profiles.csv\n",
      "[save] tasks saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\tasks.csv\n",
      "[save] clustering saved to ./artifacts/env_configs\\ep_000\\sparse_ring\\moderate\\clustering.csv\n",
      "[save] All data has been saved in the folder ./artifacts/env_configs.\n"
     ]
    }
   ],
   "source": [
    "def save_env_configs_to_csv(env_configs, out_root=\"./artifacts/env_configs\"):\n",
    "    \"\"\"\n",
    "    Save the env_configs data to CSV format in the appropriate folders based on ep, topology, and scenario.\n",
    "    \"\"\"\n",
    "    # Loop through each ep in env_configs\n",
    "    for ep_name, by_topo in env_configs.items():\n",
    "        for topo_name, by_scen in by_topo.items():\n",
    "            for scen_name, env_cfg in by_scen.items():\n",
    "                # Create the folders for each ep, topology, and scenario\n",
    "                out_dir = os.path.join(out_root, ep_name, topo_name, scen_name)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                \n",
    "                # Save the existing DataFrames in env_cfg to CSV files\n",
    "                if \"agent_profiles\" in env_cfg:\n",
    "                    agent_profiles_path = os.path.join(out_dir, \"agent_profiles.csv\")\n",
    "                    env_cfg[\"agent_profiles\"].to_csv(agent_profiles_path, index=False)\n",
    "                    print(f\"[save] agent_profiles saved to {agent_profiles_path}\")\n",
    "\n",
    "                if \"tasks\" in env_cfg:\n",
    "                    tasks_path = os.path.join(out_dir, \"tasks.csv\")\n",
    "                    env_cfg[\"tasks\"].to_csv(tasks_path, index=False)\n",
    "                    print(f\"[save] tasks saved to {tasks_path}\")\n",
    "\n",
    "                if \"clustering\" in env_cfg:\n",
    "                    clustering_path = os.path.join(out_dir, \"clustering.csv\")\n",
    "                    # If clustering data needs to be saved\n",
    "                    pd.DataFrame(env_cfg[\"clustering\"]).to_csv(clustering_path, index=False)\n",
    "                    print(f\"[save] clustering saved to {clustering_path}\")\n",
    "\n",
    "                # If there are other data to be saved, you can add them here\n",
    "                # For example, if \"tasks_df\" exists, you can save it as well\n",
    "\n",
    "    print(f\"[save] All data has been saved in the folder {out_root}.\")\n",
    "\n",
    "# Using the function\n",
    "save_env_configs_to_csv(env_configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 5: MDP Environment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.1 — Initialization & State Builder\n",
    "\n",
    "Step 5.2 — Action Handling\n",
    "\n",
    "Step 5.3 — Queue & Execution Update\n",
    "\n",
    "Step 5.4 — Reward Calculation\n",
    "\n",
    "Step 5.5 — Done / Episode Horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2 - Action Semantics (Contextual Bandit)\n",
    "def select_algo_for_cluster(cluster_profile):\n",
    "    \"\"\"\n",
    "    Select an RL algorithm for each cluster based on its profile.\n",
    "    The profile can include information like lambda_mean, task types, etc.\n",
    "    \"\"\"\n",
    "    if cluster_profile[\"lambda_mean\"] > 0.5 and cluster_profile[\"P_compute_intensive\"] > 0.5:\n",
    "        return \"PPO\"\n",
    "    elif cluster_profile[\"lambda_mean\"] < 0.5 and cluster_profile[\"P_general\"] > 0.5:\n",
    "        return \"DQN\"\n",
    "    else:\n",
    "        return \"A2C\"\n",
    "\n",
    "\n",
    "# Step 5.1 - State Construction\n",
    "# Example of scaling a feature array\n",
    "def normalize_feature_array(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features.reshape(-1, 1))  # Assuming features are in a 1D array\n",
    "\n",
    "class OffloadingEnv:\n",
    "    def __init__(self, env_cfg):\n",
    "        self.env_cfg = env_cfg\n",
    "        self.delta = env_cfg[\"Delta\"]\n",
    "        self.t_slots = env_cfg[\"T_slots\"]\n",
    "        self.agent_profiles = env_cfg[\"agent_profiles\"]\n",
    "        self.alpha = 1.0  # Weight for delay penalty\n",
    "        self.beta = 1.0   # Weight for drop penalty\n",
    "        \n",
    "        # Initialize the queues for each agent\n",
    "        self.queues = {agent_id: {'local': [], 'mec': [], 'cloud': []} for agent_id in self.agent_profiles[\"agent_id\"]}\n",
    "        \n",
    "        # Initialize other environment variables like CPU/memory\n",
    "        self.cpu_capacity = env_cfg[\"private_cpu\"]\n",
    "        self.cloud_capacity = env_cfg[\"cloud_cpu\"]\n",
    "        \n",
    "        # Initialize the algorithm mapping for each cluster\n",
    "        self.algo_map = self.assign_algorithms_to_clusters()\n",
    "\n",
    "    def assign_algorithms_to_clusters(self):\n",
    "        \"\"\"\n",
    "        Assign RL algorithms to clusters based on their profiles.\n",
    "        \"\"\"\n",
    "        algo_map = {}\n",
    "        for cluster_id, cluster_profile in self.env_cfg[\"clustering\"][\"profiles\"].items():\n",
    "            algo_type = select_algo_for_cluster(cluster_profile)\n",
    "            algo_map[cluster_id] = {\"type\": algo_type, \"agent\": None}\n",
    "        return algo_map\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        for agent_id in self.queues:\n",
    "            self.queues[agent_id] = {'local': [], 'mec': [], 'cloud': []}\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Perform one step in the environment for each agent.\n",
    "        actions: dict of {agent_id: action}\n",
    "        \"\"\"\n",
    "        # Update the queues based on actions\n",
    "        for agent_id, action in actions.items():\n",
    "            if action == 0:  # LOCAL\n",
    "                # Add task to local queue\n",
    "                self.queues[agent_id]['local'].append(self._get_task(agent_id))\n",
    "            elif action == 1:  # MEC\n",
    "                # Add task to MEC queue\n",
    "                self.queues[agent_id]['mec'].append(self._get_task(agent_id))\n",
    "            elif action == 2:  # CLOUD\n",
    "                # Add task to Cloud queue\n",
    "                self.queues[agent_id]['cloud'].append(self._get_task(agent_id))\n",
    "\n",
    "        # Process tasks in queues (FIFO)\n",
    "        self._process_queues()\n",
    "\n",
    "        # Calculate reward for each agent based on task completions, delays, etc.\n",
    "        rewards = self._calculate_rewards()\n",
    "\n",
    "        # Update state\n",
    "        next_state = self._get_state()\n",
    "\n",
    "        # Check if episode is done (based on T_slots)\n",
    "        done = self._check_done()\n",
    "\n",
    "        return next_state, rewards, done, {}\n",
    "  \n",
    "    def _process_queues(self):\n",
    "        \"\"\"Process tasks in the queues based on GPS and FIFO.\"\"\"\n",
    "        total_active_queues = sum([1 for queues in self.queues.values() if any(queue) for queue in queues.values()])\n",
    "        cpu_share = self.cpu_capacity / total_active_queues if total_active_queues > 0 else 0\n",
    "\n",
    "        for agent_id, queues in self.queues.items():\n",
    "            for queue_name, queue in queues.items():\n",
    "                # If there are tasks in the queue, process them\n",
    "                if queue:\n",
    "                    task = queue.pop(0)  # FIFO\n",
    "                    task_processing_time = task[\"c_cycles\"] / cpu_share  # Assuming equal CPU share\n",
    "                    # Process the task (you may want to add logic for processing based on GPS share here)\n",
    "                    self._task_complete(agent_id, task, queue_name)\n",
    "\n",
    "                    # Update the queue state after processing the task\n",
    "                    print(f\"Processed task for agent {agent_id} in {queue_name} queue, estimated time: {task_processing_time:.2f} cycles.\")\n",
    "\n",
    "    def _task_complete(self, agent_id, task, queue_name):\n",
    "        \"\"\"Handle task completion (can be expanded with delay/drop logic).\"\"\"\n",
    "        print(f\"Task completed for agent {agent_id} in {queue_name} queue.\")\n",
    "        # Here you can implement delay or drop logic if required.\n",
    "    \n",
    "    def _calculate_rewards(self):\n",
    "        \"\"\"Calculate reward based on task delays, drop rates, etc.\"\"\"\n",
    "        rewards = {}\n",
    "        for agent_id, queues in self.queues.items():\n",
    "            for queue_name, queue in queues.items():\n",
    "                if queue:  # If there are tasks in the queue\n",
    "                    task = queue[0]  # Take the first task for reward calculation\n",
    "                    task_completion_time = self._calculate_task_time(task)\n",
    "                    delay = self._calculate_delay(task, task_completion_time)\n",
    "                    drop = self._check_task_drop(task)\n",
    "\n",
    "                    # Reward calculation based on delay and drop\n",
    "                    rewards[agent_id] = -self.alpha * delay - self.beta * drop\n",
    "        return rewards\n",
    "    \n",
    "    def _calculate_task_time(self, task):\n",
    "        \"\"\"Calculate the total processing time for a task.\"\"\"\n",
    "        # Here we calculate total time based on task's computation cycles\n",
    "        # For now, assuming constant computation power, you can update based on more detailed logic\n",
    "        processing_time = task[\"c_cycles\"] / self.cpu_capacity  # Example logic\n",
    "        return processing_time\n",
    "\n",
    "    def _calculate_delay(self, task, completion_time):\n",
    "        \"\"\"Calculate delay based on the task's total execution time and its deadline.\"\"\"\n",
    "        # Assuming task has a deadline attribute\n",
    "        if \"deadline_slots\" in task and task[\"deadline_slots\"] > 0:\n",
    "            delay = max(0, completion_time - task[\"deadline_slots\"])  # Time past the deadline\n",
    "            return delay\n",
    "        return 0  # No delay if no deadline\n",
    "    \n",
    "    def _check_task_drop(self, task):\n",
    "        \"\"\"Check if the task is dropped due to exceeding its deadline.\"\"\"\n",
    "        if \"deadline_slots\" in task and task[\"deadline_slots\"] > 0:\n",
    "            if task[\"deadline_slots\"] <= 0:  # If deadline is violated\n",
    "                return 1  # Drop penalty\n",
    "        return 0  # No drop if deadline is not violated\n",
    "\n",
    "    # Update _get_state to normalize relevant features\n",
    "    def _get_state(self):\n",
    "        \"\"\"Return the state for each agent, which includes queue lengths, resources, etc.\"\"\"\n",
    "        state = {}\n",
    "        for agent_id, queues in self.queues.items():\n",
    "            # Retrieve features from tasks and agent profile for state\n",
    "            task_features = self._get_task_features(agent_id)\n",
    "            agent_profile = self.agent_profiles.loc[self.agent_profiles['agent_id'] == agent_id]\n",
    "            cluster_id = agent_profile[\"cluster_id\"].values[0]  # Assuming cluster_id exists in agent_profiles\n",
    "            \n",
    "            # Normalize the relevant task features and agent profile features\n",
    "            task_features[\"b_mb\"] = normalize_feature_array(task_features[\"b_mb\"])\n",
    "            task_features[\"c_cycles\"] = normalize_feature_array(task_features[\"c_cycles\"])\n",
    "            task_features[\"mem_mb\"] = normalize_feature_array(task_features[\"mem_mb\"])\n",
    "            task_features[\"rho_cyc_per_mb\"] = normalize_feature_array(task_features[\"rho_cyc_per_mb\"])\n",
    "\n",
    "            state[agent_id] = {\n",
    "                \"local_queue_length\": len(queues['local']),\n",
    "                \"mec_queue_length\": len(queues['mec']),\n",
    "                \"cloud_queue_length\": len(queues['cloud']),\n",
    "                \"task_features\": task_features,  # Features related to the current task\n",
    "                \"agent_profile\": agent_profile,  # Agent's resource profile (lambda, etc.)\n",
    "                \"cluster_id\": self._one_hot_cluster(cluster_id)  # One-hot encode cluster_id\n",
    "            }\n",
    "        return state\n",
    "\n",
    "    # One-hot encoding for task type and cluster id\n",
    "    def _one_hot_task_type(task_type):\n",
    "        task_types = ['general', 'latency_sensitive', 'compute_intensive', 'data_intensive', 'deadline_hard']\n",
    "        task_type_onehot = [0] * len(task_types)\n",
    "        if task_type in task_types:\n",
    "            task_type_onehot[task_types.index(task_type)] = 1\n",
    "        return task_type_onehot\n",
    "\n",
    "    def _one_hot_cluster(self, cluster_id):\n",
    "        \"\"\"One-hot encode the cluster_id.\"\"\"\n",
    "        num_clusters = self.env_cfg[\"clustering\"][\"profiles\"][\"K\"]\n",
    "        cluster_onehot = [0] * num_clusters\n",
    "        cluster_onehot[cluster_id] = 1\n",
    "        return cluster_onehot\n",
    "\n",
    "    # Update task features to include one-hot encoding for task_type\n",
    "    def _get_task_features(self, agent_id):\n",
    "        \"\"\"Get the current task features for the agent from the pre-generated tasks.\"\"\"\n",
    "        tasks_df = self.env_cfg['tasks']\n",
    "        agent_tasks = tasks_df[tasks_df['agent_id'] == agent_id]\n",
    "        \n",
    "        if not agent_tasks.empty:\n",
    "            task = agent_tasks.iloc[0]\n",
    "            task_features = {\n",
    "                \"b_mb\": task[\"b_mb\"],\n",
    "                \"c_cycles\": task[\"c_cycles\"],\n",
    "                \"mem_mb\": task[\"mem_mb\"],\n",
    "                \"rho_cyc_per_mb\": task[\"rho_cyc_per_mb\"],\n",
    "                \"deadline_slots\": task[\"deadline_slots\"],\n",
    "                \"task_type\": self._one_hot_task_type(task[\"task_type\"]),  # One-hot encode task_type\n",
    "            }\n",
    "            return task_features\n",
    "        else:\n",
    "            return {}  # If no tasks for this agent, return an empty dictionary\n",
    "    \n",
    "    def _check_done(self):\n",
    "        \"\"\"Check if the episode is done (based on T_slots).\"\"\"\n",
    "        # If the number of slots (T_slots) has been completed, end the episode\n",
    "        if self.t_slots <= 0:\n",
    "            # Save the statistics of the episode like delays, drop rate, etc.\n",
    "            self._log_episode_statistics()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _log_episode_statistics(self):\n",
    "        \"\"\"Log the statistics of the current episode.\"\"\"\n",
    "        # Calculate and log relevant statistics such as average delay, drop rate, and reward\n",
    "        print(\"Logging statistics for the episode...\")\n",
    "        # Example (you can modify this based on your requirement):\n",
    "        avg_delay = np.mean([self._calculate_delay(task, self._calculate_task_time(task)) for task in self._get_all_tasks()])\n",
    "        avg_drop_rate = np.mean([self._check_task_drop(task) for task in self._get_all_tasks()])\n",
    "        \n",
    "        print(f\"Average Delay: {avg_delay}, Average Drop Rate: {avg_drop_rate}\")\n",
    "        # You can also save these statistics to a file or a list for further analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
